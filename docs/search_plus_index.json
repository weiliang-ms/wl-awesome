{"./":{"url":"./","title":"Introduction","keywords":"","body":"工作笔记 1.Linux基础 1.1常用shell 01查询文件内tab键 02计算shell运行时间 03防火墙开放端口 04关闭selinux 05开启tcp端口监听 06文件切割 07磁盘占用异常排查 08查看tcp连接状态 09打包iso 10创建大文件 11磁盘监控脚本 12ssl生成脚本 1.2包管理器 apt choco gem helm pip yum A-配置本地源 B-阿里源 C-软件及依赖导出 D-构建rpm包 1.3系统设置 01时钟服务器配置 02时区配置 03关闭防火墙 04关闭selinux 05调整文件描述符等 06配置yum本地源 07配置sudo用户 08配置互信 09关闭图形化 10配置hostname 1.4软件更新 01升级内核 02升级gcc 03升级openssl 04升级openssh 1.5驱动 GPU驱动 1.6虚拟化 01vsphere使用 02kvm 03openstack 04Ovirt 05iDRAC 06migration 1.7存储 ceph 01核心概念 01简述 02相关术语 03存储流程解析 04高可用性 05存储类型 06crush介绍 07ceph协议 08数据分段 09集群动态管理 10对比raid 11对比SAN、NAS、DAS 02集成部署 01硬件需求 02硬件配置建议 03操作系统建议 04环境初始化 05安装ceph核心组件 06添加硬盘至集群内 07安装dashboard 111ceph本地源搭建 03存储使用 块设备类型存储使用 文件系统类型存储使用 04对接k8s 01csi简介 02块设备类型存储使用 03文件系统类型存储使用 05运维管理 01服务启停 02pool的CRUD 03pool的常用配置 04配置pg放置组 05配置管理块存储 111卸载 raid 01-raid0 02-raid1 03-raid01 04-raid5 05-raid10 trouble shooting cpu cpu占用过高 安全 网络 proxy windows bridge nat route wireshark skill 2.容器 docker cli cmd image install network security storage thin k8s addons application helm base intro changelog cmd faq ingress install binary k8s-kubeadm k8s-rke kubeadm-offline issue monitor weavescope network calico dns quota security sidecar storage OpenEBS rook Velero workload endpoint job pod kubesphere 容器原理 cgroup foundation ns-ipc ns-mnt ns-net ns-user ns-uts ns 镜像构建 buildah podman skopeo 3.集成部署 ftp git hadoop mysql nginx nodejs ntp openjdk oracle pdns poweradmin python tomcat trafodion 4.持续集成&交付 argo jenkins 5.cncf cncf kubevirt 6.编程 golang letcode 7.FAQ sre vocabulary Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"1.Linux基础/1.1常用shell/":{"url":"1.Linux基础/1.1常用shell/","title":"1.1常用shell","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:21:03 "},"1.Linux基础/1.1常用shell/01查询文件内`tab`键.html":{"url":"1.Linux基础/1.1常用shell/01查询文件内`tab`键.html","title":"01查询文件内tab键","keywords":"","body":"查询文件内tab键 适用于yaml校验(制表符) shell script grep $'\\t' 文件名 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:10:29 "},"1.Linux基础/1.1常用shell/02计算shell运行时间.html":{"url":"1.Linux基础/1.1常用shell/02计算shell运行时间.html","title":"02计算shell运行时间","keywords":"","body":"计算shell运行时间 用date相减 ``shell script startTime=date +%Y%m%d-%H:%M:%SstartTime_s=date +%s` endTime=date +%Y%m%d-%H:%M:%S endTime_s=date +%s sumTime=$[ $endTime_s - $startTime_s ] echo \"$startTime ---> $endTime\" \"Total:$sumTime seconds\" > `time` ```shell script time sh xxx.sh # 会返回3个时间数据 # real 该命令的总耗时, 包括user和sys及io等待, 时间片切换等待等等 # user 该命令在用户模式下的CPU耗时,也就是内核外的CPU耗时,不含IO等待这些时间 # sys 该命令在内核中的CPU耗时,不含IO,时间片切换耗时. Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:11:53 "},"1.Linux基础/1.1常用shell/03防火墙开放端口.html":{"url":"1.Linux基础/1.1常用shell/03防火墙开放端口.html","title":"03防火墙开放端口","keywords":"","body":"防火墙开放端口 el6 #开放端口（7777） iptables -I INPUT -p tcp -m state --state NEW -m tcp --dport 7777 -j ACCEPT #保存 /etc/rc.d/init.d/iptables save #重载 service iptables restart el7 firewall-cmd --zone=public --add-port=7777/tcp --permanent #重新载入 firewall-cmd --reload Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:12:51 "},"1.Linux基础/1.1常用shell/04关闭selinux.html":{"url":"1.Linux基础/1.1常用shell/04关闭selinux.html","title":"04关闭selinux","keywords":"","body":"关闭selinux setenforce 0 sed -i \"s#SELINUX=enforcing#SELINUX=disabled#g\" /etc/selinux/config Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:13:30 "},"1.Linux基础/1.1常用shell/05开启tcp端口监听.html":{"url":"1.Linux基础/1.1常用shell/05开启tcp端口监听.html","title":"05开启tcp端口监听","keywords":"","body":"开启tcp端口监听 （测试网络连通性） python -m SimpleHTTPServer 9099 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:14:08 "},"1.Linux基础/1.1常用shell/06文件切割.html":{"url":"1.Linux基础/1.1常用shell/06文件切割.html","title":"06文件切割","keywords":"","body":"文件切割 split -d -b 100m enterprise.log enterprise- Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:16:05 "},"1.Linux基础/1.1常用shell/07磁盘占用异常排查.html":{"url":"1.Linux基础/1.1常用shell/07磁盘占用异常排查.html","title":"07磁盘占用异常排查","keywords":"","body":"磁盘占用异常排查 #查找 du -m --max-depth=1 |sort -gr lsof |grep delete #删除 lsof |grep delete|awk '{print $2}'|xargs -n1 kill -9 清理文件句柄 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:17:51 "},"1.Linux基础/1.1常用shell/08查看tcp连接状态.html":{"url":"1.Linux基础/1.1常用shell/08查看tcp连接状态.html","title":"08查看tcp连接状态","keywords":"","body":"查看tcp连接状态 netstat -na|awk '/^tcp/ {++S[$NF]} END {for(i in S) print i,S[i]}' Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:17:51 "},"1.Linux基础/1.1常用shell/09打包iso.html":{"url":"1.Linux基础/1.1常用shell/09打包iso.html","title":"09打包iso","keywords":"","body":"打包iso mkisofs -o ./package.iso -J -R -A -V -v package Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:19:00 "},"1.Linux基础/1.1常用shell/10创建大文件.html":{"url":"1.Linux基础/1.1常用shell/10创建大文件.html","title":"10创建大文件","keywords":"","body":"创建大文件 创建 fallocate -l 10G test4 撤销 fallocate -d test4 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:20:19 "},"1.Linux基础/1.1常用shell/11磁盘监控脚本.html":{"url":"1.Linux基础/1.1常用shell/11磁盘监控脚本.html","title":"11磁盘监控脚本","keywords":"","body":"磁盘监控脚本 系统版本：CentOS7 脚本/usr/bin/disk-monitor.sh内容如下： #!/bin/bash LOCAL_HOST=192.168.1.3 RECEIVE_LIST=\"aaa@xxx.com\" CC_LIST=\"bbb@xxx.com,ccc@xxx.com,ddd@xxx.com\" MOUNT_NODE_COUNT=`df -h|wc -l` echo \"挂载节点数为：${MOUNT_NODE_COUNT}\" while [[ ${MOUNT_NODE_COUNT} -ne 1 ]]; do FILE_SYSTEM=`df -h |sed -n \"$MOUNT_NODE_COUNT p\"|awk '{print $1}'` MOUNT_NODE=`df -h |sed -n \"$MOUNT_NODE_COUNT p\"|awk '{print $6}'` PART_FREE_SPACE=`df -h |sed -n \"$MOUNT_NODE_COUNT p\"|awk '{print $4}'` UTILIZATION_RATE=`df -h |sed -n \"$MOUNT_NODE_COUNT p\"|awk '{print $5}'` UTILIZATION_RATE_VALUE=`echo ${UTILIZATION_RATE}|sed 's/.$//'` # echo \"文件系统：`echo ${FILE_SYSTEM}`，挂在节点：`echo \"$MOUNT_NODE\"`，分区磁盘使用率为：`echo ${UTILIZATION_RATE}`, 剩余磁盘空间：`echo ${PART_FREE_SPACE}`\" if [[ ${UTILIZATION_RATE_VALUE} -gt 95 ]]; then MAIL_CONTENT=\"[当前地址]：${LOCAL_HOST} [文件系统]：`echo ${FILE_SYSTEM}` [挂在节点]：`echo \"$MOUNT_NODE\"` [分区磁盘使用率]：`echo ${UTILIZATION_RATE}` 已达告警阈值，请及时清理！！！\" echo ${MAIL_CONTENT} echo \"${MAIL_CONTENT}\" | mail -s \"磁盘剩余空间告警\" -c ${CC_LIST} ${RECEIVE_LIST} &> /dev/null fi let MOUNT_NODE_COUNT-- done 安装mailx yum install -y mailx 配置mailx，/etc/mail.rc追加以下内容 set from=aaa@xxx.com set smtp=smtp.xxx.com:587 set smtp-auth-user=aaa set smtp-auth-password=****** set smtp-auth=login set smtp-use-starttls set ssl-verify=ignore set nss-config-dir=/etc/pki/nssdb/ 配置定时任务 cat >> /etc/crontab Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:20:19 "},"1.Linux基础/1.1常用shell/12ssl生成脚本.html":{"url":"1.Linux基础/1.1常用shell/12ssl生成脚本.html","title":"12ssl生成脚本","keywords":"","body":"ssl生成脚本 #!/bin/bash # 域名 export domain=www.example.com # IP地址（可选） export address=192.168.1.11 # 国家 export contryName=CN # 省/州/邦 export stateName=Liaoning # 地方/城市名 export locationName=Shenyang # 组织/公司名称 export organizationName=example # 组织/公司部门名称 export sectionName=develop echo \"Getting Certificate Authority...\" openssl genrsa -out ca.key 4096 openssl req -x509 -new -nodes -sha512 -days 3650 \\ -subj \"/C=$contryName/ST=$stateName/L=$locationName/O=$organizationNaem/OU=$sectionName/CN=$domain\" \\ -key ca.key \\ -out ca.crt echo \"Create your own Private Key...\" openssl genrsa -out $domain.key 4096 echo \"Generate a Certificate Signing Request...\" openssl req -sha512 -new \\ -subj \"/C=$contryName/ST=$stateName/L=$locationName/O=$organizationNaem/OU=$sectionName/CN=$domain\" \\ -key $domain.key \\ -out $domain.csr echo \"Generate the certificate of your registry host...\" cat > v3.ext Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:21:03 "},"1.Linux基础/1.1常用shell/13批量修改文件格式.html":{"url":"1.Linux基础/1.1常用shell/13批量修改文件格式.html","title":"13批量修改文件格式","keywords":"","body":" 批量修改为unix for file in `find hack/lib -name *.sh` do vi +':w ++ff=unix' +':q' ${file} done Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-27 17:20:15 "},"1.Linux基础/1.2包管理器/apt/":{"url":"1.Linux基础/1.2包管理器/apt/","title":"apt","keywords":"","body":"配置清华源 清理已有源 rm -f /etc/apt/sources.list.d/* 添加清华源 cat > /etc/apt/sources.list 更新 sudo apt-get update Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:03 "},"1.Linux基础/1.2包管理器/choco/":{"url":"1.Linux基础/1.2包管理器/choco/","title":"choco","keywords":"","body":"choco 管理员运行cmd，执行 shell script @powershell -NoProfile -ExecutionPolicy Bypass -Command \"iex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))\" && SET PATH= %PATH %; %ALLUSERSPROFILE %\\chocolatey\\bin Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:03 "},"1.Linux基础/1.2包管理器/gem/":{"url":"1.Linux基础/1.2包管理器/gem/","title":"gem","keywords":"","body":"gem源配置 查看默认源 gem sources -l 配置代理 修改文件/usr/bin/gem begin args += ['--http-proxy','http://x.x.x.x:port'] Gem::GemRunner.new.run args rescue Gem::SystemExitException => e exit e.exit_code end 修改默认源 gem sources -r https://rubygems.org/ -a https://gems.ruby-china.com/ bundle config mirror.https://rubygems.org https://gems.ruby-china.com Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:03 "},"1.Linux基础/1.2包管理器/helm/":{"url":"1.Linux基础/1.2包管理器/helm/","title":"helm","keywords":"","body":"helm Kubernetes的软件包管理工具 版本说明 Helm 2 是 C/S 架构，主要分为客户端 helm 和服务端 Tiller; Helm 3 中移除了 Tiller, 版本相关的数据直接存储在了 Kubernetes 中 Helm 组件及相关术语 helm Helm 是一个命令行下的客户端工具。主要用于 Kubernetes 应用程序 Chart 的创建、打包、发布以及创建和管理本地和远程的 Chart 仓库。 Chart Helm 的软件包，采用 TAR 格式。类似于 APT 的 DEB 包或者 YUM 的 RPM 包，其包含了一组定义 Kubernetes 资源相关的 YAML 文件。 Repoistory Helm 的软件仓库，Repository 本质上是一个 Web 服务器，该服务器保存了一系列的 Chart 软件包以供用户下载，并且提供了一个该 Repository 的 Chart 包的清单文件以供查询。Helm 可以同时管理多个不同的 Repository。 Release 使用 helm install 命令在 Kubernetes 集群中部署的 Chart 称为 Release。可以理解为 Helm 使用 Chart 包部署的一个应用实例 helm安装 下载helm release压缩包 release版本 解压，添加到PATH 创建应用 初始化 [root@node3 cloud]# helm create redis Creating redis 应用目录结构 [root@node3 cloud]# tree redis/ redis/ ├── charts ├── Chart.yaml ├── templates │ ├── deployment.yaml │ ├── _helpers.tpl │ ├── hpa.yaml │ ├── ingress.yaml │ ├── NOTES.txt │ ├── serviceaccount.yaml │ ├── service.yaml │ └── tests │ └── test-connection.yaml └── values.yaml Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:03 "},"1.Linux基础/1.2包管理器/pip/":{"url":"1.Linux基础/1.2包管理器/pip/","title":"pip","keywords":"","body":"配置pip 配置源 mkdir ~/.pip cat >> ~/.pip/pip.conf 下载包（只下载不安装） pip download -d -r requirement.txt requirement.txt格式内容 scipy numpy jupyter ipython easydict Cython h5py numpy mahotas requests bs4 lxml pillow redis torch torchvision paramiko pycrypto uliengineering matplotlib keras==2.1.5 web.py==0.40.dev0 scikit-image==0.15.0 lmdb pandas opencv-contrib-python==4.0.0.21 tensorflow-gpu==1.8 安装（离线导出）包 pip3 install --no-index --find-links=./pip -r requirement.txt Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:03 "},"1.Linux基础/1.2包管理器/yum/A-配置本地源.html":{"url":"1.Linux基础/1.2包管理器/yum/A-配置本地源.html","title":"A-配置本地源","keywords":"","body":"本地yum源 适用于 主机不可以直连外网、且不可以通过代理访问外网 1、查看操作系统 cat /etc/system-release 2、获取系统安装镜像 1、获取方式一：找系统运维管理员提供，推荐 2、获取方式二：自己下载，不推荐，文件大小一般4G左右，小版本一定要匹配！ 官方下载地址：https://wiki.centos.org/Download 3、上传挂载 注意路径、文件名需要替换，以下命令相当于将CentOS-7-x86_64-DVD-1511.iso，解压到/media mount -o loop ~/CentOS-7-x86_64-DVD-1511.iso /media 4、卸载、拷贝、删除 mkdir -p /yum && cp -r /media/* /yum/ unmout /media 5、删除原有yum源repo文件 rm -f /etc/yum.repos.d/*.repo 6、新建yum repo文件 cat >> /etc/yum.repos.d/c7.repo 7、测试 yum clean all && yum makecache yum install -y telnet vim Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:03 "},"1.Linux基础/1.2包管理器/yum/B-阿里源.html":{"url":"1.Linux基础/1.2包管理器/yum/B-阿里源.html","title":"B-阿里源","keywords":"","body":"阿里yum源 1、配置DNS解析 echo \"nameserver 114.114.114.114\" >> /etc/resolv.conf 2、删除原有yum源repo文件 rm -f /etc/yum.repos.d/*.repo 3、下载阿里yum源文件 CentOS 6 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo CentOS 7 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:03 "},"1.Linux基础/1.2包管理器/yum/C-软件及依赖导出.html":{"url":"1.Linux基础/1.2包管理器/yum/C-软件及依赖导出.html","title":"C-软件及依赖导出","keywords":"","body":"导出依赖与使用 导出（yum源可用） yum install yum-plugin-downloadonly -y yum install --downloadonly --downloaddir=./gcc gcc 生成repo依赖关系 yum install -y createrepo createrepo ./gcc 压缩 tar zcvf gcc.tar.gz gcc 使用（yum源不可用） tar zxvf gcc.tar.gz -C / cat > /etc/yum.repos.d/gcc.repo Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:03 "},"1.Linux基础/1.2包管理器/yum/D-构建rpm包.html":{"url":"1.Linux基础/1.2包管理器/yum/D-构建rpm包.html","title":"D-构建rpm包","keywords":"","body":"rpm制作 安装rpmbuild yum install rpm-build -y 创建目录 mkdir ~/rpmbuild/{BUILD,BUILDROOT,RPMS,SOURCES,SPECS,SRPMS} 编写~/rpmbuild/SPECS/nginx.spec 样例 %define realname nginx %define orgnize neu %define realver 1.18.0 %define srcext tar.gz %define opensslVersion openssl-1.0.2r # Common info Name: %{realname} Version: %{realver} Release: %{orgnize}%{?dist} Summary:Nginx is a web server software License:GPL URL:http://nginx.org Source0: %{realname}-%{realver}%{?extraver}.%{srcext} Source1: %{opensslVersion}.tar.gz Source2: headers-more-nginx-module-master.tar.gz Source3: naxsi-0.56.tar.gz Source4: nginx_upstream_check_module-master.tar.gz Source5: ngx-fancyindex-master.tar.gz Source6: ngx_cache_purge-2.3.tar.gz Source11: nginx.logrotate #Source12: nginx.conf Source13: conf Source14: nginx Source21: pcre-8.44.tar.gz Source22: zlib-1.2.11.tar.gz Source23: LuaJIT-2.0.5.tar.gz Source24: lua-nginx-module-0.10.13.tar.gz Source25: ngx_devel_kit-0.3.0.tar.gz #Patch: nginx-memset_zero.patch # Install-time parameters Provides: httpd http_daemon webserver %{?suse_version:suse_help_viewer} Requires: logrotate #BuildRequires: gcc zlib-devel pcre-devel %description nginx [engine x] is an HTTP and reverse proxy server %post chkconfig nginx on sed -i \"/* soft nofile 655350/d\" /etc/security/limits.conf echo \"* soft nofile 655350\" >> /etc/security/limits.conf sed -i \"/* hard nofile 655350/d\" /etc/security/limits.conf echo \"* hard nofile 655350\" >> /etc/security/limits.conf sed -i \"/* soft nproc 65535/d\" /etc/security/limits.conf echo \"* soft nproc 65535\" >> /etc/security/limits.conf sed -i \"/* hard nproc 65535/d\" /etc/security/limits.conf echo \"* hard nproc 65535\" >> /etc/security/limits.conf #sed -i '/\\/etc\\/logrotate.d\\/nginx/d' /etc/crontab #echo \"0 0 * * * root bash /usr/sbin/logrotate -f /etc/logrotate.d/nginx\" >> /etc/crontab mkdir -p ~/.vim cp -r -v /opt/nginx/vim ~/.vim/ cat > ~/.vim/filetype.vim c && %__mv -f c CHANGES.ru %__install -d %{buildroot}~/.vim %__install -D -m755 %{S:11} %{buildroot}%{_sysconfdir}/logrotate.d/%{name} %__install -D -m755 %{S:14} %{buildroot}%{_sysconfdir}/init.d/%{name} %__cp -r -v %{_builddir}/%{realname}-%{realver}%{?extraver}/lj2 %{buildroot}/opt/nginx/ %__cp -r -v %{_builddir}/%{realname}-%{realver}%{?extraver}/contrib/vim %{buildroot}/opt/nginx/ %__cp -r -v %{S:13}/* %{buildroot}/opt/nginx/conf/ %clean [ \"%{buildroot}\" != \"/\" ] && rm -rf %{buildroot} %files %config(noreplace) %{_sysconfdir}/logrotate.d/%{name} %config(noreplace) %{_sysconfdir}/init.d/%{name} %doc /opt/nginx/* %changelog 构建 rpmbuild -ba ~/rpmbuild/SPECS/nginx.spec Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:03 "},"1.Linux基础/1.3系统设置/":{"url":"1.Linux基础/1.3系统设置/","title":"1.3系统设置","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:29:26 "},"1.Linux基础/1.3系统设置/01时钟服务器配置.html":{"url":"1.Linux基础/1.3系统设置/01时钟服务器配置.html","title":"01时钟服务器配置","keywords":"","body":"时钟服务器配置 安装ntpdate yum install -y ntp 配置定时任务，至少保证5分钟同步一次 */5 * * * * ntpdate ntp-server Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:24:24 "},"1.Linux基础/1.3系统设置/02时区配置.html":{"url":"1.Linux基础/1.3系统设置/02时区配置.html","title":"02时区配置","keywords":"","body":"配置为上海时区 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:24:24 "},"1.Linux基础/1.3系统设置/03关闭防火墙.html":{"url":"1.Linux基础/1.3系统设置/03关闭防火墙.html","title":"03关闭防火墙","keywords":"","body":"关闭防火墙 systemctl disable firewalld --now Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:25:08 "},"1.Linux基础/1.3系统设置/04关闭selinux.html":{"url":"1.Linux基础/1.3系统设置/04关闭selinux.html","title":"04关闭selinux","keywords":"","body":"关闭selinux setenforce 0 sed -i \"s#SELINUX=enforcing#SELINUX=disabled#g\" /etc/selinux/config Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:25:40 "},"1.Linux基础/1.3系统设置/05调整文件描述符等.html":{"url":"1.Linux基础/1.3系统设置/05调整文件描述符等.html","title":"05调整文件描述符等","keywords":"","body":"调整文件描述符等 cat >> /etc/pam.d/login /etc/security/limits.conf cat >> /etc/security/limits.conf /etc/security/limits.d/20-nproc.conf cat >> /etc/security/limits.d/20-nproc.conf /proc/sys/fs/file-max Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:26:45 "},"1.Linux基础/1.3系统设置/06配置yum本地源.html":{"url":"1.Linux基础/1.3系统设置/06配置yum本地源.html","title":"06配置yum本地源","keywords":"","body":"配置yum本地源 rm -rf /etc/yum.repos.d/* mount -o loop CentOS-7-x86_64-DVD-2009.iso /media mkdir -p /yum cp -r /media/* /yum umount /media rm -f CentOS-7-x86_64-DVD-2009.iso 配置文件 cat > /etc/yum.repos.d/c7.repo Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:27:14 "},"1.Linux基础/1.3系统设置/07配置sudo用户.html":{"url":"1.Linux基础/1.3系统设置/07配置sudo用户.html","title":"07配置sudo用户","keywords":"","body":"配置sudo用户 强密码生成 或利用以下指令生成 pwmake 128 初始化用户，配置sudo权限 useradd -m neusoft && echo \"m&t+arz4SEvWq5)QG\" | passwd --stdin neusoft echo \"neusoft ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:29:02 "},"1.Linux基础/1.3系统设置/08配置互信.html":{"url":"1.Linux基础/1.3系统设置/08配置互信.html","title":"08配置互信","keywords":"","body":"配置互信 配置root用户 ssh-keygen -t rsa -b 2048 -N '' -f ~/.ssh/id_rsa cat .ssh/id_rsa.pub > ~/.ssh/authorized_keys chmod -R 600 ~/.ssh Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:29:02 "},"1.Linux基础/1.3系统设置/09关闭图形化.html":{"url":"1.Linux基础/1.3系统设置/09关闭图形化.html","title":"09关闭图形化","keywords":"","body":"关闭图形化 centos7 systemctl set-default multi-user.target init 3 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:29:02 "},"1.Linux基础/1.3系统设置/10配置hostname.html":{"url":"1.Linux基础/1.3系统设置/10配置hostname.html","title":"10配置hostname","keywords":"","body":"配置hostname 方法一 cat >> /etc/sysconfig/network /proc/sys/kernel/hostname 方法二 cat >> /etc/sysconfig/network 方法三 cat >> /etc/sysconfig/network 方法四 hostnamectl --static set-hostname master Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:29:26 "},"1.Linux基础/1.4软件更新/":{"url":"1.Linux基础/1.4软件更新/","title":"1.4软件更新","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:54:41 "},"1.Linux基础/1.4软件更新/01升级内核.html":{"url":"1.Linux基础/1.4软件更新/01升级内核.html","title":"01升级内核","keywords":"","body":"el7在线升级稳定版内核 导入public key,添加扩展源 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org yum install https://www.elrepo.org/elrepo-release-7.0-4.el7.elrepo.noarch.rpm -y 安装最新稳定版 yum -y --enablerepo=elrepo-kernel install kernel-lt.x86_64 kernel-lt-devel.x86_64 删除旧版本工具包 yum remove kernel-tools-libs.x86_64 kernel-tools.x86_64 -y 安装新版本工具包 yum --disablerepo=\\* --enablerepo=elrepo-kernel install -y kernel-lt-tools.x86_64 查看内核列表 awk -F\\' '$1==\"menuentry \" {print i++ \" : \" $2}' /etc/grub2.cfg 重建内核 grub2-mkconfig -o /boot/grub2/grub.cfg 配置新版内核 sed -i \"s/GRUB_DEFAULT=saved/GRUB_DEFAULT=0/g\" /etc/default/grub 重启 reboot 删除旧版本内核 oldkernel=`rpm -qa|grep kernel-[0-9]` && yum remove -y $oldkernel el7在线升级主线版内核 导入public key,添加扩展源 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org yum install https://www.elrepo.org/elrepo-release-7.0-4.el7.elrepo.noarch.rpm -y 安装最新主线版 yum -y --enablerepo=elrepo-kernel install kernel-ml.x86_64 kernel-ml-devel.x86_64 删除旧版本工具包 rpm -qa|grep kernel-3|xargs -n1 rpm -e rpm -e kernel-tools-libs-* 安装新版本工具包 yum --disablerepo=\\* --enablerepo=elrepo-kernel install -y kernel-ml-tools.x86_64 查看内核列表 awk -F\\' '$1==\"menuentry \" {print i++ \" : \" $2}' /etc/grub2.cfg 重建内核 grub2-mkconfig -o /boot/grub2/grub.cfg 配置新版内核 grub2-set-default 0 系统盘非raid模式直接重启 reboot trouble shooting pstore: unknown compression: deflate 启动异常 修改引导配置 vim /etc/default/grub 在GRUB_CMDLINE_LINUX最后添加mgag200.modeset=0 配置样例 GRUB_TIMEOUT=5 GRUB_DISTRIBUTOR=\"$(sed 's, release .*$,,g' /etc/system-release)\" GRUB_DEFAULT=saved GRUB_DISABLE_SUBMENU=true GRUB_TERMINAL_OUTPUT=\"console\" GRUB_CMDLINE_LINUX=\"crashkernel=auto spectre_v2=retpoline rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet mgag200.modeset=0\" GRUB_DISABLE_RECOVERY=\"true\" 重新生成引导文件 grub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-23 09:59:45 "},"1.Linux基础/1.4软件更新/02升级gcc.html":{"url":"1.Linux基础/1.4软件更新/02升级gcc.html","title":"02升级gcc","keywords":"","body":"升级gcc 升级至5.4 下载介质 gcc-5.4.0.tar.gz gmp-4.3.2.tar.gz mpc-1.0.1.tar.gz mpfr-2.4.2.tar.gz 安装依赖 ```shell script tar zxvf gmp-4.3.2.tar.gz cd gmp-4.3.2 ./configure --prefix=/usr/local/gmp-4.3.2 \\ && make -j $(nproc) && make install && cd - tar zxvf mpfr-2.4.2.tar.gz cd mpfr-2.4.2 ./configure --prefix=/usr/local/mpfr-2.4.2 \\ --with-gmp=/usr/local/gmp-4.3.2 \\ && make -j $(nproc) && make install && cd - tar zxvfv mpc-1.0.1.tar.gz cd mpc-1.0.1 ./configure --prefix=/usr/local/mpc-1.0.1 \\ --with-gmp=/usr/local/gmp-4.3.2 --with-mpfr=/usr/local/mpfr-2.4.2 \\ && make -j $(nproc) && make install && cd - > 配置环境变量 ```shell script echo \"export LD_LIBRARY_PATH=\\$LD_LIBRARY_PATH:/usr/local/gmp-4.3.2/lib:/usr/local/mpc-1.0.1/lib:/usr/local/mpfr-2.4.2/lib\" >> /etc/profile . /etc/profile 编译gcc ```shell script tar -xzvf gcc-5.4.0.tar.gz && mkdir gcc-5.4.0/gcc-build && cd gcc-5.4.0/gcc-build \\ && ../configure --prefix=/usr/local/gcc-5.4.0 --enable-threads=posix \\ --disable-checking --disable-multilib --enable-languages=c,c++ \\ --with-gmp=/usr/local/gmp-4.3.2 --with-mpfr=/usr/local/mpfr-2.4.2 \\ --with-mpc=/usr/local/mpc-1.0.1 && make -j $(nproc) && make install && cd - > 备份更新 ```shell script mkdir -p /usr/local/bakup/gcc mv /usr/bin/{gcc,g++} /usr/local/bakup/gcc/ cp /usr/local/gcc-5.4.0/bin/gcc /usr/bin/gcc cp /usr/local/gcc-5.4.0/bin/g++ /usr/bin/g++ Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:53:36 "},"1.Linux基础/1.4软件更新/03升级openssl.html":{"url":"1.Linux基础/1.4软件更新/03升级openssl.html","title":"03升级openssl","keywords":"","body":"openssl 1.下载最新稳定版 openssl release地址 2.安装必要依赖 ```shell script yum install -y wget gcc perl > 3.解压编译 ```shell script tar zxvf openssl-OpenSSL_*.tar.gz cd openssl-OpenSSL* ./config shared --openssldir=/usr/local/openssl --prefix=/usr/local/openssl make -j $(nproc) && make install sed -i '/\\/usr\\/local\\/openssl\\/lib/d' /etc/ld.so.conf echo \"/usr/local/openssl/lib\" >> /etc/ld.so.conf ldconfig -v mv /usr/bin/openssl /usr/bin/openssl.old ln -s /usr/local/openssl/bin/openssl /usr/bin/openssl cd - openssl version Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:54:09 "},"1.Linux基础/1.4软件更新/04升级openssh.html":{"url":"1.Linux基础/1.4软件更新/04升级openssh.html","title":"04升级openssh","keywords":"","body":"升级openssh 1.下载openssh包 下载站点 2.开启telnet(防止失败) ```shell script yum install -y telnet-server telnet xinetd systemctl restart telnet.socket systemctl restart xinetd echo 'pts/0' >>/etc/securetty echo 'pts/1' >>/etc/securetty systemctl restart telnet.socket > 3.安装 备份旧`ssh`配置文件 ```shell mv /etc/ssh/ /etc/ssh-bak 编译安装 yum install -y pam-devel zlib-devel tar zxvf openssh-*.tar.gz cd openssh* ./configure --prefix=/usr --sysconfdir=/etc/ssh --with-ssl-dir=/usr/local/openssl --with-md5-passwords make -j $(nproc) && make install 复制启动脚本： \\cp contrib/redhat/sshd.init /etc/init.d/sshd \\chkconfig sshd on 验证版本信息： ssh -V 配置 ```shell script cat > /etc/ssh/sshd_config 调整`service`,重启`ssh`服务 ```shell script sed -i \"s;Type=notify;#Type=notify;g\" /usr/lib/systemd/system/sshd.service systemctl daemon-reload && systemctl restart sshd 成功后关闭telnet shell script systemctl disable telnet.socket --now systemctl disable xinetd --now Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:55:25 "},"1.Linux基础/1.5驱动/":{"url":"1.Linux基础/1.5驱动/","title":"1.5驱动","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:58:40 "},"1.Linux基础/1.5驱动/GPU驱动.html":{"url":"1.Linux基础/1.5驱动/GPU驱动.html","title":"GPU驱动","keywords":"","body":"安装Gpu驱动 1.查看Gpu版本 lspci | grep NVIDIA 执行结果如下 00:08.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 SXM2 32GB] (rev a1) 说明版本为Tesla V100 2.安装驱动 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm yum install nvidia-x11-drv nvidia-x11-drv-32bit -y Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:59:04 "},"1.Linux基础/1.6虚拟化/01vsphere使用.html":{"url":"1.Linux基础/1.6虚拟化/01vsphere使用.html","title":"01vsphere使用","keywords":"","body":"磁盘扩容 1.编辑虚拟机调整硬盘空间，重启虚拟机 2.查看分区后扩容后的大小 fdisk -l 已经扩到了100G（扩容前30G） 3.新建分区并设置分区为LVM格式 fdisk /dev/sda 最后键入w写入 4.创建物理卷，并加入到卷组 partprobe pvcreate /dev/sda3 扩展vg卷组大小 vgs vgextend /dev/centos /dev/sda3 vgs 5.使用lvextend命令来扩容lv逻辑卷空间大小 lvs lvextend -L +69.9G /dev/centos/root 查看文件系统 df -lhT 重新加载逻辑卷的大小 xfs类型文件系统执行 xfs_growfs /dev/centos/root ext2、ext3、ext4类型文件系统执行 resize2fs /dev/centos/root 6.查看磁盘大小 df -h Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 12:03:41 "},"1.Linux基础/1.6虚拟化/02kvm.html":{"url":"1.Linux基础/1.6虚拟化/02kvm.html","title":"02kvm","keywords":"","body":" Table of Contents generated with DocToc 安装kvm KVM的web管理界面 安装vnc 安装kvm 关闭selinux setenforce 0 sed -i \"s#SELINUX=enforcing#SELINUX=disabled#g\" /etc/selinux/config 安装依赖 yum install kvm kmod-kvm qemu -y 判断是否载入kvm模块 /sbin/lsmod | grep kvm 拷贝指令 cp /usr/libexec/qemu-kvm /usr/bin/ KVM的web管理界面 Wok Wok基于cherrypy的web框架，可以通过一些插件来进行扩展，例如：虚拟化管理、主机管理、系统管理。它可以在任何支持HTML5的网页浏览器中运行。 Kimchi Kimchi是一个基于HTML5的KVM管理工具，是Wok的一个插件（使用Kimchi前一定要先安装了wok），通过Kimchi可以更方便的管理KVM。 项目地址 安装work wok下载链接 yum install -y wok-2.5.0-0.el7.centos.noarch.rpm 安装kimchi kimchi下载链接 yum install -y kimchi-2.5.0-0.el7.centos.noarch.rpm 启动wok systemctl daemon-reload systemctl start wokd 开放8001端口 firewall-cmd --zone=public --add-port=8001/tcp --permanent #重新载入 firewall-cmd --reload 访问 安装vnc 安装拷贝文件 yum install tigervnc-server -y cp /lib/systemd/system/vncserver@.service /etc/systemd/system/vncserver.service 修改配置 vim /etc/systemd/system/vncserver.service 设置密码 重载启动 systemctl daemon-reload Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:06 "},"1.Linux基础/1.6虚拟化/03openstack.html":{"url":"1.Linux基础/1.6虚拟化/03openstack.html","title":"03openstack","keywords":"","body":" Table of Contents generated with DocToc 控制节点 计算节点 参考文章 节点信息 控制节点：132.232.81.250 计算节点：139.155.29.65 控制节点 0.修改hostname cat >> /etc/sysconfig/network > /etc/hosts 1.配置yum源 rm -f /etc/yum.repos.d/*.repo curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo 更新 cat > /etc/yum.repos.d/cloud.repo 2.安装openstack客户端 yum install python-openstackclient -y yum install openstack-selinux -y 3.mariadb数据库的安装 参考安装配置 OpenStack使用数据库来存储，支持大部分数据库MariaDB或、MySQL或者PostgreSQL，数据库运行于控制节点 yum install mariadb mariadb-server python2-PyMySQL -y 启动 systemctl enable mariadb.service --now 初始化 mysql_secure_installation 根据提示依次输入 Enter current password for root (enter for none): # 输入数据库超级管理员root的密码(注意不是系统root的密码)，第一次进入还没有设置密码则直接回车 Set root password? [Y/n] # 设置密码，y New password: # 新密码 Re-enter new password: # 再次输入密码 Remove anonymous users? [Y/n] # 移除匿名用户， y Disallow root login remotely? [Y/n] # 拒绝root远程登录，n，不管y/n，都会拒绝root远程登录 Remove test database and access to it? [Y/n] # 删除test数据库，y：删除。n：不删除，数据库中会有一个test数据库，一般不需要 Reload privilege tables now? [Y/n] # 重新加载权限表，y。或者重启服务也许 4.安装rabbitmq 安装 yum install rabbitmq-server -y 启动 systemctl enable rabbitmq-server --now 初始化 rabbitmqctl add_user openstack RABBIT_PASS rabbitmqctl set_permissions openstack \".*\" \".*\" \".*\" 5.安装Memcached yum install memcached python-memcached -y 调整配置 sed -i \"s#127.0.0.1#0.0.0.0#g\" /etc/sysconfig/memcached 启动 systemctl enable memcached.service --now 6.安装keystone服务 创建数据库用户 CREATE DATABASE keystone; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' \\ IDENTIFIED BY 'KEYSTONE_DBPASS'; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' \\ IDENTIFIED BY 'KEYSTONE_DBPASS'; 安装keystone相关软件包 yum install openstack-keystone httpd mod_wsgi -y 配置keystone cat > /etc/keystone/keystone.conf 同步数据 su -s /bin/sh -c \"keystone-manage db_sync\" keystone 初始化fernet keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone 配置httpd echo \"ServerName controller\" >>/etc/httpd/conf/httpd.conf echo 'Listen 5000 Listen 35357 WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP} WSGIProcessGroup keystone-public WSGIScriptAlias / /usr/bin/keystone-wsgi-public WSGIApplicationGroup %{GLOBAL} WSGIPassAuthorization On ErrorLogFormat \"%{cu}t %M\" ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined Require all granted WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP} WSGIProcessGroup keystone-admin WSGIScriptAlias / /usr/bin/keystone-wsgi-admin WSGIApplicationGroup %{GLOBAL} WSGIPassAuthorization On ErrorLogFormat \"%{cu}t %M\" ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined Require all granted ' >/etc/httpd/conf.d/wsgi-keystone.conf 启动httpd systemctl start httpd systemctl enable httpd 初始化keystone export OS_TOKEN=ADMIN_TOKEN export OS_URL=http://controller:35357/v3 export OS_IDENTITY_API_VERSION=3 openstack service create --name keystone --description \"OpenStack Identity\" identity openstack endpoint create --region RegionOne identity public http://controller:5000/v3 openstack endpoint create --region RegionOne identity internal http://controller:5000/v3 openstack endpoint create --region RegionOne identity admin http://controller:35357/v3 # 创建域,项目,用户,角色 openstack domain create --description \"Default Domain\" default openstack project create --domain default --description \"Admin Project\" admin openstack user create --domain default --password ADMIN_PASS admin openstack role create admin openstack role add --project admin --user admin admin openstack project create --domain default \\ --description \"Service Project\" service unset OS_TOKEN OS_URL ###把他加入开机自启，不然下次启动会无法访问 export OS_PROJECT_DOMAIN_NAME=default export OS_USER_DOMAIN_NAME=default export OS_PROJECT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=ADMIN_PASS export OS_AUTH_URL=http://controller:35357/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 验证keystone服务是否正常 7.安装glance镜像服务 mysql中创库授权 CREATE DATABASE glance; GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' \\ IDENTIFIED BY 'GLANCE_DBPASS'; GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' \\ IDENTIFIED BY 'GLANCE_DBPASS'; 在keystone创建系统账号,并关联角色 openstack user create --domain default --password GLANCE_PASS glance openstack role add --project service --user glance admin 在keystone上创建服务名称,注册api openstack service create --name glance --description \"OpenStack Image\" image openstack endpoint create --region RegionOne image public http://controller:9292 openstack endpoint create --region RegionOne image internal http://controller:9292 openstack endpoint create --region RegionOne image admin http://controller:9292 安装相关软件包 yum install openstack-glance openstack-utils -y 修改配置文件 openstack-config --set /etc/glance/glance-api.conf database connection mysql+pymysql://glance:GLANCE_DBPASS@controller/glance openstack-config --set /etc/glance/glance-api.conf glance_store stores file,http openstack-config --set /etc/glance/glance-api.conf glance_store default_store file openstack-config --set /etc/glance/glance-api.conf glance_store filesystem_store_datadir /var/lib/glance/images/ openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_uri http://controller:5000 openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_url http://controller:35357 openstack-config --set /etc/glance/glance-api.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_type password openstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_domain_name default openstack-config --set /etc/glance/glance-api.conf keystone_authtoken user_domain_name default openstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_name service openstack-config --set /etc/glance/glance-api.conf keystone_authtoken username glance openstack-config --set /etc/glance/glance-api.conf keystone_authtoken password GLANCE_PASS openstack-config --set /etc/glance/glance-api.conf paste_deploy flavor keystone #cat glance-registry.conf >/etc/glance/glance-registry.conf openstack-config --set /etc/glance/glance-registry.conf database connection mysql+pymysql://glance:GLANCE_DBPASS@controller/glance openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_uri http://controller:5000 openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_url http://controller:35357 openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_type password openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_domain_name default openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken user_domain_name default openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_name service openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken username glance openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken password GLANCE_PASS openstack-config --set /etc/glance/glance-registry.conf paste_deploy flavor keystone 同步数据(创表) su -s /bin/sh -c \"glance-manage db_sync\" glance 启动服务 systemctl enable openstack-glance-api.service openstack-glance-registry.service systemctl start openstack-glance-api.service openstack-glance-registry.service 验证。上传cirros-0.3.4-x86_64-disk.img到当前目录 openstack image create \"cirros\" --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare --public 检查上传结果 openstack image list 8.安装nova计算服务控制端 mysql中创库授权 CREATE DATABASE nova_api; CREATE DATABASE nova; GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' \\ IDENTIFIED BY 'NOVA_DBPASS'; GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' \\ IDENTIFIED BY 'NOVA_DBPASS'; GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' \\ IDENTIFIED BY 'NOVA_DBPASS'; GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' \\ IDENTIFIED BY 'NOVA_DBPASS'; 在keystone创建系统账号,并关联角色 openstack user create --domain default --password NOVA_PASS nova openstack role add --project service --user nova admin 在keystone上创建服务名称,注册api openstack service create --name nova \\ --description \"OpenStack Compute\" compute openstack endpoint create --region RegionOne \\ compute public http://controller:8774/v2.1/%\\(tenant_id\\)s openstack endpoint create --region RegionOne \\ compute internal http://controller:8774/v2.1/%\\(tenant_id\\)s openstack endpoint create --region RegionOne \\ compute admin http://controller:8774/v2.1/%\\(tenant_id\\)s 安装相关软件包 yum install -y openstack-nova-api openstack-nova-placement-api \\ openstack-nova-conductor openstack-nova-console \\ openstack-nova-novncproxy openstack-nova-scheduler 修改配置文件 cp /etc/nova/nova.conf{,.bak} grep -Ev '^$|#' /etc/nova/nova.conf.bak >/etc/nova/nova.conf openstack-config --set /etc/nova/nova.conf DEFAULT enabled_apis osapi_compute,metadata openstack-config --set /etc/nova/nova.conf DEFAULT transport_url rabbit://openstack:RABBIT_PASS@controller openstack-config --set /etc/nova/nova.conf DEFAULT auth_strategy keystone openstack-config --set /etc/nova/nova.conf DEFAULT my_ip 10.0.0.11 openstack-config --set /etc/nova/nova.conf DEFAULT use_neutron True openstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver openstack-config --set /etc/nova/nova.conf api_database connection mysql+pymysql://nova:NOVA_DBPASS@controller/nova_api openstack-config --set /etc/nova/nova.conf database connection mysql+pymysql://nova:NOVA_DBPASS@controller/nova openstack-config --set /etc/nova/nova.conf glance api_servers http://controller:9292 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_uri http://controller:5000 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_url http://controller:35357 openstack-config --set /etc/nova/nova.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_type password openstack-config --set /etc/nova/nova.conf keystone_authtoken project_domain_name default openstack-config --set /etc/nova/nova.conf keystone_authtoken user_domain_name default openstack-config --set /etc/nova/nova.conf keystone_authtoken project_name service openstack-config --set /etc/nova/nova.conf keystone_authtoken username nova openstack-config --set /etc/nova/nova.conf keystone_authtoken password NOVA_PASS openstack-config --set /etc/nova/nova.conf oslo_concurrency lock_path /var/lib/nova/tmp openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_host controller openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_userid openstack openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_password RABBIT_PASS openstack-config --set /etc/nova/nova.conf libvirt virt_type qemu openstack-config --set /etc/nova/nova.conf libvirt cpu_mode none openstack-config --set /etc/nova/nova.conf vnc enabled True openstack-config --set /etc/nova/nova.conf vnc vncserver_listen 0.0.0.0 openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address '$my_ip' openstack-config --set /etc/nova/nova.conf vnc novncproxy_base_url http://controller:6080/vnc_auto.html openstack-config --set /etc/nova/nova.conf neutron url http://controller:9696 openstack-config --set /etc/nova/nova.conf neutron auth_url http://controller:35357 openstack-config --set /etc/nova/nova.conf neutron auth_type password openstack-config --set /etc/nova/nova.conf neutron project_domain_name default openstack-config --set /etc/nova/nova.conf neutron user_domain_name default openstack-config --set /etc/nova/nova.conf neutron region_name RegionOne openstack-config --set /etc/nova/nova.conf neutron project_name service openstack-config --set /etc/nova/nova.conf neutron username neutron openstack-config --set /etc/nova/nova.conf neutron password NEUTRON_PASS openstack-config --set /etc/nova/nova.conf neutron service_metadata_proxy True openstack-config --set /etc/nova/nova.conf neutron metadata_proxy_shared_secret METADATA_SECRET 修改myip为实际IP sed -i \"s#my_ip = 10.0.0.11#my_ip = 172.27.0.13#g\" /etc/nova/nova.conf 配置placement sed -i '/\\[placement\\]/d' /etc/nova/nova.conf cat >> /etc/nova/nova.conf 修改/etc/httpd/conf.d/00-nova-placement-api.conf,在之间添加如下代码 = 2.4> Require all granted Order allow,deny Allow from all 同步nova-api数据库 su -s /bin/sh -c \"nova-manage api_db sync\" nova 注册cell0数据库 su -s /bin/sh -c \"nova-manage cell_v2 map_cell0\" nova 创建cell1的cell su -s /bin/sh -c \"nova-manage cell_v2 create_cell --name=cell1 --verbose\" nova 同步nova数据库 su -s /bin/sh -c \"nova-manage db sync\" nova 验证cell0和cell1的注册是否正确 nova-manage cell_v2 list_cells 启动服务 systemctl enable openstack-nova-api.service \\ openstack-nova-consoleauth.service openstack-nova-scheduler.service \\ openstack-nova-conductor.service openstack-nova-novncproxy.service systemctl restart openstack-nova-api.service openstack-nova-consoleauth.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service nova服务注册 openstack service create --name nova --description \"OpenStack Compute\" compute openstack endpoint create --region RegionOne compute public http://controller:8774/v2.1 openstack endpoint create --region RegionOne compute internal http://controller:8774/v2.1 openstack endpoint create --region RegionOne compute admin http://controller:8774/v2.1 openstack service create --name placement --description \"Placement API\" placement openstack endpoint create --region RegionOne placement public http://controller:8778 openstack endpoint create --region RegionOne placement internal http://controller:8778 openstack endpoint create --region RegionOne placement admin http://controller:8778 验证控制节点服务 openstack host list 计算节点 1.配置yum源 rm -f /etc/yum.repos.d/*.repo curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo 更新 cat > /etc/yum.repos.d/cloud.repo /etc/yum.repos.d/CentOS-Virt.repo 2.安装openstack客户端 yum install python-openstackclient -y yum install openstack-selinux -y 3.修改hostname，配置/etc/hosts文件 cat >> /etc/sysconfig/network > /etc/hosts 4.计算节点安装nova计算服务agent端 安装软件 yum install openstack-nova-compute openstack-utils -y 配置 cp /etc/nova/nova.conf{,.bak} grep '^[a-Z\\[]' /etc/nova/nova.conf.bak >/etc/nova/nova.conf openstack-config --set /etc/nova/nova.conf DEFAULT transport_url rabbit://openstack:RABBIT_PASS@controller openstack-config --set /etc/nova/nova.conf DEFAULT auth_strategy keystone openstack-config --set /etc/nova/nova.conf DEFAULT my_ip 10.0.0.31 openstack-config --set /etc/nova/nova.conf DEFAULT use_neutron True openstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver openstack-config --set /etc/nova/nova.conf glance api_servers http://controller:9292 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_uri http://controller:5000 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_url http://controller:35357 openstack-config --set /etc/nova/nova.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_type password openstack-config --set /etc/nova/nova.conf keystone_authtoken project_domain_name default openstack-config --set /etc/nova/nova.conf keystone_authtoken user_domain_name default openstack-config --set /etc/nova/nova.conf keystone_authtoken project_name service openstack-config --set /etc/nova/nova.conf keystone_authtoken username nova openstack-config --set /etc/nova/nova.conf keystone_authtoken password NOVA_PASS openstack-config --set /etc/nova/nova.conf oslo_concurrency lock_path /var/lib/nova/tmp openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_host controller openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_userid openstack openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_password RABBIT_PASS openstack-config --set /etc/nova/nova.conf vnc enabled True openstack-config --set /etc/nova/nova.conf vnc vncserver_listen 0.0.0.0 openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address '$my_ip' openstack-config --set /etc/nova/nova.conf vnc novncproxy_base_url http://controller:6080/vnc_auto.html 调整my_ip sed -i \"s#10.0.0.31#172.27.0.8#g\" /etc/nova/nova.conf 启动 systemctl start libvirtd systemctl enable libvirtd systemctl start openstack-nova-compute systemctl enable openstack-nova-compute 控制节点验证 openstack compute service list 计算节点加入控制节点（控制节点执行） su -s /bin/sh -c \"nova-manage cell_v2 discover_hosts --verbose\" nova Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:06 "},"1.Linux基础/1.6虚拟化/04Ovirt.html":{"url":"1.Linux基础/1.6虚拟化/04Ovirt.html","title":"04Ovirt","keywords":"","body":" Table of Contents generated with DocToc 磁盘阵列 oVirt安装 添加存储 添加主机 添加存储域 创建iso域 上传镜像 新增虚机 磁盘阵列 先安装配置raid10,挂载路径/data 根据磁盘数量创建阵列、推荐raid5 oVirt安装 配置阿里yum源 关闭selinux setenforce 0 sed -i \"s#SELINUX=enforcing#SELINUX=disabled#g\" /etc/selinux/config 安装 ovirt-release42.rpm yum install -y ovirt-release42.rpm 修改ovirt-4.2.repo、ovirt-4.2-dependencies.repo #baseurl调整为baseurl mirrorlist调整为#mirrorlist gpgcheck=1调整为gpgcheck=0 sed -i 's#gpgcheck=1#gpgcheck=0#g' /etc/yum.repos.d/*.repo yum -y update --nogpgcheck yum install firewalld -y yum install -y ovirt-engine --nogpgcheck 启动 systemctl start ovirt-engine systemctl enable ovirt-engine 配置 确保80没被占用,或修改httpd服务端口 参考地址 需要开启防火墙，不然配置报错 engine-setup #默认配置 #engine-setup --accept-defaults 修改配置 echo \"SSO_CALLBACK_PREFIX_CHECK=false\" > /etc/ovirt-engine/engine.conf.d/99-sso.conf 重启 systemctl restart ovirt-engine 开放443端口 firewall-cmd --zone=public --add-port=443/tcp --permanent #重新载入 firewall-cmd --reload 登陆 添加存储 计算 => 数据中心 => 新建 添加主机 关闭新建ssh连接确认 sed -i \"s;# StrictHostKeyChecking ask;StrictHostKeyChecking no;g\" /etc/ssh/ssh_config systemctl restart sshd 关闭yum公钥检测 默认使用的时候没问题，通过ovirt使用提示校验公钥失败 sed -i 's#gpgcheck=1#gpgcheck=0#g' /etc/yum.repos.d/*.repo 新增主机 计算 => 主机 => 新建 点击确定前，确保目标主机yum可用，且关闭公钥检测 查看部署日志 tail -200f /var/log/ovirt-engine/engine.log 添加存储域 存储 => 存储域 => 新建 创建iso域 安装配置nfs yum install -y nfs-utils rpcbind systemctl daemon-reload systemctl enable rpcbind systemctl enable nfs-server systemctl start rpcbind systemctl start nfs-server mkdir -p /images echo \"/images 192.168.0.0/16(rw)\">>/etc/exports exportfs -a chmod 777 -R /images 详细教程 创建iso域 上传镜像 ovirt-iso-uploader -v --iso-domain=images upload /root/CentOS-7-x86_64-Minimal-1810.iso 新增虚机 计算 -> 虚拟机 -> 新建 新增磁盘 添加网络 配置内存cpu、时区 配置引导 安装图形化界面客户端 windows下载地址 启动、打开控制台 打开文件、安装操作系统 关机虚机、点击右上角... 创建模板 填写模板信息 从模板机创建虚拟机 计算 -> 模板 -> 创建虚拟机 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:06 "},"1.Linux基础/1.6虚拟化/05iDRAC.html":{"url":"1.Linux基础/1.6虚拟化/05iDRAC.html","title":"05iDRAC","keywords":"","body":" Table of Contents generated with DocToc 安装java 安装java 1.下载虚拟控制台连接文件 2.widows环境下下载安装java1.7 jre1.7下载地址 3.调整java设置 控制面板->程序->Java Java->查看->勾选取消高版本->确定 高级->勾选调试三项->应用确定 4.设置引导 登录iDRAC控制台,配置如下 5.运行jnlp文件 鼠标右键步骤1下载的文件 -> 打开方式 -> 选择jre1.7的javaws.exe文件 允许使用过期版本 控制台日志输出 iDRAC虚拟控制台 6.安装操作系统 添加虚拟介质 添加映射本地iso 执行温引导 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:06 "},"1.Linux基础/1.6虚拟化/06migration.html":{"url":"1.Linux基础/1.6虚拟化/06migration.html","title":"06migration","keywords":"","body":"安装openstack单节点 配置阿里yum镜像源 rm -f /etc/yum.repos.d/* 在线 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo 离线：手动下载后上传至/etc/yum.repos.d/ Centos-7.repo epel-7.repo 关闭防火墙 systemctl disable firewalld --now 关闭selinux setenforce 0 sed -i \"s#SELINUX=enforcing#SELINUX=disabled#g\" /etc/selinux/config 关闭NetworkManager服务 systemctl disable NetworkManager --now 安装依赖 yum update -y yum install python3-devel libffi-devel gcc openssl-devel python3-libselinux python3-pip ansible -y 配置pip源与代理 proxy=http://xxx.xxx.xxx.xxx:8080替换为实际代理，如不需代理删除该配置项 mkdir ~/.pip cat >> ~/.pip/pip.conf 安装kolla-ansible pip3 install -U pip pip3 install kolla-ansible 安装docker yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum makecache fast yum -y install docker-ce systemctl enable docker --now 配置镜像加速 yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum makecache fast yum -y install docker-ce systemctl enable docker --now 拷贝配置 mkdir -p /etc/kolla cp -r /usr/local/share/kolla-ansible/etc_examples/kolla/* /etc/kolla cp /usr/local/share/kolla-ansible/ansible/inventory/* /etc/kolla 修改hostname添加解析 hostnamectl set-hostname kolla echo '192.168.1.6 kolla' >> /etc/hosts 调整配置 生成/etc/kolla/passwords.yml配置项密码 /usr/local/bin/kolla-genpwd 修改keystone_admin_password值 vi /etc/kolla/passwords.yml 找到keystone_admin_password，替换值为Admin@123 调整网络配置 em1为宿主机网卡名称 sed -i \"s;#network_interface: \"eth0\";network_interface: \"em1\";g\" /etc/kolla/globals.yml sed -i \"s;#enable_haproxy: \\\"yes\\\";enable_haproxy: \\\"no\\\";g\" /etc/kolla/globals.yml 192.168.1.6为宿主机IP sed -i \"s;#kolla_internal_vip_address: \\\"10.10.10.254\\\";kolla_internal_vip_address: \\\"192.168.1.6\\\";g\" /etc/kolla/globals.yml 克隆 git clone https://opendev.org/openstack/openstack-ansible 新增yum源 openstack-train.repo cat > /etc/yum.repos.d/openstack-train.repo kvm-common.repo cat > /etc/yum.repos.d/kvm-common.repo 配置yum代理 适用于主机通过代理访问互联网场景 以下变量注意替换 username: 代理用户名 password: 代理用户密码 proxy_host: 代理IP地址 proxy_port: 代理端口 echo \"proxy=http://username:password@proxy_host:proxy_port\" >> /etc/yum.conf 重建yum缓存 yum clean all yum makecache 安装OpenStack-packstack软件包 yum -y install openstack-packstack 回退leatherman版本 yum downgrade leatherman -y 生成默认配置 packstack --gen-answer-file=~/openstack.txt 修改~/openstack.txt配置 修改内容如下 sed -i \"s#CONFIG_SWIFT_INSTALL=y#CONFIG_SWIFT_INSTALL=n#g\" ~/openstack.txt sed -i \"s#CONFIG_AODH_INSTALL=y#CONFIG_AODH_INSTALL=n#g\" ~/openstack.txt sed -i \"s#CONFIG_NEUTRON_ML2_TYPE_DRIVERS=geneve,flat#CONFIG_NEUTRON_ML2_TYPE_DRIVERS=vxlan,flat#g\" ~/openstack.txt sed -i \"s#CONFIG_NEUTRON_ML2_TENANT_NETWORK_TYPES=geneve#CONFIG_NEUTRON_ML2_TENANT_NETWORK_TYPES=vxlan#g\" ~/openstack.txt sed -i \"s#CONFIG_NEUTRON_ML2_MECHANISM_DRIVERS=ovn#CONFIG_NEUTRON_ML2_MECHANISM_DRIVERS=openvswitch#g\" ~/openstack.txt 手动修改项 CONFIG_COMPUTE_HOSTS=192.168.19 #计算节点ip地址 CONFIG_NEUTRON_ML2_FLAT_NETWORKS=physnet1 #flat网络这边要设置物理网卡名字 CONFIG_NEUTRON_L2_AGENT=openvswitch #L2网络的代理模式,也可选择linuxbridge CONFIG_NEUTRON_OVS_BRIDGE_MAPPINGS=physnet1:br-ex #这边要设置物理网卡的名字 CONFIG_NEUTRON_OVS_BRIDGE_IFACES=br-ex:eth0 #这边br-ex:eth0是网络节点的nat网卡，到时候安装完毕之后IP地址会飘到这个上 更改主机密码（123456需要替换） sed -i -r 's/(.+_PW)=.+/\\1=123456/' openstack.txt 备份配置 cp openstack.txt openstack.txt.bak 安装 packstack --answer-file=~/openstack.txt 如出现如下错误 ... Applying Puppet manifests [ ERROR ] ... 执行以下语句 sed -i \"s;#baseurl;baseurl;g\" /etc/yum.repos.d/*.repo sed -i \"s;mirrorlist=;#mirrorlist=;g\" /etc/yum.repos.d/*.repo rm -f /etc/yum.repos.d/CentOS-* rm -f * rm -rf /var/tmp/packstack/ packstack --allinone 错误二 ... Error: (pymysql.err.OperationalError) (1045, u\"Access denied for user 'nova'@'192.168.1.6' (using password: YES)\") (Background on this error at: http://sqlalche.me/e/e3q8) ... 执行以下命令 su -s /bin/sh -c \"nova-manage cell_v2 create_cell --name=cell1 --verbose\" nova 获取admin登录口令 [root@localhost ~]# cat keystonerc_admin|grep OS_PASSWORD export OS_PASSWORD='ab2529c81120445a' Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:06 "},"1.Linux基础/1.6虚拟化/07磁盘分区相关.html":{"url":"1.Linux基础/1.6虚拟化/07磁盘分区相关.html","title":"07磁盘分区相关","keywords":"","body":"突破2T上限 修改分区表类型 parted /dev/sdb mklabel gpt 分区 fdisk /dev/sdb 格式化 mkfs.ext4 /dev/sdb1 挂载 mount /dev/sdb1 /data1 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-09 16:45:01 "},"1.Linux基础/1.6虚拟化/08网卡bond.html":{"url":"1.Linux基础/1.6虚拟化/08网卡bond.html","title":"08网卡bond","keywords":"","body":"配置网卡Bond模式 网卡1配置 tee /etc/sysconfig/network-scripts/ifcfg-eno1 网卡2配置 tee /etc/sysconfig/network-scripts/ifcfg-ens4f0 bond0配置 tee /etc/sysconfig/network-scripts/ifcfg-bond0 重启网络 systemctl restart network Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-23 09:56:46 "},"1.Linux基础/1.7存储/ceph/":{"url":"1.Linux基础/1.7存储/ceph/","title":"ceph","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 15:43:27 "},"1.Linux基础/1.7存储/ceph/01核心概念/01简述.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/01简述.html","title":"01简述","keywords":"","body":"说明 ceph相关于nfs性能较高，且能提供块存储、文件系统、对象存储网关三种类型存储。 ceph集成部署较为简单、且成本较低(相对于nas、SAN) ceph可利用现有服务器资源（裸金属服务器），而不用额外购买存储设备。构建高性能、高可用的统一存储系统，供公司内部适用 主流云计算系统（Openstack等）一般选取ceph作为后端存储，说明其优异性 ceph开源，相比商用软件有天然优势 Ceph解析 Ceph是一个提供对象存储、块存储和文件存储的统一存储系统。 Ceph是高度可靠、易于管理和免费的。Ceph的强大功能可以改变企业IT基础设施和管理大量数据的能力。 Ceph提供了超强的可伸缩性--数以千计的客户端访问pb到eb的数据. Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 15:34:57 "},"1.Linux基础/1.7存储/ceph/01核心概念/02相关术语.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/02相关术语.html","title":"02相关术语","keywords":"","body":"Ceph术语 本术语表中的术语旨在解释说明Ceph现有的技术术语。 cephx：Ceph认证协议，操作类似Kerberos，但不存在单点故障情况 Ceph平台：所有Ceph软件 Ceph堆栈：Ceph的两种或多种组件的集合 集群映射：包括MON映射、OSD映射、PG映射、MDS映射和CRUSH映射 Ceph对象存储：由Ceph存储集群和Ceph对象网关组成 RGW：Ceph的S3/Swift网关组件 RBD：Ceph块存储组件 Ceph块存储：用于与librbd、管理程序（如QEMU或Xen）和管理程序抽象层（如libvirt）结合使用 Ceph文件系统：Ceph的POSIX文件系统组件 OSD：物理或逻辑存储单元(如LUN)。有时，Ceph用户使用术语OSD来指代Ceph OSD守护进程，尽管正确的术语是Ceph OSD Ceph OSD：Ceph OSD守护进程，与逻辑盘(OSD)交互。 OSD id：定义OSD的整数。它由监视器生成，作为创建新OSD的一部分 OSD fsid：这是一个唯一标识符，用于进一步提高OSD的唯一性，它可以在OSD路径中的osd_fsid文件中找到。这个fsid术语可以与uuid互换使用 OSD uuid：与OSD fsid一样，这是OSD唯一标识符，可以与OSD fsid互换使用 bluestore：OSD BlueStore是OSD守护程序（kraken和更新版本）的新后端。与filestore不同，它直接将对象存储在Ceph块设备上，而不需要任何文件系统接口 filestore：OSD守护进程的后端，需要日志记录、将文件写入文件系统 MON：Ceph监控软件 MGR：Ceph管理器软件，收集集群所有状态 MDS：Ceph元数据服务 Ceph Client：可以访问Ceph存储集群的Ceph组件的集合。组件包括Ceph对象网关、Ceph块设备、Ceph文件系统以及它们相应的库、内核模块、用户空间文件系统 Ceph Kernel Modules：内核模块的集合，可以用来与Ceph系统交互(例如，Ceph.ko, rbd.ko)。 Ceph Client Libraries：可用于与Ceph系统组件交互的库集合 Ceph Release：任何明显编号的Ceph版本 Ceph Point Release：任何只包含错误或安全修正的特别发行版 Ceph Interim Release：Ceph的版本尚未通过质量保证测试，但可能包含新特性 Ceph Release Candidate：Ceph的一个主要版本，已经经历了最初的质量保证测试，并且已经为beta测试做好了准备 Ceph Stable Release：Ceph的主要版本，其中所有来自前一个临时版本的特性都成功地通过了质量保证测试 Teuthology：在Ceph上执行脚本测试的软件集合 CRUSH：可伸缩散列下的受控复制(Controlled Replication Under Scalable Hashing)。这是Ceph用来计算对象存储位置的算法 CRUSH rule：适用于特定池的压缩数据放置规则 Pools：池是用于存储对象的逻辑分区 systemd oneshot：一种systemd类型，其中一个命令定义在ExecStart中，它将在完成时退出(它不打算后台化) LVM tags：用于LVM卷和组的可扩展元数据。它用于存储关于设备及其与osd的关系的特定于`c Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 12:07:58 "},"1.Linux基础/1.7存储/ceph/01核心概念/03存储流程解析.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/03存储流程解析.html","title":"03存储流程解析","keywords":"","body":"Ceph存储集群 Ceph提供了一个基于RADOS(一种可扩展的、可靠的pb级存储集群存储服务)的无限可扩展的Ceph存储集群. Ceph存储集群由两种类型的守护进程组成: Ceph Monitor（mon） Ceph OSD Daemon（OSD） 其中Ceph Monitor维护集群映射的主副本。多节点Ceph Monitor确保了Ceph Monitor守护进程失败时的高可用性。 Ceph客户端从Ceph Monitor获取集群信息 ceph osd守护进程检查自己的状态和其他OSD的状态，并向Ceph Monitor上报。 Ceph客户端和每个ceph osd守护进程使用CRUSH算法高效地计算数据位置信息，而不必依赖于中央查找表。 Ceph的高级特性包括通过librados提供到Ceph存储集群的本地接口，以及建立在librados之上的许多服务接口。 Ceph数据存储流程 Ceph存储集群从Ceph客户端接收数据——无论是通过一个Ceph块设备、Ceph对象存储、Ceph文件系统还是使用librados创建的自定义实现——它将数据作为对象存储。 每个对象都对应于文件系统中的一个文件，文件系统存储在对象存储设备上。ceph osd守护进程处理存储磁盘的读写操作。 ceph osd守护进程将所有数据作为对象存储在一个平面命名空间中(没有目录层次结构)。 对象具有标识符、二进制数据和由一组名称/值对组成的元数据。语义完全由Ceph客户端决定。 例如，CephFS使用元数据存储文件属性，如文件所有者、创建日期、最后修改日期等。其中，对象ID全局唯一。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 15:43:27 "},"1.Linux基础/1.7存储/ceph/01核心概念/04高可用性.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/04高可用性.html","title":"04高可用性","keywords":"","body":"Ceph的可伸缩性和高可用性 在传统的架构中，客户端与一个集中的组件(例如，网关、代理、API、facade等)通信，该组件充当一个进入复杂子系统的单一入口点。 这对性能和可伸缩性都施加了限制，同时引入了单点故障(例如，如果集中式组件宕机，整个系统也宕机） Ceph消除了集中式网关，使客户端可以直接与ceph osd守护进程交互。ceph osd守护进程在其他Ceph节点上创建对象副本，以确保数据的安全性和高可用性。 Ceph还使用一组mon来确保高可用性。为了消除集中化，Ceph使用了一种称为CRUSH的算法 mon高可用 在Ceph客户机能够读写数据之前，它们必须访问Ceph mon以获取集群映射的最新副本。Ceph存储集群可以使用单个mon进行操作;然而，这引入了单点故障(即，如果监视器出现故障，Ceph客户机就无法读写数据)。 为了增加可靠性和容错性，Ceph支持mon集群。在一个监视器集群中，延迟和其他故障可能导致一个或多个监视器落后于集群的当前状态。由于这个原因，Ceph必须在关于集群状态的各个监视器实例之间达成一致。Ceph总是使用大多数监视器(例如，1、2:3、3:5、4:6等)和Paxos算法来在监视器之间建立关于集群当前状态的共识 即部署多点ceph mon 规避单点故障 身份认证高可用性 为了识别用户并防止中间人攻击，Ceph提供了cephx身份验证系统来验证用户和守护进程。（cephx协议不处理传输中的数据加密(例如，SSL/TLS)或静止时的加密。） Cephx使用共享密钥进行身份验证，这意味着客户端和监控集群都拥有客户端密钥的副本。身份验证协议是这样的，双方能够向对方证明他们有一个密钥的副本，而不实际暴露它。这提供了相互的身份验证，这意味着集群确定用户拥有密钥，用户也确定集群拥有密钥的副本 Ceph的一个关键的可伸缩性特性是避免对Ceph对象存储的集中接口，这意味着Ceph客户端必须能够直接与OSD交互。为了保护数据，Ceph提供了其cephx身份验证系统，该系统对操作Ceph客户端的用户进行身份验证。cephx协议的操作方式与Kerberos类似 要使用cephx，管理员必须首先设置用户。在下面的图表中client.admin从命令行调用ceph auth get-or-create-key来生成用户名和密钥。 Ceph的auth子系统生成用户名和密钥，将一个副本存储在监视器中，并将用户的密钥传输回client.admin。 这意味着客户端和监视器共享一个密钥来使用cephx. 为了使用监视器进行身份验证，客户端将用户名传递给监视器，监视器生成一个会话密钥并使用与用户名相关联的密钥对其进行加密。 然后，监视器将加密的票据传输回客户端。随后，客户机使用共享密钥解密，以检索会话密钥。会话密钥标识当前会话的用户。 然后客户端使用会话密钥签名的用户请求票据。监视器生成一个票据，用用户的密钥对其加密，并将其传回客户机。 客户端解密票据，并使用它对整个集群中的OSDs和元数据服务器的请求进行签名 cephx协议对客户端机器和Ceph服务器之间的通信进行身份验证。 在初始身份验证之后，客户机和服务器之间发送的每个消息都使用票据进行签名， 监视器、OSD和元数据服务器可以使用它们的共享秘密来验证该票据 这种身份验证提供的保护在Ceph客户端和Ceph服务器主机之间。身份验证没有扩展到Ceph客户端之外。 如果用户从远程主机访问Ceph客户端，Ceph身份验证不应用于用户的主机和客户端主机之间的连接 分级缓存 缓存层为Ceph客户端提供了更好的I/O性能，用于存储在备份存储层中的数据子集。 分级缓存包括创建一个作为缓存层的相对快速/昂贵的存储设备(例如，固态驱动器)池，以及一个配置为纠错码或相对较慢/便宜的设备作为经济存储层的后备池。 Ceph objecter处理放置对象的位置，而分层代理确定何时将对象从缓存刷新到后备存储层。因此，缓存层和后备存储层对Ceph客户端是完全透明的 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 15:38:39 "},"1.Linux基础/1.7存储/ceph/01核心概念/05存储类型.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/05存储类型.html","title":"05存储类型","keywords":"","body":"Ceph存储类型 Ceph客户端包括许多服务接口: 块设备：Ceph块设备(又称RBD)服务提供可调整大小、精简配置的块设备，并提供快照和克隆。Ceph跨集群划分块设备以获得高性能。Ceph既支持内核对象(KO)，也支持直接使用librbd的QEMU管理程序——避免了虚拟化系统的内核对象开销 对象存储：Ceph对象存储服务(简称RGW)提供RESTful api，兼容Amazon S3和OpenStack Swift接口 文件系统：Ceph文件系统(cepphfs)服务提供一个兼容POSIX的文件系统，可以挂载，也可以作为用户空间中的文件系统(FUSE)使用。 1.Ceph块存储 块是字节序列（例如，512字节的数据块）。基于块的存储接口是使用旋转介质（如硬盘、CD、软盘，甚至传统的9磁道磁带）存储数据的最常用方法。 块设备接口的普遍性使得虚拟块设备成为与Ceph这样的海量数据存储系统交互的理想候选设备 Ceph块设备是精简配置的，可调整大小，并在Ceph集群中的多个OSD上存储数据条带化。 Ceph块设备利用RADOS功能，如快照、复制和一致性。 Ceph的RADOS块设备（RBD）使用内核模块或librbd库与OSD交互 Ceph的block设备以无限的可扩展性向内核模块或kvm（如QEMU）以及为基于云的计算系统（如OpenStack和CloudStack）提供高性能存储， 这些系统依赖libvirt和QEMU与Ceph block设备集成。您可以使用同一集群同时操作Ceph-RADOS网关、CephFS文件系统和Ceph-block设备。 Ceph块设备在Ceph存储集群中的多个对象上划分块设备映像，每个对象映射到一个放置组并分布，放置组分布在整个集群中不同的ceph osd守护进程上。 精简配置的可快照Ceph块设备是虚拟化和云计算的一个有吸引力的选择。 在虚拟机场景中，人们通常在QEMU/KVM中部署带有rbd网络存储驱动程序的Ceph块设备，其中服务端使用librbd向客户端提供块设备服务。 许多云计算栈使用libvirt与管理程序集成。您可以通过QEMU和libvirt使用瘦配置的Ceph块设备来支持OpenStack和CloudStack以及其他解决方案。 2.Ceph文件系统 Ceph文件系统(cepphfs)提供了posix兼容的文件系统作为一种服务，它是在基于对象的Ceph存储集群之上分层的。 cepfs文件映射到Ceph存储集群中存储的对象。Ceph客户端将cepfs文件系统挂载为内核对象或用户空间中的文件系统(FUSE) Ceph文件系统服务包括部署在Ceph存储集群中的Ceph元数据服务器(MDS)。 MDS的目的是将所有文件系统元数据(目录、文件所有权、访问模式等)存储在高可用性Ceph元数据服务器中，元数据驻留在内存中。 MDS(称为Ceph - MDS的守护进程)存在的原因是，简单的文件系统操作，如列出目录或更改目录(ls、cd)，会给ceph osd守护进程带来不必要的负担。 因此，将元数据从数据中分离出来意味着Ceph文件系统可以提供高性能服务，而不会对Ceph存储集群造成负担。 cepfs将元数据与数据进行分离，元数据存储在MDS中，文件数据存储在Ceph存储集群中的一个或多个对象中。 Ceph文件系统旨在与POSIX兼容。为了实现高可用性或可伸缩性，ceph-mds可以作为单个进程运行，也可以将其分发到多个物理机器。 高可用：额外的ceph-mds实例可以是备用的，随时准备接管任何失效的active ceph-mds的职责。这很容易，因为包括日志在内的所有数据都存储在RADOS上。该转换由ceph-mon自动触发 可扩展：多个ceph mds实例可以处于活动状态，它们将目录树拆分为子树（以及单个繁忙目录的碎片），从而有效地平衡所有活动服务器之间的负载 3.Ceph 对象存储 Ceph对象存储守护进程radosgw是一个FastCGI服务，它提供了一个RESTful的HTTP API来存储对象和元数据。 它以自己的数据格式在Ceph存储集群之上分层，并维护自己的用户数据库、身份验证和访问控制。 RADOS网关采用统一的命名空间，既可以使用OpenStack swift接口，也可以使用Amazon s3接口。 例如，一个应用使用s3兼容的API写入数据，另一个应用使用swift兼容的API读取数据 S3/Swift对象和存储集群对象对比： Ceph的Object Storage使用Object这个术语来描述它存储的数据。 S3和Swift对象与Ceph写入Ceph存储集群的对象不同。 Ceph对象存储对象映射到Ceph存储集群对象。 S3和Swift对象不一定与存储集群中存储的对象以1:1的方式对应。 S3或Swift对象有可能映射到多个Ceph对象。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 15:23:34 "},"1.Linux基础/1.7存储/ceph/01核心概念/06crush介绍.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/06crush介绍.html","title":"06crush介绍","keywords":"","body":"CRUSH算法介绍 Ceph客户端和ceph osd守护进程都使用CRUSH算法来有效地计算对象的位置信息，而不是依赖于一个中心查找表。 与以前的方法相比，CRUSH提供了更好的数据管理机制，可以实现大规模的数据管理。 CRUSH使用智能数据复制来确保弹性，这更适合于超大规模存储。 集群映射 Ceph依赖于Ceph客户端和ceph osd守护进程了解集群拓扑，其中包括5个映射，统称为“集群映射”： Monitor映射: 包含集群fsid、每个监视器的位置、名称、地址和端口、映射创建的时间，以及它最后一次修改时间。 要查看监视器映射，执行ceph mon dump。 [root@ceph01 ~]# ceph mon dump dumped monmap epoch 2 epoch 2 fsid b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294 last_changed 2021-02-22 14:36:08.199609 created 2021-02-22 14:27:26.357269 min_mon_release 14 (nautilus) 0: [v2:192.168.1.69:3300/0,v1:192.168.1.69:6789/0] mon.ceph01 1: [v2:192.168.1.70:3300/0,v1:192.168.1.70:6789/0] mon.ceph02 2: [v2:192.168.1.71:3300/0,v1:192.168.1.71:6789/0] mon.ceph03 OSD映射:包含集群的fsid，映射创建和最后修改的时间，池的列表，副本大小，PG号，OSD的列表和状态(如up, in)。 执行ceph osd dump命令，查看OSD映射 [root@ceph01 ~]# ceph osd dump epoch 1 fsid b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294 created 2021-02-22 14:27:48.482130 modified 2021-02-22 14:27:48.482130 flags sortbitwise,recovery_deletes,purged_snapdirs,pglog_hardlimit crush_version 1 full_ratio 0.95 backfillfull_ratio 0.9 nearfull_ratio 0.85 require_min_compat_client jewel min_compat_client jewel require_OSD_release nautilus max_OSD 0 PG Map：包含PG版本、它的时间戳、最后一个OSD Map epoch、完整比率，以及每个放置组的详细信息，例如PG ID、Up Set、Acting Set、PG的状态（例如active+clean），以及每个池的数据使用统计信息 CRUSH Map:包含一个存储设备列表，故障域层次结构(例如，设备、主机、机架、行、房间等)，以及存储数据时遍历层次结构的规则。执行ceph osd getcrushmap -o {filename};然后，通过执行crushtool -d {comp-crushmap-filename} -o {decomp-crushmap-filename}来反编译它。 使用cat查看反编译后的映射。 MDS Map:包含当前MDS Map的epoch、Map创建的时间和最后一次修改的时间。它还包含存储元数据的池、元数据服务器的列表，以及哪些元数据服务器已经启动和运行. 每个映射维护其操作状态更改的迭代历史。Ceph监视器维护集群映射的主副本，包括集群成员、状态、变更和Ceph存储集群的总体运行状况 crush class 说明 从luminous版本ceph新增了一个功能crush class，这个功能又可以称为磁盘智能分组。因为这个功能就是根据磁盘类型自动的进行属性的关联，然后进行分类。无需手动修改crushmap，极大的减少了人为的操作。 ceph中的每个OSD设备都可以选择一个class类型与之关联，默认情况下，在创建OSD的时候会自动识别设备类型，然后设置该设备为相应的类。通常有三种class类型：hdd，ssd，nvme。 查看集群OSD crush tree(ceph01节点执行) [root@ceph01 ~]# ceph osd crush tree --show-shadow ID CLASS WEIGHT TYPE NAME -2 ssd 25.76233 root default~ssd -4 ssd 9.75183 host ceph01~ssd 0 ssd 0.87329 OSD.0 1 ssd 0.87329 OSD.1 2 ssd 0.87329 OSD.2 3 ssd 0.87329 OSD.3 4 ssd 0.87329 OSD.4 5 ssd 0.87329 OSD.5 6 ssd 0.87329 OSD.6 7 ssd 0.90970 OSD.7 8 ssd 0.90970 OSD.8 9 ssd 0.90970 OSD.9 10 ssd 0.90970 OSD.10 -6 ssd 8.00525 host ceph02~ssd 11 ssd 0.87329 OSD.11 12 ssd 0.87329 OSD.12 13 ssd 0.87329 OSD.13 14 ssd 0.87329 OSD.14 15 ssd 0.87329 OSD.15 16 ssd 0.90970 OSD.16 17 ssd 0.90970 OSD.17 18 ssd 0.90970 OSD.18 19 ssd 0.90970 OSD.19 -8 ssd 8.00525 host ceph03~ssd 20 ssd 0.87329 OSD.20 21 ssd 0.87329 OSD.21 22 ssd 0.87329 OSD.22 23 ssd 0.87329 OSD.23 24 ssd 0.87329 OSD.24 25 ssd 0.90970 OSD.25 26 ssd 0.90970 OSD.26 27 ssd 0.90970 OSD.27 28 ssd 0.90970 OSD.28 修改nvme类型class ceph osd crush rm-device-class 7 ceph osd crush set-device-class nvme OSD.7 ceph osd crush rm-device-class 8 ceph osd crush set-device-class nvme OSD.8 ceph osd crush rm-device-class 9 ceph osd crush set-device-class nvme OSD.9 ceph osd crush rm-device-class 10 ceph osd crush set-device-class nvme OSD.10 ceph osd crush rm-device-class 16 ceph osd crush set-device-class nvme OSD.16 ceph osd crush rm-device-class 17 ceph osd crush set-device-class nvme OSD.17 ceph osd crush rm-device-class 18 ceph osd crush set-device-class nvme OSD.18 ceph osd crush rm-device-class 19 ceph osd crush set-device-class nvme OSD.19 ceph osd crush rm-device-class 25 ceph osd crush set-device-class nvme OSD.25 ceph osd crush rm-device-class 26 ceph osd crush set-device-class nvme OSD.26 ceph osd crush rm-device-class 27 ceph osd crush set-device-class nvme OSD.27 ceph osd crush rm-device-class 28 ceph osd crush set-device-class nvme OSD.28 查看集群OSD crush tree [root@ceph01 ~]# ceph osd crush tree --show-shadow ID CLASS WEIGHT TYPE NAME -12 nvme 10.91638 root default~nvme -9 nvme 3.63879 host ceph01~nvme 7 nvme 0.90970 OSD.7 8 nvme 0.90970 OSD.8 9 nvme 0.90970 OSD.9 10 nvme 0.90970 OSD.10 -10 nvme 3.63879 host ceph02~nvme 16 nvme 0.90970 OSD.16 17 nvme 0.90970 OSD.17 18 nvme 0.90970 OSD.18 19 nvme 0.90970 OSD.19 -11 nvme 3.63879 host ceph03~nvme 25 nvme 0.90970 OSD.25 26 nvme 0.90970 OSD.26 27 nvme 0.90970 OSD.27 28 nvme 0.90970 OSD.28 -2 ssd 14.84595 root default~ssd -4 ssd 6.11304 host ceph01~ssd 0 ssd 0.87329 OSD.0 1 ssd 0.87329 OSD.1 2 ssd 0.87329 OSD.2 3 ssd 0.87329 OSD.3 4 ssd 0.87329 OSD.4 5 ssd 0.87329 OSD.5 6 ssd 0.87329 OSD.6 -6 ssd 4.36646 host ceph02~ssd 11 ssd 0.87329 OSD.11 12 ssd 0.87329 OSD.12 13 ssd 0.87329 OSD.13 14 ssd 0.87329 OSD.14 15 ssd 0.87329 OSD.15 -8 ssd 4.36646 host ceph03~ssd 20 ssd 0.87329 OSD.20 21 ssd 0.87329 OSD.21 22 ssd 0.87329 OSD.22 23 ssd 0.87329 OSD.23 24 ssd 0.87329 OSD.24 crush pool使用 创建crush rule # ceph osd crush rule create-replicated # 创建`class` 为`SSD`的rule ceph osd crush rule create-replicated SSD_rule default host ssd 创建资源池 #ceph osd pool create {pool_name} {pg_num} [{pgp_num}] ceph osd pool create ssd-pool 256 256 查看ssd-poolcrush rule [root@ceph01 ~]# ceph osd pool get ssd-pool crush_rule crush_rule: replicated_rule 设置ssd-poolcrush rule为SSD_rule #ceph osd pool set crush_rule ceph osd pool set ssd-pool crush_rule SSD_rule 查看ssd-poolcrush rule [root@ceph01 ~]# ceph osd pool get ssd-pool crush_rule crush_rule: SSD_rule 设置ssd-pool配额 #osd pool set-quota max_objects|max_bytes 设置最大对象数10 ceph osd pool set-quota ssd-pool max_objects 10 设置存储大小为1G ceph osd pool set-quota ssd-pool max_bytes 1G 扩容ssd-pool配额 #osd pool set-quota max_objects|max_bytes 设置最大对象数20 ceph osd pool set-quota ssd-pool max_objects 20 设置存储大小为2G ceph osd pool set-quota ssd-pool max_bytes 2G 修改ssd-pool为ssd-demo-pool ceph osd pool rename ssd-pool ssd-demo-pool 最佳实践：创建不同类型存储池（基于HDD、SSD，提供不同场景使用） Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 15:34:57 "},"1.Linux基础/1.7存储/ceph/01核心概念/07ceph协议.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/07ceph协议.html","title":"07ceph协议","keywords":"","body":"Ceph协议 Ceph客户端使用原生协议与Ceph存储集群进行交互，Ceph将这个功能打包到librados库中。 下图描述了基本架构： Ceph协议与librados 现代应用程序需要一个具有异步通信功能的简单对象存储接口。 Ceph存储集群提供了一个具有异步通信能力的简单对象存储接口。 该接口提供了对整个集群中的对象的直接、并行访问。如： 池操作 快照和写时复制克隆 读/写对象-创建或删除-整个对象或字节范围-追加或截断 创建/设置/获取/删除XATTRs 创建/设置/获取/删除键值对 复合运算与双ack语义 对象类 对象观测通知 客户端可以持久性观测一个对象，并保持与主OSD的会话处于打开状态。 客户端可以向所有观察者发送通知消息和有效负载，并在观察者收到通知时接收通知。 这使客户端能够将任何对象用作同步/通信通道 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 15:43:27 "},"1.Linux基础/1.7存储/ceph/01核心概念/08数据分段.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/08数据分段.html","title":"08数据分段","keywords":"","body":"数据分段 存储设备有吞吐量限制，这会影响性能和可伸缩性。因此，存储系统通常支持跨多个存储设备分段存储顺序信息，以提高吞吐量和性能。 RAID是最常见的数据分条形式。与Ceph的条带化最相似的RAID类型是raid0，或“条带卷”。Ceph的分条提供了RAID 0分条的吞吐量，n-way RAID镜像的可靠性和更快的恢复。 Ceph存储集群中Ceph存储的对象没有条带化。Ceph对象存储、Ceph块设备和Ceph文件系统在多个Ceph存储集群对象上条带化它们的数据。 通过librados直接写入Ceph存储集群的Ceph客户端必须为自己执行条带化（和并行I/O）以获得这些好处。 最简单的Ceph条带格式涉及1个对象的条带计数。Ceph客户端将条带单元写入Ceph存储群集对象，直到该对象达到最大容量，然后为额外的数据条带创建另一个对象。 最简单的条带化形式对于小块设备图像、S3或Swift对象和cepfs文件就足够了。 然而，这种简单的形式并没有最大限度地利用Ceph在放置组之间分发数据的能力，因此并没有极大地提高性能。下图描述了条带化的最简单形式： 如果预期会有较大的镜像、较大的S3或Swift对象（例如，视频）或较大的cepfs目录，那么通过在对象集中的多个对象上对客户端数据进行条带化，会有相当大的读/写性能改进。 当客户端并行地将条带单元写入相应的对象时，会显著提升写入性能。由于对象被映射到不同的放置组并进一步映射到不同的OSD，因此每次写入都以最大的写入速度并行进行。 对单个磁盘的写入将受到磁头移动（例如，每次寻道6ms）和该设备带宽（例如，100MB/s）的限制。 通过将写操作扩展到多个对象（映射到不同的放置组和OSD），Ceph可以减少每个驱动器的寻道次数，并结合多个驱动器的吞吐量，以实现更快的写（或读）速度。 在下面的图中，客户端数据在由4个对象组成的对象集(下图中的对象集1)上进行分条， 其中第一个分条单元是对象0中的分条单元0，第四个分条单元是对象3中的分条单元3。 写入第四个分条后，客户端确定对象集是否已满。 如果对象集未满，客户端将开始再次向第一个对象(下图中的对象0)写入条带。 如果对象集已满，客户端将创建一个新的对象集(下图中的对象集2)， 并开始写入新对象集(下图中的对象4)中的第一个对象中的第一个条带(条带单元16)。 决定Ceph条带化数据的方式的三个重要变量: 对象大小：Ceph存储集群中的对象具有最大可配置大小（例如，2MB、4MB等）。对象大小应该足够大以容纳多个条带单位，并且应该是条带单位的倍数 条带宽度：条带具有可配置的单元大小（例如64kb）。Ceph客户端将要写入对象的数据划分为大小相等的条带单元，但最后一个条带单元除外。条带宽度应该是对象大小的一小部分，这样一个对象可以包含许多条纹单位。 条带计数：Ceph客户端在由条带计数确定的一系列对象上写入条带单元序列。这一系列对象被称为对象集。Ceph客户端写入对象集中的最后一个对象后，它返回到对象集中的第一个对象 一旦Ceph客户端将数据分条到条带单元并将条带单元映射到对象，Ceph的CRUSH算法将对象映射到放置组，并将放置组映射到ceph osd守护进程，然后将对象作为文件存储在存储磁盘上 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 15:43:41 "},"1.Linux基础/1.7存储/ceph/01核心概念/09集群动态管理.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/09集群动态管理.html","title":"09集群动态管理","keywords":"","body":"动态集群管理 每个池（pool）都有许多放置组（PG），CRUSH动态地将PGs映射到OSDs。当Ceph客户端存储对象时，CRUSH将每个对象映射到放置组 将对象映射到放置组将在ceph osd守护进程和Ceph客户端之间创建一个间接层。 Ceph存储集群必须能够增长(或收缩)，并重新平衡动态存储对象的位置。 如果Ceph客户端“知道”哪个ceph osd守护进程有哪个对象，那么Ceph客户端和ceph osd守护进程之间就会产生一个紧密耦合。 相反，CRUSH算法将每个对象映射到一个放置组，然后将每个放置组映射到一个或多个ceph osd守护进程。 这一间接层允许Ceph在新的ceph osd守护进程和底层OSD设备上线时动态重新平衡。下图描述了CRUSH如何将对象映射到放置组，以及将放置组映射到OSD。 有了集群映射的副本和CRUSH算法，客户端就可以准确地计算出在读写特定对象时应该使用哪个OSD。 计算放置组ID 当Ceph客户端绑定到Ceph mon时，它将检索集群映射的最新副本。通过集群映射，客户获取集群中的所有mon、OSD和mds信息。但是，它对对象位置一无所知。 计算过程 。这很简单:Ceph将数据存储在命名池中(例如，“liverpool”)。 当客户端想要存储一个命名对象(例如，“john”、“paul”、“george”、“ringo”等)时，它使用对象名、哈希码、池中的PGs数量和池名计算放置组。Ceph客户端使用以下步骤计算PG id。 1、客户端输入对象ID和pool 2、Ceph获取对象ID并对其进行哈希运算 3、Ceph计算pg数的哈希模,（例如，58）获取PG ID 4、Ceph获取给定池名的池ID（例如，“liverpool”=4） 5、Ceph将池ID前置到PG ID(例如，4.58)。 重新平衡 当你将ceph osd守护进程添加到Ceph存储集群时，集群映射会随着新的OSD更新。再次计算PG id，这将更改集群映射。 下面的图描述了重新平衡的过程(虽然很粗略，因为在大型集群中影响更小)， 其中一些(但不是所有)pg从现有OSD (OSD 1和OSD 2)迁移到新的OSD (OSD 3)。即使在重新平衡时，崩溃也是稳定的。 许多放置组保持原来的配置，每个OSD增加了一些容量，因此在重新平衡完成后，新OSD上不会出现负载峰值。 数据一致性 作为维护数据一致性和清洁度的一部分，ceph osd还可以清理放置组中的对象。 也就是说，ceph osd可以将一个放置组中的对象元数据与其存储在其他OSD中的放置组中的副本进行比较。 清理（通常每天执行）捕获OSD错误或文件系统错误。 OSD还可以通过逐位比较对象中的数据来执行更深入的清理。深度清理（通常每周执行一次）会在磁盘上发现在轻度清理时不明显的坏扇区。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 15:37:17 "},"1.Linux基础/1.7存储/ceph/01核心概念/10对比raid.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/10对比raid.html","title":"10对比raid","keywords":"","body":"对比raid RAID(Redundant Array of Independent Disks)即独立冗余磁盘阵列，是一种把多块独立的硬盘（物理硬盘）按不同的方式组合起来形成一个硬盘组（逻辑硬盘），让用户认为只有一个单个超大硬盘，从而提供比单个硬盘更高的存储性能和提供数据备份技术 RAID 漫长的重建过程，而且在重建过程中，不能有第二块盘损坏，否则会引发更大的问题； 备用盘增加TCO ，作为备用盘，当没有硬盘故障时，就会一直闲置的 不能保证两块盘同时故障后，数据的可靠性 在重建结束前，客户端无法获取到足够的IO资源 无法避免网络、服务器硬件、操作系统、电源等故障 Ceph 为了保证可靠性，采用了数据复制的方式，这意味着不再需要RAID，也就克服了RAID存在的诸多问题 Ceph 数据存储原则：一个Pool 有若干PG，每个PG 包含若干对象，一个对象只能存储在一个PG中，而Ceph 默认一个PG 包含三个OSD，每个OSD都可看做一块硬盘。 因此，一个对象存储在Ceph中时，就被保存了三份。当一个磁盘故障时，还剩下2个PG，系统就会从另外两个PG中复制数据到其他磁盘上。这个是由crush算法决定 磁盘复制属性值可以通过管理员进行调整 磁盘存储上使用了加权机制，所以磁盘大小不一致也不会出现问题 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 14:16:18 "},"1.Linux基础/1.7存储/ceph/01核心概念/11对比SAN、NAS、DAS.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/11对比SAN、NAS、DAS.html","title":"11对比SAN、NAS、DAS","keywords":"","body":"对比SAN、NAS、DAS DAS Direct Attached Storage，即直连附加存储，第一代存储系统，通过SCSI总线扩展至一个外部的存储，磁带整列，作为服务器扩展的一部分 NAS Network Attached Storage，即网络附加存储，通过网络协议如NFS远程获取后端文件服务器共享的存储空间，将文件存储单独分离出来 SAN Storage Area Network，即存储区域网络，分为IP-SAN和FC-SAN，即通过TCP/IP协议和FC(Fiber Channel)光纤协议连接到存储服务器 Ceph Ceph在一个统一的存储系统中同时提供了对象存储、块存储和文件存储，即Ceph是一个统一存储，能够将企业企业中的三种存储需求统一汇总到一个存储系统中，并提供分布式、横向扩展，高度可靠性的存储系统 主要区别如下： DAS直连存储服务器使用SCSI或FC协议连接到存储阵列、通过SCSI总线和FC光纤协议类型进行数据传输； 例如一块有空间大小的裸磁盘：/dev/sdb。DAS存储虽然组网简单、成本低廉但是可扩展性有限、无法多主机实现共享、目前已经很少使用 NAS网络存储服务器使用TCP网络协议连接至文件共享存储、常见的有NFS、CIFS协议等；通过网络的方式映射存储中的一个目录到目标主机，如/data。 NAS网络存储使用简单，通过IP协议实现互相访问，多台主机可以同时共享同一个存储。但是NAS网络存储的性能有限，可靠性不是很高。 SAN存储区域网络服务器使用一个存储区域网络IP或FC连接到存储阵列、常见的SAN协议类型有IP-SAN和FC-SAN。SAN存储区域网络的性能非常好、可扩展性强；但是成本特别高、尤其是FC存储网络：因为需要用到HBA卡、FC交换机和支持FC接口的存储 存储结构/性能对比 DAS NAS FC-SAN IP-SAN Ceph 成本 低 较低 高 较高 高 数据传输速度 快 慢 极快 较快 快 扩展性 无扩展性 较低 易于扩展 最易扩展 易于扩展 服务器访问存储方式 块 文件 块 块 对象、文件、块 服务器系统性能开销 低 较低 低 较高 低 安全性 高 低 高 低 高 是否集中管理存储 否 是 是 是 否 备份效率 低 较低 高 较高 高 网络传输协议 无 TCP/IP FC TCP/IP 私有协议(TCP) Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 14:16:37 "},"1.Linux基础/1.7存储/ceph/02集成部署/01硬件需求.html":{"url":"1.Linux基础/1.7存储/ceph/02集成部署/01硬件需求.html","title":"01硬件需求","keywords":"","body":"硬件需求 Ceph被设计成在普通硬件上运行，这使得构建和维护pb级数据集群在经济上可行,在规划集群硬件时，需要平衡许多考虑因素，包括故障域和潜在的性能问题。 硬件规划应该包括在多个主机上分布Ceph守护进程和其他使用Ceph的进程。官方建议在为特定类型的守护进程配置的主机上运行特定类型的Ceph守护进程。 即ceph集群与客户端应为不同宿主机，具体硬件需求参考如下： CPU Ceph元数据服务器动态地重新分配它们的负载，这是CPU密集型的。因此，元数据服务器应该具有强大的处理能力(例如，四核或更好的cpu)。ceph osds运行RADOS服务，使用CRUSH计算数据位置，复制数据，并维护它们自己的集群映射副本。 因此，OSD应该具有合理的处理能力(例如，双核处理器)。监视器只是维护集群映射的主副本，因此它们不是CPU密集型的。 您还必须考虑主机除了运行Ceph守护进程外，是否还将运行cpu密集型进程。 例如，如果您的主机将运行计算虚拟机(例如OpenStack Nova)，您将需要确保这些其他进程为Ceph守护进程留出足够的处理能力。 建议在不同的主机上运行额外的cpu密集型进程 样例集群CPU配置： Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 72 On-line CPU(s) list: 0-71 Thread(s) per core: 2 Core(s) per socket: 18 Socket(s): 2 NUMA node(s): 2 Vendor ID: GenuineIntel CPU family: 6 Model: 85 Model name: Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz Stepping: 7 CPU MHz: 999.914 CPU max MHz: 3900.0000 CPU min MHz: 1000.0000 BogoMIPS: 5200.00 Virtualization: VT-x L1d cache: 32K L1i cache: 32K L2 cache: 1024K L3 cache: 25344K NUMA node0 CPU(s): 0-17,36-53 NUMA node1 CPU(s): 18-35,54-71 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 invpcid_single intel_ppin intel_pt ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear spec_ctrl intel_stibp flush_l1d arch_capabilities 内存 内存越多越好 CEPH-MON&CEPH-MGR（监控、管理节点）: 监视器和管理器守护进程的内存使用情况通常随集群的大小而变化。对于小型集群， 一般1-2GB就足够了。对于大型集群，您应该提供更多(5-10GB)。 您可能还需要考虑调整mon_OSD_cache_size或rocksdb_cache_size等设置 CEPH-MDS元数据节点 : 元数据守护进程的内存利用率取决于它的缓存被配置为消耗多少内存。对于大多数系统，官方建议至少使用1GB。具体大小可调整mds_cache_memory OSDS(CEPH-OSD)存储节点：默认情况下，使用BlueStore后端的OSD需要3-5GB RAM。当使用BlueStore时， 可以通过配置选项OSD_memory_target来调整OSD的内存消耗。当使用遗留的FileStore后端时， 操作系统页面缓存用于缓存数据，所以通常不需要进行调优，并且OSD的内存消耗通常与系统中每个守护进程的PGs数量有关。 样例集群内存配置： total used free shared buff/cache available Mem: 187G 8.8G 164G 4.0G 13G 173G Swap: 0B 0B 0B 存储 请仔细规划您的数据存储配置。在规划数据存储时，需要考虑大量的成本和性能折衷。 同时进行OS操作，以及多个守护进程对单个驱动器同时进行读和写操作的请求会显著降低性能。 注意： 因为Ceph在发送ACK之前必须把所有的数据都写到日志中(至少对于XFS来说是这样)，让日志和OSD的性能平衡是非常重要的! 硬盘驱动器 OSD应该有足够的硬盘驱动器空间来存放对象数据。官方建议硬盘驱动器的最小大小为1TB。 考虑较大磁盘的每GB成本优势。官方建议将硬盘驱动器的价格除以千兆字节数，得出每千兆字节的成本，因为较大的驱动器可能对每千兆字节的成本有很大的影响。 例如，价格为$75.00的1TB硬盘的成本为每GB$0.07(即$75 / 1024 = 0.0732)。 相比之下，价格为150美元的3TB硬盘的成本为每GB 0.05美元(即150美元/ 3072 = 0.0488)。 在前面的示例中，使用1TB的磁盘通常会使每GB的成本增加40%——从而大大降低集群的成本效率。 此外，存储驱动器容量越大，每个ceph osd守护进程需要的内存就越多，尤其是在重新平衡、回填和恢复期间。 一般的经验法则是1TB的存储空间需要1GB的RAM 存储驱动器受到寻道时间、访问时间、读和写时间以及总吞吐量的限制。 这些物理限制会影响整个系统的性能——尤其是在恢复过程中。 官方建议为操作系统和软件使用专用的驱动器，为主机上运行的每个ceph osd守护进程使用一个驱动器(物理硬盘)。 大多数慢OSD问题是由于在同一个驱动器上运行一个操作系统、多个OSD和/或多个日志引起的。 由于在小型集群上故障排除性能问题的成本可能会超过额外磁盘驱动器的成本，因此可以通过避免过度使用OSD存储驱动器来加速集群设计规划 您可以在每个硬盘驱动器上运行多个ceph osd进程，但这可能会导致资源争用，并降低总体吞吐量。 您可以将日志和对象数据存储在同一个驱动器上，但这可能会增加向客户端记录写操作和ACK所需的时间。 Ceph必须先向日志写入数据，然后才能对写入数据进行验证 总结为：Ceph最佳实践规定，您应该在不同的驱动器上运行操作系统、OSD数据和OSD日志 固态硬盘 提高性能的一个机会是使用固态驱动器(SSD)来减少随机访问时间和读取延迟，同时加速吞吐量。 与硬盘驱动器相比，SSD每GB的成本通常超过10倍，但SSD的访问时间通常至少比硬盘驱动器快100倍 SSD没有可移动的机械部件，因此它们不必受到与硬盘驱动器相同类型的限制。不过SSD确实有很大的局限性。 在评估SSD时，考虑顺序读写的性能是很重要的。 当存储多个OSD的多个日志时，顺序写吞吐量为400MB/s的SSD可能比顺序写吞吐量为120MB/s的SSD性能更好 由于SSD没有可移动的机械部件，所以在Ceph中不需要大量存储空间的区域使用SSD是有意义的。 相对便宜的固态硬盘可能会吸引你的经济意识。谨慎使用。 当选择与Ceph一起使用的SSD时，可接受的IOPS是不够的。对于日志和SSD有几个重要的性能考虑因素: 写密集型语义:日志记录涉及写密集型语义，因此您应该确保选择部署的SSD在写入数据时的性能等于或优于硬盘驱动器。 廉价的SSD可能会在加速访问时间的同时引入写延迟，因为有时高性能硬盘驱动器的写速度可以与市场上一些更经济的SSD一样快甚至更快! 顺序写:当您在一个SSD上存储多个日志时，您还必须考虑SSD的顺序写限制，因为它们可能会同时处理多个OSD日志的写请求。 分区对齐:SSD性能的一个常见问题是，人们喜欢将驱动器分区作为最佳实践，但他们常常忽略了使用SSD进行正确的分区对齐，这可能导致SSD传输数据的速度慢得多。确保SSD分区对齐 虽然SSD存储对象的成本非常高，但是通过将OSD的日志存储在SSD上，将OSD的对象数据存储在单独的硬盘驱动器上，可以显著提高OSD的性能。 OSD日志配置默认为/var/lib/ceph/OSD/$cluster-$id/journal。您可以将此路径挂载到SSD或SSD分区上，使其与对象数据不只是同一个磁盘上的文件 Ceph加速CephFS文件系统性能的一种方法是将CephFS元数据的存储与CephFS文件内容的存储隔离。 Ceph为cepfs元数据提供了一个默认的元数据池。您永远不必为CephFS元数据创建一个池，但是您可以为仅指向主机的SSD存储介质的CephFS元数据池创建一个CRUSH map层次结构。 重要提示: 官方建议探索SSD的使用以提高性能。但是，在对SSD进行重大投资之前，官方强烈建议检查SSD的性能指标，并在测试配置中测试SSD，以评估性能 控制器 磁盘控制器对写吞吐量也有很大的影响。仔细考虑磁盘控制器的选择，以确保它们不会造成性能瓶颈。 注意事项 你可以在每个主机上运行多个OSD，但是你应该确保OSD硬盘的总吞吐量不超过客户端读写数据所需的网络带宽。 您还应该考虑集群在每个主机上存储的总体数据的百分比。如果某个主机上的百分比很大，并且该主机发生故障，那么它可能会导致一些问题，比如超过了完整的比例，这将导致Ceph停止操作，作为防止数据丢失的安全预防措施。 当您在每个主机上运行多个OSD时，还需要确保内核是最新的。请参阅OS推荐，了解glibc和syncfs(2)方面的注意事项，以确保在每个主机上运行多个OSD时，您的硬件能够像预期的那样执行 拥有大量OSD的主机(例如> 20)可能会产生大量线程，特别是在恢复和平衡过程中。许多Linux内核默认的最大线程数相对较小(例如，32k)。如果在拥有大量OSD的主机上启动OSD时遇到问题，请考虑设置kernel。将pid_max设置为更高的线程数。理论最大值是4,194,303线程。 例如，您可以将以下内容添加到/etc/sysctl.conf文件中: kernel.pid_max = 4194303 样例集群存储配置： Disk /dev/nvme0n1: 1000.2 GB, 1000204886016 bytes, 1953525168 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/nvme1n1: 1000.2 GB, 1000204886016 bytes, 1953525168 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/nvme2n1: 1000.2 GB, 1000204886016 bytes, 1953525168 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/nvme3n1: 1000.2 GB, 1000204886016 bytes, 1953525168 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/sdb: 960.2 GB, 960197124096 bytes, 1875385008 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/sda: 480.1 GB, 480103981056 bytes, 937703088 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk label type: gpt Disk identifier: EDC26861-DACE-4831-848D-4FA0C5F642D7 # Start End Size Type Name 1 2048 2099199 1G EFI System EFI System Partition 2 2099200 4196351 1G Microsoft basic 3 4196352 937701375 445.1G Linux LVM Disk /dev/sde: 960.2 GB, 960197124096 bytes, 1875385008 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk /dev/sdd: 960.2 GB, 960197124096 bytes, 1875385008 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk /dev/sdg: 960.2 GB, 960197124096 bytes, 1875385008 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk /dev/sdf: 960.2 GB, 960197124096 bytes, 1875385008 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/sdh: 960.2 GB, 960197124096 bytes, 1875385008 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk label type: gpt Disk identifier: 5F151167-14E6-4826-BBEB-55280AC27EEC # Start End Size Type Name 1 10487808 1875384974 889.3G ceph osd ceph data 2 2048 10487807 5G Ceph Journal ceph journal Disk /dev/sdc: 960.2 GB, 960197124096 bytes, 1875385008 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk /dev/mapper/centos-root: 478.0 GB, 477953523712 bytes, 933502976 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes 网络 官方说明如下: 每个主机至少有两个1Gbps的网络接口控制器(nic)。由于大多数普通硬盘驱动器的吞吐量大约为100MB/秒，您的网卡应该能够处理主机上OSD磁盘的流量 建议至少使用两个网卡来考虑公共(前端)网络和集群(后端)网络。集群网络(最好不连接到外部网络)处理数据复制的额外负载 通过1Gbps的网络复制1TB的数据需要3个小时，3TB(典型的驱动器配置)需要9个小时。相比之下，在10Gbps的网络中，复制时间将分别为20分钟和1小时。 在pb级集群中，OSD磁盘故障应该是一种预期，而不是异常。系统管理员希望PGs尽可能快地从降级状态恢复到active + clean状态，同时考虑到价格/性能权衡。 此外，一些部署工具(如戴尔的Crowbar)可以部署5个不同的网络，但使用vlan使硬件和网络电缆更易于管理。使用802.1q协议的vlan需要支持vlan的网卡和交换机。增加的硬件费用可能会被网络设置和维护的操作成本节省所抵消 当使用vlan处理集群与计算栈(如OpenStack、CloudStack等)之间的虚拟机流量时，也值得考虑使用10G以太网。 每个网络的机架顶部路由器也需要能够与具有更快吞吐量的脊柱路由器进行通信。40Gbps到100Gbps 您的服务器硬件应该有一个底板管理控制器(BMC)。管理和部署工具也可能广泛地使用bmc， 因此考虑使用带外网络进行管理的成本/收益权衡。管理程序SSH访问、VM镜像上传、操作系统镜像安装、管理套接字等都可能给网络带来巨大的负载。 运行三个网络可能看起来有点小题大做，但每个流量路径都代表一个潜在的容量、吞吐量和/或性能瓶颈，在部署大规模数据集群之前，您应该仔细考虑这些问题 简言之： 建议三个以上网络接口： ceph集群网络接口 对外网络接口 管理接口 样例集群网口配置： [root@ceph01 ~]# ethtool ens4f0 Settings for ens4f0: Supported ports: [ FIBRE ] Supported link modes: 10000baseT/Full Supported pause frame use: Symmetric Supports auto-negotiation: No Supported FEC modes: Not reported Advertised link modes: 10000baseT/Full Advertised pause frame use: Symmetric Advertised auto-negotiation: No Advertised FEC modes: Not reported Speed: 10000Mb/s Duplex: Full Port: FIBRE PHYAD: 0 Transceiver: internal Auto-negotiation: off Supports Wake-on: d Wake-on: d Current message level: 0x00000007 (7) drv probe link Link detected: yes Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 14:11:50 "},"1.Linux基础/1.7存储/ceph/02集成部署/02硬件配置建议.html":{"url":"1.Linux基础/1.7存储/ceph/02集成部署/02硬件配置建议.html","title":"02硬件配置建议","keywords":"","body":"故障域是指阻止访问一个或多个OSDs的任何故障。这可能是主机上已停止的守护进程;硬盘故障、操作系统崩溃、网卡故障、电源故障、网络中断、电源中断，等等。 在规划硬件需求时，您必须平衡降低成本的诱惑，即把太多的责任放在太少的故障域中，以及隔离每个潜在故障域所增加的成本 硬件配置建议 最小配置建议 Process Criteria Minimum Recommended ceph-OSD Processor 1x 64-bit AMD-64 1x 32-bit ARM dual-core or better RAM ~1GB for 1TB of storage per daemon Volume Storage 1x storage drive per daemon Journal 1x SSD partition per daemon (optional) Network 2x 1GB Ethernet NICs ceph-mon Processor 1x 64-bit AMD-64 1x 32-bit ARM dual-core or better RAM 1 GB per daemon Disk Space 10 GB per daemon Network 2x 1GB Ethernet NICs ceph-mds Processor 1x 64-bit AMD-64 quad-core 1x 32-bit ARM quad-core RAM 1 GB minimum per daemon Disk Space 1 MB per daemon Network 2x 1GB Ethernet NICs 生产环境建议 Configuration Criteria Minimum Recommended Dell PE R510 Processor 2x 64-bit quad-core Xeon CPUs RAM 16 GB Volume Storage 8x 2TB drives. 1 OS, 7 Storage Client Network 2x 1GB Ethernet NICs OSD Network 2x 1GB Ethernet NICs Mgmt. Network 2x 1GB Ethernet NICs Dell PE R515 Processor 1x hex-core Opteron CPU RAM 16 GB Volume Storage 12x 3TB drives. Storage OS Storage 1x 500GB drive. Operating System. Client Network 2x 1GB Ethernet NICs OSD Network 2x 1GB Ethernet NICs Mgmt. Network 2x 1GB Ethernet NICs Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 14:15:04 "},"1.Linux基础/1.7存储/ceph/02集成部署/03操作系统建议.html":{"url":"1.Linux基础/1.7存储/ceph/02集成部署/03操作系统建议.html","title":"03操作系统建议","keywords":"","body":"操作系统建议 内核 Ceph客户端内核: 4.14.x 4.9.x 平台 Distro Release Code Name Kernel Notes Testing CentOS 7 N/A linux-3.10.0 3 B, I, C Debian 8.0 Jessie linux-3.16.0 1, 2 B, I Debian 9.0 Stretch linux-4.9 1, 2 B, I Fedora 22 N/A linux-3.14.0 B, I RHEL 7 Maipo linux-3.10.0 B, I Ubuntu 14.04 Trusty Tahr linux-3.13.0 B, I, C Ubuntu 16.04 Xenial Xerus linux-4.4.0 3 B, I, C Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 14:13:11 "},"1.Linux基础/1.7存储/ceph/02集成部署/04环境初始化.html":{"url":"1.Linux基础/1.7存储/ceph/02集成部署/04环境初始化.html","title":"04环境初始化","keywords":"","body":"环境说明 节点信息 节点名称 节点IP 节点属性 ceph01 192.168.1.69 admin,deploy,mon ceph02 192.168.1.70 单元格 ceph03 192.168.1.70 单元格 环境初始化 配置yum 所有ceph节点，包含客户端节点 1、删除原有yum源repo文件 rm -f /etc/yum.repos.d/*.repo 2、创建yum源文件 online curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo 下载以下文件上传至/etc/yum.repos.d/ Centos-7.repo epel-7.repo 离线环境参考ceph本地源 搭建本地源 3、配置ceph镜像源仓库 cat > /etc/yum.repos.d/ceph.repo 4、配置yum代理 适用于主机通过代理访问互联网场景 以下变量注意替换 username: 代理用户名 password: 代理用户密码 proxy_host: 代理IP地址 proxy_port: 代理端口 echo \"proxy=http://username:password@proxy_host:proxy_port\" >> /etc/yum.conf 配置时钟同步 1.配置dns 该dns用以解析时钟服务地址，互联网下应为114.114.114.114 echo \"nameserver x.x.x.x\" >> /etc/resolv.conf 2.安装ntp yum install -y ntp 3.同步 时钟服务地址据实际情况调整 ntpdate time.wl.com echo \"*/5 * * * * root ntpdate time.wl.com\" >> /etc/crontab 4.调整时区 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 升级内核 1.导入kernel源 elrepo-release-7.0-4.el7.elrepo.noarch.rpm rpm -ivh elrepo-release-7.0-4.el7.elrepo.noarch.rpm -y 2.安装最新主线版 yum -y --enablerepo=elrepo-kernel install kernel-ml.x86_64 kernel-ml-devel.x86_64 3.删除旧版本工具包 rpm -qa|grep kernel-3|xargs -n1 yum remove -y 4.安装新版本工具包 yum --disablerepo=\\* --enablerepo=elrepo-kernel install -y kernel-ml-tools.x86_64 5.查看内核列表 awk -F\\' '$1==\"menuentry \" {print i++ \" : \" $2}' /etc/grub2.cfg 6.重建内核 grub2-mkconfig -o /boot/grub2/grub.cfg 7.配置新版内核 grub2-set-default 0 8.重启 reboot -f Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 14:26:56 "},"1.Linux基础/1.7存储/ceph/02集成部署/05安装ceph核心组件.html":{"url":"1.Linux基础/1.7存储/ceph/02集成部署/05安装ceph核心组件.html","title":"05安装ceph核心组件","keywords":"","body":"安装ceph组件 ceph版本为14.2.16 nautilus 1.创建ceph目录(deploy节点执行) mkdir -p /etc/ceph 2.配置主机互信(deploy节点执行) ssh-keygen -t rsa -b 2048 -N '' -f ~/.ssh/id_rsa ssh-copy-id ceph01 ssh-copy-id ceph02 ssh-copy-id ceph03 3.安装ceph(所有节点执行) yum install -y ceph ceph-deploy 4.初始化mon节点(deploy节点执行) ceph-deploy new ceph01 ceph02 ceph03 5.初始化mon(deploy节点执行) ceph-deploy mon create-initial 6.修改集群文件(deploy节点执行) cd /etc/ceph/ echo \"public_network=192.168.1.0/24\" >> /etc/ceph/ceph.conf ceph-deploy --overwrite-conf config push ceph01 ceph02 ceph03 7.配置admin节点 cd /etc/ceph/ ceph-deploy admin ceph01 ceph02 ceph03 8.查看集群状态 [root@ceph01 ~]# ceph -s cluster: id: b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294 health: HEALTH_WARN mon ceph03 is low on available space services: mon: 3 daemons, quorum ceph01,ceph02,ceph03 (age 31m) mgr: no daemons active OSD: 0 OSDs: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs: 9.安装命令补全 yum -y install bash-completion Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 14:30:22 "},"1.Linux基础/1.7存储/ceph/02集成部署/06添加硬盘至集群内.html":{"url":"1.Linux基础/1.7存储/ceph/02集成部署/06添加硬盘至集群内.html","title":"06添加硬盘至集群内","keywords":"","body":"创建osd 所有存储节点执行相同操作 1.列出节点磁盘信息 ceph-deploy disk list ceph01 ceph-deploy disk list ceph02 ceph-deploy disk list ceph03 输出如下 ... [ceph01][INFO ] Disk /dev/sda: 480.1 GB, 480103981056 bytes, 937703088 sectors [ceph01][INFO ] Disk /dev/sdb: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph01][INFO ] Disk /dev/sdf: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph01][INFO ] Disk /dev/sdd: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph01][INFO ] Disk /dev/sde: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph01][INFO ] Disk /dev/sdc: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph01][INFO ] Disk /dev/sdg: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph01][INFO ] Disk /dev/sdh: 960.2 GB, 960197124096 bytes, 1875385008 sectors ... ... [ceph02][INFO ] Disk /dev/sda: 480.1 GB, 480103981056 bytes, 937703088 sectors [ceph02][INFO ] Disk /dev/sdb: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph02][INFO ] Disk /dev/sdf: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph02][INFO ] Disk /dev/sdd: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph02][INFO ] Disk /dev/sde: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph02][INFO ] Disk /dev/sdc: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph02][INFO ] Disk /dev/sdg: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph02][INFO ] Disk /dev/sdh: 960.2 GB, 960197124096 bytes, 1875385008 sectors ... ... [ceph03][INFO ] Disk /dev/sda: 480.1 GB, 480103981056 bytes, 937703088 sectors [ceph03][INFO ] Disk /dev/sdb: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph03][INFO ] Disk /dev/sdf: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph03][INFO ] Disk /dev/sdd: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph03][INFO ] Disk /dev/sde: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph03][INFO ] Disk /dev/sdc: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph03][INFO ] Disk /dev/sdg: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph03][INFO ] Disk /dev/sdh: 960.2 GB, 960197124096 bytes, 1875385008 sectors ... 其中/dev/sda-h为SSD类型磁盘，且/dev/sda为系统盘 2.查看磁盘挂载 lsblk 输出如下 NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:16 0 894.3G 0 disk ├─sda2 8:34 0 1G 0 part /boot ├─sda3 8:35 0 445.1G 0 part │ ├─centos-swap 253:1 0 16G 0 lvm │ └─centos-root 253:0 0 429.1G 0 lvm / └─sda1 8:33 0 1G 0 part /boot/efi sr0 11:0 1 4.2G 0 rom sdb 8:0 0 894.3G 0 disk sdc 8:32 0 894.3G 0 disk sdd 8:0 0 894.3G 0 disk sde 8:64 0 894.3G 0 disk sdf 8:64 0 894.3G 0 disk sdg 8:96 0 894.3G 0 disk sdh 8:112 0 894.3G 0 disk .... 3.格式化磁盘 mkfs.ext4 /dev/sdb mkfs.ext4 /dev/sdc mkfs.ext4 /dev/sdd mkfs.ext4 /dev/sde mkfs.ext4 /dev/sdf mkfs.ext4 /dev/sdg mkfs.ext4 /dev/sdh 4.擦净节点磁盘 cd /etc/ceph/ for i in {b..h};do ceph-deploy disk zap ceph01 /dev/sd$i done cd /etc/ceph/ for i in {b..h};do ceph-deploy disk zap ceph02 /dev/sd$i done cd /etc/ceph/ for i in {b..h};do ceph-deploy disk zap ceph03 /dev/sd$i done 5.创建osd节点 cd /etc/ceph for i in {b..h};do ceph-deploy osd create --data /dev/sd$i ceph01 done cd /etc/ceph for i in {b..h};do ceph-deploy osd create --data /dev/sd$i ceph02 done cd /etc/ceph for i in {b..h};do ceph-deploy osd create --data /dev/sd$i ceph03 done Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 14:38:40 "},"1.Linux基础/1.7存储/ceph/02集成部署/07安装dashboard.html":{"url":"1.Linux基础/1.7存储/ceph/02集成部署/07安装dashboard.html","title":"07安装dashboard","keywords":"","body":"安装dashboard rpm 1.安装 ceph-deploy mgr create ceph01 2.启用Dashboard ceph mgr module enable dashboard 3.启用必要模块 ceph mgr module enable pg_autoscaler 4.用户、密码、权限 # 创建用户 #ceph dashboard ac-user-create administrator ceph dashboard ac-user-create admin Ceph-12345 administrator 5.创建自签证书 ceph dashboard create-self-signed-cert 6.查看Dashboard地址 [root@ceph01 ~]# ceph mgr services { \"dashboard\": \"https://ceph01:8443/\" } 7.登录访问 8.修改端口(可选) 确认配置 [root@ceph01 ~]# ceph config-key ls [ \"config-history/1/\", \"config-history/2/\", \"config-history/2/+global/osd_pool_default_pg_autoscale_mode\", \"config/global/osd_pool_default_pg_autoscale_mode\", \"mgr/dashboard/accessdb_v1\", \"mgr/dashboard/crt\", \"mgr/dashboard/jwt_secret\", \"mgr/dashboard/key\" ] 修改端口 ceph config set mgr mgr/dashboard/ssl_server_port 7000 使变更的配置生效 ceph mgr module disable dashboard ceph mgr module enable dashboard 9.配置访问前缀(可选) ceph config set mgr mgr/dashboard/url_prefix /ceph-ui 重启mgr systemctl restart ceph-mgr@ceph01 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 14:43:11 "},"1.Linux基础/1.7存储/ceph/02集成部署/111ceph本地源搭建.html":{"url":"1.Linux基础/1.7存储/ceph/02集成部署/111ceph本地源搭建.html","title":"111ceph本地源搭建","keywords":"","body":"ceph本地源 适用无法直连或通过代理连接互联网镜像源 联网主机：用于导出ceph依赖,操作系统为CentOS7 离线主机：实际部署ceph应用的主机(多节点实例) 依赖导出（联网主机） rm -f /etc/yum.repos.d/*.repo curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo yum update -y yum install yum-plugin-downloadonly -y yum install --downloadonly --downloaddir=./ceph ceph ceph-common ceph-deploy 生成repo依赖关系（联网主机） yum install -y createrepo createrepo ./ceph 压缩（联网主机） tar zcvf ceph.tar.gz ceph 配置使用（离线主机） tar zxvf ceph.tar.gz -C / cat > /etc/yum.repos.d/ceph.repo Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 14:26:56 "},"1.Linux基础/1.7存储/ceph/03存储使用/块设备类型存储使用.html":{"url":"1.Linux基础/1.7存储/ceph/03存储使用/块设备类型存储使用.html","title":"块设备类型存储使用","keywords":"","body":"块设备使用 ceph管理节点 1.创建池 [root@ceph01 ~]# ceph osd pool create rbd-demo-pool 64 64 pool 'rbd-demo-pool' created 2.设置配额 [root@ceph01 ~]# ceph osd pool set-quota rbd-demo-pool max_bytes 1G set-quota max_bytes = 1073741824 for pool rbd-demo-pool 3.关联应用 [root@ceph01 ~]# ceph osd pool application enable rbd-demo-pool rbd enabled application 'rbd' on pool 'rbd-demo-pool' 4.初始化 [root@ceph01 ~]# rbd pool init rbd-demo-pool 5.创建rbd用户 ceph auth get-or-create client.qemu mon 'profile rbd' osd 'profile rbd pool=rbd-demo-pool' mgr 'profile rbd pool=rbd-demo-pool' -o /etc/ceph/ceph.client.qemu.keyring 6.创建rbd映像 在将块设备添加到节点之前，必须先在Ceph存储集群中为其创建映像。要创建块设备映像，请执行以下操作： rbd create --size 1G rbd-demo-pool/rbd-demo-image 7.查看块设备映像 # rbd ls {poolname} [root@ceph01 ~]# rbd ls rbd-demo-pool rbd-demo-image 8.查看块设备映像信息 [root@ceph01 ~]# rbd info rbd-demo-pool/rbd-demo-image rbd image 'rbd-demo-image': size 1 GiB in 256 objects order 22 (4 MiB objects) snapshot_count: 0 id: 3d92a06e59b5 block_name_prefix: rbd_data.3d92a06e59b5 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten op_features: flags: create_timestamp: Fri Mar 19 15:45:58 2021 access_timestamp: Fri Mar 19 15:45:58 2021 modify_timestamp: Fri Mar 19 15:45:58 2021 客户端 1.删除原有yum源repo文件 rm -f /etc/yum.repos.d/*.repo 2.创建yum源文件（客户端） online curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo offline 下载以下文件上传至/etc/yum.repos.d/ Centos-7.repo epel-7.repo 3.配置ceph镜像源仓库 cat > /etc/yum.repos.d/ceph.repo 4.配置yum代理 适用于主机通过代理访问互联网场景 以下变量注意替换 username: 代理用户名 password: 代理用户密码 proxy_host: 代理IP地址 proxy_port: 代理端口 echo \"proxy=http://username:password@proxy_host:proxy_port\" >> /etc/yum.conf 5.安装ceph-common yum install -y ceph-common 6.拷贝配置文件 mkdir -p /etc/ceph 7.客户端创建挂载目录 mkdir -p /ceph chmod 777 /ceph 8.从服务端scp以下文件至客户端/etc/ceph下 /etc/ceph/ceph.client.qemu.keyring /etc/ceph/ceph.conf scp /etc/ceph/{ceph.conf,ceph.client.admin.keyring} ip:/etc/ceph/ 9.映射块设备 [root@localhost ~]# rbd map rbd-demo-pool/rbd-demo-image --name client.qemu /dev/rbd0 [root@localhost ~]# echo \"rbd-demo-pool/rbd-demo-image id=qemu,keyring=/etc/ceph/ceph.client.qemu.keyring\" >> /etc/ceph/rbdmap 10.格式化块设备 mkfs.ext4 -q /dev/rbd0 11.挂载使用 mount /dev/rbd0 /ceph 12.查看挂载 lsblk 13.查看块设备映射 [root@localhost ~]# rbd device list id pool namespace image snap device 0 rbd-demo-pool rbd-demo-image - /dev/rbd0 14.修改fstab，设置开机挂载 echo \"/dev/rbd0 /ceph ext4 defaults,noatime,_netdev 0 0\" >> /etc/fstab 15.配置开机自启动 vim /etc/init.d/rbdmap 填充以下内容 #!/bin/bash #chkconfig: 2345 80 60 #description: start/stop rbdmap ### BEGIN INIT INFO # Provides: rbdmap # Required-Start: $network # Required-Stop: $network # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Ceph RBD Mapping # Description: Ceph RBD Mapping ### END INIT INFO DESC=\"RBD Mapping\" RBDMAPFILE=\"/etc/ceph/rbdmap\" . /lib/lsb/init-functions #. /etc/redhat-lsb/lsb_log_message，加入此行后不正长 do_map() { if [ ! -f \"$RBDMAPFILE\" ]; then echo \"$DESC : No $RBDMAPFILE found.\" exit 0 fi echo \"Starting $DESC\" # Read /etc/rbdtab to create non-existant mapping newrbd= RET=0 while read DEV PARAMS; do case \"$DEV\" in \"\"|\\#*) continue ;; */*) ;; *) DEV=rbd/$DEV ;; esac OIFS=$IFS IFS=',' for PARAM in ${PARAMS[@]}; do CMDPARAMS=\"$CMDPARAMS --$(echo $PARAM | tr '=' ' ')\" done IFS=$OIFS if [ ! -b /dev/rbd/$DEV ]; then echo $DEV rbd map $DEV $CMDPARAMS [ $? -ne \"0\" ] && RET=1 newrbd=\"yes\" fi done 16.赋权 yum install redhat-lsb -y chmod +x /etc/init.d/rbdmap service rbdmap start chkconfig rbdmap on 适用场景 可以当成本地盘来用： 虚机存储 开发数据库存储 存储日志 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 12:19:41 "},"1.Linux基础/1.7存储/ceph/03存储使用/文件系统类型存储使用.html":{"url":"1.Linux基础/1.7存储/ceph/03存储使用/文件系统类型存储使用.html","title":"文件系统类型存储使用","keywords":"","body":"ceph文件系统使用 服务端 1.安装mds（ceph节点安装，建议3个mds） ceph-deploy mds create ceph01 ceph02 ceph03 2.创建cephfs存储池与元数据池 ceph osd pool create cephfs_data 64 ceph osd pool create cephfs_metadata 64 3.创建文件系统 ceph fs new cephfs cephfs_metadata cephfs_data 4.关联应用 [root@ceph01 ~]# ceph osd pool application enable cephfs_data cephfs enabled application 'cephfs' on pool 'cephfs_data' 5.设置配额 ceph osd pool set-quota cephfs_data max_bytes 100G 6.创建用户 [root@ceph01 ~]# ceph auth get-or-create client.cephfs mon 'allow r' mds 'allow r, allow rw path=/' osd 'allow rw pool=cephfs_data' [client.cephfs] key = AQCHWlRg46I6EBAAg+xBZnFsqOYIGluPd5h1QA== 客户端 内核需4.x 1.删除原有yum源repo文件 rm -f /etc/yum.repos.d/*.repo 2.创建yum源文件（客户端） online curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo offline 下载以下文件上传至/etc/yum.repos.d/ Centos-7.repo epel-7.repo 3.配置ceph镜像源仓库 cat > /etc/yum.repos.d/ceph.repo 4.配置yum代理 适用于主机通过代理访问互联网场景 以下变量注意替换 username: 代理用户名 password: 代理用户密码 proxy_host: 代理IP地址 proxy_port: 代理端口 echo \"proxy=http://username:password@proxy_host:proxy_port\" >> /etc/yum.conf 5.安装ceph-common yum install -y ceph-common 6.创建目录 配置目录 mkdir -p /etc/ceph 挂载cephfs目录 mkdir -p /cephfs 7.服务端创建认证文件 服务端执行以下命令获取cephfs用户认证信息 [root@ceph01 ~]# ceph auth get client.cephfs exported keyring for client.cephfs [client.cephfs] key = AQCHWlRg46I6EBAAg+xBZnFsqOYIGluPd5h1QA== caps mds = \"allow r, allow rw path=/\" caps mon = \"allow r\" caps osd = \"allow rw pool=cephfs_data\" 8.客户端创建认证文件 cat /etc/ceph/cephfs.key AQCHWlRg46I6EBAAg+xBZnFsqOYIGluPd5h1QA== EOF 9.客户端挂载文件系统 mount -t ceph 192.168.1.69:6789,192.168.1.70:6789,192.168.1.71:6789:/ /cephfs -o name=cephfs,secretfile=/etc/ceph/cephfs.key 10.创建测试文件 touch /cephfs/123 11.配置开机挂载 cat > /etc/fstab 192.168.1.69:6789,192.168.1.70:6789,192.168.1.71:6789:/ /cephfs ceph name=cephfs,secretfile=/etc/ceph/cephfs.key,noatime,_netdev 0 2 EOF 12.重启主机验证 reboot 适用场景 文件共享 网站文件、代码存储 数据备份 日志存储 数据分析 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 12:26:01 "},"1.Linux基础/1.7存储/ceph/04对接k8s/01csi简介.html":{"url":"1.Linux基础/1.7存储/ceph/04对接k8s/01csi简介.html","title":"01csi简介","keywords":"","body":"csi简介 Kubernetes从1.9版本开始引入容器存储接口Container Storage Interface（CSI）机制，用于在Kubernetes和外部存储系统之间建立一套标准的存储管理接口，通过该接口为容器提供存储服务。 csi设计背景 Kubernetes通过PV、PVC、Storageclass已经提供了一种强大的基于插件的存储管理机制， 但是各种存储插件提供的存储服务都是基于一种被称为in-true（树内）的方式提供的， 这要求存储插件的代码必须被放进Kubernetes的主干代码库中才能被Kubernetes调用， 属于紧耦合的开发模式。这种in-tree方式会带来一些问题： 存储插件的代码需要与Kubernetes的代码放在同一代码库中，并与Kubernetes的二进制文件共同发布 存储插件代码的开发者必须遵循Kubernetes的代码开发规范 存储插件代码的开发者必须遵循Kubernetes的发布流程，包括添加对Kubernetes存储系统的支持和错误修复 Kubernetes社区需要对存储插件的代码进行维护，包括审核、测试等工作 存储插件代码中的问题可能会影响Kubernetes组件的运行，并且很难排查问题 存储插件代码与Kubernetes的核心组件（kubelet和kubecontroller-manager）享有相同的系统特权权限，可能存在可靠性和安全性问题。 部署第三方驱动的可执行文件仍然需要宿主机的root权限，存在安全隐患 存储插件在执行mount、attach这些操作时，通常需要在宿主机上安装一些第三方工具包和依赖库， 使得部署过程更加复杂，例如部署Ceph时需要安装rbd库，部署GlusterFS时需要安装mount.glusterfs库，等等 基于以上这些问题和考虑，Kubernetes逐步推出与容器对接的存储接口标准，存储提供方只需要基于标准接口进行存储插件的实现，就能使用Kubernetes的原生存储机制为容器提供存储服务。这套标准被称为CSI（容器存储接口）。 在CSI成为Kubernetes的存储供应标准之后，存储提供方的代码就能和Kubernetes代码彻底解耦，部署也与Kubernetes核心组件分离，显然，存储插件的开发由提供方自行维护，就能为Kubernetes用户提供更多的存储功能，也更加安全可靠。 基于CSI的存储插件机制也被称为out-of-tree（树外）的服务提供方式，是未来Kubernetes第三方存储插件的标准方案。 csi架构 KubernetesCSI存储插件的关键组件和推荐的容器化部署架构： CSI Controller CSI Controller的主要功能是提供存储服务视角对存储资源和存储卷进行管理和操作。 在Kubernetes中建议将其部署为单实例Pod，可以使用StatefulSet或Deployment控制器进行部署，设置副本数量为1，保证为一种存储插件只运行一个控制器实例。 在这个Pod内部署两个容器： 与Master（kube-controller-manager）通信的辅助sidecar容器。在sidecar容器内又可以包含external-attacher和external-provisioner两个容器，它们的功能分别如下: external-attacher：监控VolumeAttachment资源对象的变更，触发针对CSI端点的ControllerPublish和ControllerUnpublish操作。 external-provisioner：监控PersistentVolumeClaim资源对象的变更，触发针对CSI端点的CreateVolume和DeleteVolume操作。 CSI Driver存储驱动容器，由第三方存储提供商提供，需要实现上述接口。 这两个容器通过本地Socket（Unix DomainSocket，UDS），并使用gPRC协议进行通信。 sidecar容器通过Socket调用CSI Driver容器的CSI接口，CSI Driver容器负责具体的存储卷操作。 CSI Node CSI Node的主要功能是对主机（Node）上的Volume进行管理和操作。在Kubernetes中建议将其部署为DaemonSet，在每个Node上都运行一个Pod。 在这个Pod中部署以下两个容器： 与kubelet通信的辅助sidecar容器node-driver-registrar，主要功能是将存储驱动注册到kubelet中 CSI Driver存储驱动容器，由第三方存储提供商提供，主要功能是接收kubelet的调用，需要实现一系列与Node相关的CSI接口，例如NodePublishVolume接口（用于将Volume挂载到容器内的目标路径）、NodeUnpublishVolume接口（用于从容器中卸载Volume），等等。 node-driver-registrar容器与kubelet通过Node主机的一个hostPath目录下的unixsocket进行通信。CSI Driver容器与kubelet通过Node主机的另一个hostPath目录下的unixsocket进行通信，同时需要将kubelet的工作目录（默认为/var/lib/kubelet）挂载给CSIDriver容器，用于为Pod进行Volume的管理操作（包括mount、umount等）。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 12:29:37 "},"1.Linux基础/1.7存储/ceph/04对接k8s/02块设备类型存储使用.html":{"url":"1.Linux基础/1.7存储/ceph/04对接k8s/02块设备类型存储使用.html","title":"02块设备类型存储使用","keywords":"","body":"k8s接入ceph块存储 使用ceph-csi 实现 ceph服务端 1.创建一个池，用以为k8s提供块存储服务 [root@ceph01 ~]# ceph osd pool create rbd-k8s-pool 256 256 SSD_rule pool 'rbd-k8s-pool' created 2.设置配额 [root@ceph01 ~]# ceph osd pool set-quota rbd-k8s-pool max_bytes 100G set-quota max_bytes = 107374182400 for pool rbd-k8s-pool 3.关联应用 [root@ceph01 ~]# ceph osd pool application enable rbd-k8s-pool rbd enabled application 'rbd' on pool 'rbd-k8s-pool' 4.初始化 rbd pool init rbd-k8s-pool 5.创建用户 [root@ceph01 ~]# ceph auth get-or-create client.kubernetes mon 'profile rbd' osd 'profile rbd pool=rbd-k8s-pool' mgr 'profile rbd pool=rbd-k8s-pool' [client.kubernetes] key = AQCS6kFg0NRDIBAAorr8r5Oxiz1eYH61VvLVYA== k8s节点 主节点执行以下步骤 1.下载配置文件 ceph-csi-3.2.0.zip 2.上传配置文件解压 unzip ceph-csi-3.2.0.zip 3.创建一个命名空间，用于管理ceph-csi kubectl create ns ceph-csi 4.更改ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-config-map.yaml 首先获取集群信息(ceph管理节点执行) b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294为集群ID mon节点地址：192.168.1.69:6789,192.168.1.70:6789,192.168.1.71:6789 [root@ceph01 ~]# ceph mon dump dumped monmap epoch 2 epoch 2 fsid b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294 last_changed 2021-02-22 14:36:08.199609 created 2021-02-22 14:27:26.357269 min_mon_release 14 (nautilus) 0: [v2:192.168.1.69:3300/0,v1:192.168.1.69:6789/0] mon.ceph01 1: [v2:192.168.1.70:3300/0,v1:192.168.1.70:6789/0] mon.ceph02 2: [v2:192.168.1.71:3300/0,v1:192.168.1.71:6789/0] mon.ceph03 更改csi-config-map.yaml内容如下： vim ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-config-map.yaml 内容参考如下 --- apiVersion: v1 kind: ConfigMap data: config.json: |- [ { \"clusterID\": \"b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294\", \"monitors\": [ \"192.168.1.69:6789\", \"192.168.1.70:6789\", \"192.168.1.71:6789\" ] } ] metadata: name: ceph-csi-config 5.创建csi-config-map kubectl -n ceph-csi apply -f ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-config-map.yaml 6.创建csi-rbd-secret 创建 cat ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-rbd-secret.yaml apiVersion: v1 kind: Secret metadata: name: csi-rbd-secret namespace: ceph-csi stringData: userID: kubernetes userKey: AQCS6kFg0NRDIBAAorr8r5Oxiz1eYH61VvLVYA== EOF 其中：AQCS6kFg0NRDIBAAorr8r5Oxiz1eYH61VvLVYA==可通过在ceph服务端执行ceph auth get client.kubernetes获取 发布 kubectl apply -f ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-rbd-secret.yaml 7.配置清单中的namespace改成ceph-csi sed -i \"s/namespace: default/namespace: ceph-csi/g\" $(grep -rl \"namespace: default\" ./ceph-csi-3.2.0/deploy/rbd/kubernetes) sed -i -e \"/^kind: ServiceAccount/{N;N;a\\ namespace: ceph-csi # 输入到这里的时候需要按一下回车键，在下一行继续输入 }\" $(egrep -rl \"^kind: ServiceAccount\" ./ceph-csi-3.2.0/deploy/rbd/kubernetes) 8.创建ServiceAccount和RBAC ClusterRole/ClusterRoleBinding资源对象 kubectl create -f ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml kubectl create -f ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml 9.创建PodSecurityPolicy kubectl create -f ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-provisioner-psp.yaml kubectl create -f ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-nodeplugin-psp.yaml 10.调整csi-rbdplugin-provisioner.yaml和csi-rbdplugin.yaml 将csi-rbdplugin.yaml中的kms部分配置注释掉 # vim ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-rbdplugin.yaml ... - name: ceph-csi-encryption-kms-config mountPath: /etc/ceph-csi-encryption-kms-config/ ... ... - name: ceph-csi-encryption-kms-config configMap: name: ceph-csi-encryption-kms-config ... 11.将csi-rbdplugin-provisioner.yaml中的kms部分配置注释掉 # vim ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml ... - name: ceph-csi-encryption-kms-config mountPath: /etc/ceph-csi-encryption-kms-config/ ... ... - name: ceph-csi-encryption-kms-config configMap: name: ceph-csi-encryption-kms-config ... 12.将csi-rbdplugin.yaml中的image部分调整为可访问镜像地址 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1 quay.io/cephcsi/cephcsi:v3.2.0 13.将csi-rbdplugin-provisioner.yaml中的image部分调整为可访问镜像地址 quay.io/cephcsi/cephcsi:v3.2.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2 k8s.gcr.io/sig-storage/csi-attacher:v3.0.2 k8s.gcr.io/sig-storage/csi-resizer:v1.0.1 14.发布csi-rbdplugin-provisioner.yaml和csi-rbdplugin.yaml kubectl -n ceph-csi create -f ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml kubectl -n ceph-csi create -f ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-rbdplugin.yaml 15.查看运行状态 [root@ceph01 ~]# kubectl get pod -n ceph-csi NAME READY STATUS RESTARTS AGE csi-rbdplugin-ddc42 3/3 Running 0 76s csi-rbdplugin-fwwfv 3/3 Running 0 76s csi-rbdplugin-provisioner-76959bd74d-gwd9k 7/7 Running 0 5h32m csi-rbdplugin-provisioner-76959bd74d-nb574 7/7 Running 0 5h32m 16.创建StorageClass b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294为ceph集群ID注意替换 生成配置文件 cat ceph-csi-3.2.0/deploy/rbd/kubernetes/storageclass.yaml --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-csi-rbd-sc provisioner: rbd.csi.ceph.com parameters: clusterID: b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294 pool: rbd-k8s-pool imageFeatures: layering csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret csi.storage.k8s.io/controller-expand-secret-namespace: ceph-csi csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi csi.storage.k8s.io/fstype: ext4 reclaimPolicy: Delete allowVolumeExpansion: true mountOptions: - discard EOF 创建 kubectl apply -f ceph-csi-3.2.0/deploy/rbd/kubernetes/storageclass.yaml 配置为默认storage class kubectl patch storageclass ceph-csi-rbd-sc -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' 查看storage class [root@ceph01 ~]# kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE ceph-csi-rbd-sc (default) rbd.csi.ceph.com Delete Immediate true 117s 17.创建pvc验证可用性 生成配置 cat ceph-csi-3.2.0/deploy/rbd/kubernetes/pvc-demo.yaml --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: ceph-pvc-demo namespace: default spec: storageClassName: ceph-csi-rbd-sc accessModes: - ReadWriteOnce resources: requests: storage: 1Gi EOF 创建 kubectl apply -f ceph-csi-3.2.0/deploy/rbd/kubernetes/pvc-demo.yaml 查看 [root@ceph01 ~]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE ceph-pvc-demo Bound pvc-7b7d4d8a-c4f4-40b6-9372-661ece7c385e 1Gi RWO ceph-csi-rbd-sc 13s 18.pvc扩容 生成配置 cat ceph-csi-3.2.0/deploy/rbd/kubernetes/nginx-demo.yaml apiVersion: v1 kind: Pod metadata: name: testpv labels: role: web-frontend spec: containers: - name: web image: nginx ports: - name: web containerPort: 80 volumeMounts: - name: ceph-pvc-demo mountPath: \"/usr/share/nginx/html\" volumes: - name: ceph-pvc-demo persistentVolumeClaim: claimName: ceph-pvc-demo EOF 发布 kubectl apply -f ceph-csi-3.2.0/deploy/rbd/kubernetes/nginx-demo.yaml 查看pvc [root@ceph01 ~]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE ceph-pvc-demo Bound pvc-360f8c5b-0f82-4f17-957b-a7eb5cf93f7e 1Gi RWO ceph-csi-rbd-sc 2m50s 编辑修改pvc kubectl edit pvc ceph-pvc-demo 修改以下内容，storage: 1Gi调整为storage: 10Gi ... spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi ... 重启 kubectl get pod testpv -o yaml | kubectl replace --force -f - 再次查看pvc [root@ceph01 ~]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE ceph-pvc-demo Bound pvc-360f8c5b-0f82-4f17-957b-a7eb5cf93f7e 10Gi RWO ceph-csi-rbd-sc 9m26s Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 12:41:00 "},"1.Linux基础/1.7存储/ceph/04对接k8s/03文件系统类型存储使用.html":{"url":"1.Linux基础/1.7存储/ceph/04对接k8s/03文件系统类型存储使用.html","title":"03文件系统类型存储使用","keywords":"","body":"k8s接入ceph文件系统 ceph服务端 1.安装mds ceph-deploy mds create ceph01 ceph02 ceph03 2.创建cephfs存储池与元数据池，用以为k8s提供文件系统服务 ceph osd pool create cephfs_data 64 ceph osd pool create cephfs_metadata 64 3.创建文件系统 ceph fs new k8s-cephfs cephfs_metadata cephfs_data 4.关联应用 [root@ceph01 ~]# ceph osd pool application enable cephfs_data cephfs enabled application 'cephfs' on pool 'cephfs_data' 5.设置配额 ceph osd pool set-quota cephfs_data max_bytes 100G 6.创建用户 [root@ceph01 kubernetes]# ceph auth get-or-create client.cephfs mon 'allow r' mds 'allow r, allow rw path=/' osd 'allow rw pool=cephfs_data' [client.cephfs] key = AQCoW0dgQk4qGhAAwayKv70OSyyWB3XpZ1JLYQ== k8s节点 1.下载配置文件 ceph-csi-3.2.0.zip 2.上传配置文件解压 unzip ceph-csi-3.2.0.zip 3.创建一个命名空间，用于管理ceph-csi kubectl create ns ceph-csi 4.更改ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-config-map.yaml 获取集群信息(ceph管理节点执行) [root@ceph01 ~]# ceph mon dump dumped monmap epoch 2 epoch 2 fsid b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294 last_changed 2021-02-22 14:36:08.199609 created 2021-02-22 14:27:26.357269 min_mon_release 14 (nautilus) 0: [v2:192.168.1.69:3300/0,v1:192.168.1.69:6789/0] mon.ceph01 1: [v2:192.168.1.70:3300/0,v1:192.168.1.70:6789/0] mon.ceph02 2: [v2:192.168.1.71:3300/0,v1:192.168.1.71:6789/0] mon.ceph03 b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294为集群ID 监控节点地址：192.168.1.69:6789,192.168.1.70:6789,192.168.1.71:6789 更改 vim ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-config-map.yaml 更改后csi-config-map.yaml内容如下： --- apiVersion: v1 kind: ConfigMap data: config.json: |- [ { \"clusterID\": \"b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294\", \"monitors\": [ \"192.168.1.69:6789\", \"192.168.1.70:6789\", \"192.168.1.71:6789\" ] } ] metadata: name: ceph-csi-config 5.创建csi-config-map kubectl -n ceph-csi apply -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-config-map.yaml 6.创建csi-cephfs-secret 创建 cat ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-cephfs-secret.yaml --- apiVersion: v1 kind: Secret metadata: name: csi-cephfs-secret namespace: ceph-csi stringData: # Required for statically provisioned volumes userID: cephfs userKey: AQCoW0dgQk4qGhAAwayKv70OSyyWB3XpZ1JLYQ== # Required for dynamically provisioned volumes adminID: admin adminKey: AQDkTjNgNObEHBAANGLCF23SLKYmqUd2Nxtbaw== EOF AQCoW0dgQk4qGhAAwayKv70OSyyWB3XpZ1JLYQ====可通过在ceph服务端执行ceph auth get client.cephfs获取 发布 kubectl apply -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-cephfs-secret.yaml 7.配置清单中的namespace改成ceph-csi sed -i \"s/namespace: default/namespace: ceph-csi/g\" $(grep -rl \"namespace: default\" ./ceph-csi-3.2.0/deploy/cephfs/kubernetes) sed -i -e \"/^kind: ServiceAccount/{N;N;a\\ namespace: ceph-csi # 输入到这里的时候需要按一下回车键，在下一行继续输入 }\" $(egrep -rl \"^kind: ServiceAccount\" ./ceph-csi-3.2.0/deploy/cephfs/kubernetes) 8.创建ServiceAccount和RBAC ClusterRole/ClusterRoleBinding资源对象 kubectl create -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-provisioner-rbac.yaml kubectl create -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-nodeplugin-rbac.yaml 9.创建PodSecurityPolicy kubectl create -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-provisioner-psp.yaml kubectl create -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-nodeplugin-psp.yaml 10.调整csi-cephfsplugin-provisioner.yaml和csi-cephfsplugin.yaml 将csi-cephfsplugin.yaml中的image部分调整为可访问镜像地址 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1 quay.io/cephcsi/cephcsi:v3.2.0 将csi-cephfsplugin-provisioner.yaml中的image部分调整为可访问镜像地址 quay.io/cephcsi/cephcsi:v3.2.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2 k8s.gcr.io/sig-storage/csi-attacher:v3.0.2 k8s.gcr.io/sig-storage/csi-resizer:v1.0.1 11.发布csi-cephfsplugin-provisioner.yaml和csi-cephfsplugin.yaml kubectl -n ceph-csi create -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-cephfsplugin-provisioner.yaml kubectl -n ceph-csi create -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-cephfsplugin.yaml 12.查看运行状态 kubectl get pod -n ceph-csi 13.生成StorageClass配置文件 b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294为ceph集群ID注意替换 生成配置文件 cat ceph-csi-3.2.0/deploy/cephfs/kubernetes/storageclass.yaml --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-csi-cephfs-sc provisioner: cephfs.csi.ceph.com parameters: clusterID: b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294 pool: cephfs_data fsName: k8s-cephfs imageFeatures: layering csi.storage.k8s.io/provisioner-secret-name: csi-cephfs-secret csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi csi.storage.k8s.io/controller-expand-secret-name: csi-cephfs-secret csi.storage.k8s.io/controller-expand-secret-namespace: ceph-csi csi.storage.k8s.io/node-stage-secret-name: csi-cephfs-secret csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi csi.storage.k8s.io/fstype: ext4 reclaimPolicy: Delete allowVolumeExpansion: true mountOptions: - discard EOF 14.创建StorageClass kubectl apply -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/storageclass.yaml 15.配置为默认storage class(已有默认，需要编辑更改) kubectl patch storageclass ceph-csi-rbd-sc -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' 16.查看storage class [root@ceph01 ~]# kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE ceph-csi-cephfs-sc cephfs.csi.ceph.com Delete Immediate true 5s ceph-csi-rbd-sc (default) rbd.csi.ceph.com Delete Immediate true 27h 17.创建pvc验证可用性 生成配置 cat ceph-csi-3.2.0/deploy/cephfs/kubernetes/pvc-demo.yaml --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: cephfs-pvc-demo namespace: default spec: storageClassName: ceph-csi-cephfs-sc accessModes: - ReadWriteMany resources: requests: storage: 1Gi EOF 创建 kubectl apply -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/pvc-demo.yaml 查看 [root@ceph01 ~]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE ceph-pvc-demo Bound pvc-360f8c5b-0f82-4f17-957b-a7eb5cf93f7e 20Gi RWO ceph-csi-rbd-sc 28h cephfs-pvc-demo Bound pvc-9400e0ab-2e44-4ce6-af39-a403441931e5 1Gi RWX ceph-csi-cephfs-sc 71s 18.pvc扩容 生成配置 cat ceph-csi-3.2.0/deploy/cephfs/kubernetes/nginx-demo.yaml apiVersion: v1 kind: Pod metadata: name: cephfs-testpv labels: role: web-frontend spec: containers: - name: web image: nginx ports: - name: web containerPort: 80 volumeMounts: - name: cephfs-pvc-demo mountPath: \"/usr/share/nginx/html\" volumes: - name: cephfs-pvc-demo persistentVolumeClaim: claimName: cephfs-pvc-demo EOF 发布 kubectl apply -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/nginx-demo.yaml 查看pvc [root@ceph01 ~]# kubectl get pvc cephfs-pvc-demo NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE cephfs-pvc-demo Bound pvc-9400e0ab-2e44-4ce6-af39-a403441931e5 1Gi RWX ceph-csi-cephfs-sc 7m2s 编辑修改pvc kubectl edit pvc cephfs-pvc-demo 修改以下内容，storage: 1Gi调整为storage: 10Gi ... spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi ... 再次查看pvc [root@ceph01 ~]# kubectl get pvc cephfs-pvc-demo NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE cephfs-pvc-demo Bound pvc-9400e0ab-2e44-4ce6-af39-a403441931e5 10Gi RWX ceph-csi-cephfs-sc 9m 与ceph rbd不同的是，扩容pvc时不需重启后端应用 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 14:53:09 "},"1.Linux基础/1.7存储/ceph/05运维管理/01服务启停.html":{"url":"1.Linux基础/1.7存储/ceph/05运维管理/01服务启停.html","title":"01服务启停","keywords":"","body":"服务启停 按节点启动所有ceph服务 systemctl start ceph.target 或 sudo systemctl start ceph-osd.target sudo systemctl start ceph-mon.target sudo systemctl start ceph-mds.target 按节点停止所有ceph服务 systemctl stop ceph\\*.service ceph\\*.target 或 sudo systemctl stop ceph-mon\\*.service ceph-mon.target sudo systemctl stop ceph-osd\\*.service ceph-osd.target sudo systemctl stop ceph-mds\\*.service ceph-mds.target 控制节点管理集群所有服务 启 sudo systemctl start ceph-osd@{id} sudo systemctl start ceph-mon@{hostname} sudo systemctl start ceph-mds@{hostname} 停 sudo systemctl stop ceph-osd@{id} sudo systemctl stop ceph-mon@{hostname} sudo systemctl stop ceph-mds@{hostname} Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 14:55:02 "},"1.Linux基础/1.7存储/ceph/05运维管理/02pool的CRUD.html":{"url":"1.Linux基础/1.7存储/ceph/05运维管理/02pool的CRUD.html","title":"02pool的CRUD","keywords":"","body":"池管理 查看pool [root@ceph01 ~]# ceph osd lspools 1 ssd-demo-pool 2 nvme-demo-pool 创建一个pool 格式 ceph osd pool create {pool-name} {pg-num} [{pgp-num}] [replicated] \\ [crush-rule-name] [expected-num-objects] 或 ceph osd pool create {pool-name} {pg-num} {pgp-num} erasure \\ [erasure-code-profile] [crush-rule-name] [expected_num_objects] 删除池 语法格式 ceph osd pool delete {pool-name} [{pool-name} --yes-i-really-really-mean-it] 修改配置 vim /etc/ceph/ceph.conf 添加如下： [mon] mon_allow_pool_delete=true 更新 cd /etc/ceph ceph-deploy --overwrite-conf config push ceph01 ceph02 ceph03 重启 systemctl restart ceph-mon.target 删除ddd-pool [root@ceph01 ceph]# ceph osd pool delete ddd-pool ddd-pool --yes-i-really-really-mean-it pool 'ddd-pool' removed 池重命名 ceph osd pool rename {current-pool-name} {new-pool-name} 显示池统计信息 [root@ceph01 ~]# rados df POOL_NAME USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS RD WR_OPS WR USED COMPR UNDER COMPR nvme-demo-pool 12 KiB 1 0 3 0 0 0 0 0 B 1 1 KiB 0 B 0 B ssd-demo-pool 12 KiB 1 0 3 0 0 0 0 0 B 1 1 KiB 0 B 0 B total_objects 2 total_used 30 GiB total_avail 26 TiB total_space 26 TiB 查看池io [root@ceph01 ~]# ceph osd pool stats ssd-demo-pool pool ssd-demo-pool id 1 nothing is going on 创建池快照 # ceph osd pool mksnap {pool-name} {snap-name} [root@ceph01 ~]# ceph osd pool mksnap ssd-demo-pool ssd-demo-pool-snap-20210301 created pool ssd-demo-pool snap ssd-demo-pool-snap-20210301 删除池快照 # ceph osd pool rmsnap {pool-name} {snap-name} [root@ceph01 ~]# ceph osd pool rmsnap ssd-demo-pool ssd-demo-pool-snap-20210301 removed pool ssd-demo-pool snap ssd-demo-pool-snap-20210301 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 15:01:25 "},"1.Linux基础/1.7存储/ceph/05运维管理/03pool的常用配置.html":{"url":"1.Linux基础/1.7存储/ceph/05运维管理/03pool的常用配置.html","title":"03pool的常用配置","keywords":"","body":"池的常用配置 PG配置 设置池的放置组数 ceph osd pool set {pool-name} pgp_num {pgp_num} 获取池的放置组数 ceph osd pool get {pool-name} pg_num 获取集群的PG统计信息 # ceph pg dump [--format {format}] ceph pg dump -f json 将池与应用程序关联 池在使用之前需要与应用程序相关联。将与cepfs一起使用的池或由RGW自动创建的池将自动关联。 用于与RBD一起使用的池应该使用RBD工具进行初始化。 # ceph osd pool application enable {pool-name} {application-name}(cephfs, rbd, rgw) [root@ceph01 ~]# ceph osd pool application enable ssd-demo-pool rbd enabled application 'rbd' on pool 'ssd-demo-pool' [root@ceph01 ~]# ceph osd pool application enable nvme-demo-pool cephfs enabled application 'cephfs' on pool 'nvme-demo-pool' 池配额 您可以为每个池的最大字节数和/或最大对象数设置池配额。 # ceph osd pool set-quota {pool-name} [max_objects {obj-count}] [max_bytes {bytes}] ceph osd pool set-quota data max_objects 10000 要删除配额，请将其值设置为0 ceph osd pool set-quota data max_objects 0 设置对象副本数 默认为3 ceph osd pool set {poolname} size {num-replicas} pg_autoscale_mode 放置组（PGs）是Ceph分发数据的内部实现细节。 过启用pg autoscaling，您可以允许集群提出建议或根据集群的使用方式自动调整PGs。 系统中的每个池都有一个pg_autoscale_mode属性，可以设置为off、on或warn： off：禁用此池的自动缩放。由管理员为每个池选择适当的PG数量。 on：启用给定池的PG计数的自动调整。 warn：当PG计数需要调整时发出健康警报（默认） 为指定池设置放置组数自动伸缩 # ceph osd pool set pg_autoscale_mode [root@ceph01 ~]# ceph osd pool set ssd-demo-pool pg_autoscale_mode on set pool 1 pg_autoscale_mode to on 设置集群内所有池放置组数自动伸缩 # ceph config set global osd_pool_default_pg_autoscale_mode ceph config set global osd_pool_default_pg_autoscale_mode on 查看集群内放置组数伸缩策略 [root@ceph01 ~]# ceph osd pool autoscale-status POOL SIZE TARGET SIZE RATE RAW CAPACITY RATIO TARGET RATIO EFFECTIVE RATIO BIAS PG_NUM NEW PG_NUM AUTOSCALE nvme-demo-pool 7 3.0 11178G 0.0000 1.0 256 32 warn ssd-demo-pool 7 3.0 15202G 0.0000 1.0 32 on 创建ddd-pool，并查看集群内放置组数伸缩策略 [root@ceph01 ~]# ceph osd pool create ddd-pool 1 1 pool 'ddd-pool' created [root@ceph01 ~]# ceph osd pool autoscale-status POOL SIZE TARGET SIZE RATE RAW CAPACITY RATIO TARGET RATIO EFFECTIVE RATIO BIAS PG_NUM NEW PG_NUM AUTOSCALE nvme-demo-pool 7 3.0 26380G 0.0000 1.0 256 32 warn ssd-demo-pool 7 3.0 15202G 0.0000 1.0 32 on ddd-pool 0 3.0 26380G 0.0000 1.0 1 32 on 查看池伸缩建议 POOL SIZE TARGET SIZE RATE RAW CAPACITY RATIO TARGET RATIO EFFECTIVE RATIO BIAS PG_NUM NEW PG_NUM AUTOSCALE nvme-demo-pool 7 3.0 26380G 0.0000 1.0 256 32 warn ssd-demo-pool 7 3.0 15202G 0.0000 1.0 32 on ddd-pool 0 3.0 26380G 0.0000 1.0 32 on SIZE：存储在池中的数据量 TARGET SIZE：管理员指定的他们希望最终存储在此池中的数据量 RATE：池的乘数，用于确定消耗了多少原始存储容量。例如，3副本池的比率为3.0，而k=4，m=2擦除编码池的比率为1.5。 RAW CAPACITY：负责存储此池(可能还有其他池)数据的OSD上的裸存储容量的总和。 RATIO：当前池正在消耗的总容量的比率(即RATIO = size * rate / raw capacity)。 TARGET RATIO：管理员指定的期望此池消耗的存储空间相对于设置了目标比率的其他池的比率。如果同时指定了目标大小字节和比率，则该比率优先 EFFECTIVE RATIO：有效比率是通过两种方式调整后的目标比率： 减去设置了目标大小的池预期使用的任何容量 在设置了目标比率的池之间规范化目标比率，以便它们共同以空间的其余部分为目标。例如，4个池的目标收益率为1.0，其有效收益率为0.25。 系统使用实际比率和有效比率中的较大者进行计算。 PG_NUM：池的当前PG数 NEW PG_NUM：池的pgu NUM期望值。它始终是2的幂，并且只有当期望值与当前值相差超过3倍时才会出现。 AUTOSCALE：pool_pg_autosacle模式，可以是on、off或warn。 自动缩放 允许集群根据使用情况自动扩展PGs是最简单的方法。Ceph将查看整个系统的总可用存储和PGs的目标数量，查看每个池中存储了多少数据，并尝试相应地分配PGs。 系统采用的方法相对保守，仅在当前pg(pg_num)的数量比它认为应该的数量少3倍以上时才对池进行更改。 每个OSD的pg目标数基于mon_target_pg_per_OSD可配置（默认值：100），可通过以下方式进行调整: ceph config set global mon_target_pg_per_OSD 100 指定池的期望大小 当第一次创建集群或池时，它将消耗集群总容量的一小部分，并且在系统看来似乎只需要少量的放置组。 但是，在大多数情况下，集群管理员都很清楚，随着时间的推移，哪些池将消耗大部分系统容量。 通过向Ceph提供此信息，可以从一开始就使用更合适数量的pg，从而防止pg_num的后续更改以及在进行这些调整时与移动数据相关的开销 池的目标大小可以通过两种方式指定：要么根据池的绝对大小（即字节），要么作为相对于设置了目标大小比的其他池的权重。 ddd-pool预计使用1G存储空间 ceph osd pool set ddd-pool target_size_bytes 1G 相对于设置了target_size_ratio的其他池，mpool预计将消耗1.0。 如果mpool是集群中唯一的池，这意味着预计将使用总容量的100%。 如果有第二个带有target_size_ratio1.0的池，那么两个池都希望使用50%的集群容量。 ceph osd pool set mypool target_size_ratio 1.0 池其他配置 查看可配置项 [root@ceph01 ~]# ceph osd pool -h General usage: ============== usage: ceph [-h] [-c CEPHCONF] [-i INPUT_FILE] [-o OUTPUT_FILE] [--setuser SETUSER] [--setgroup SETGROUP] [--id CLIENT_ID] [--name CLIENT_NAME] [--cluster CLUSTER] [--admin-daemon ADMIN_SOCKET] [-s] [-w] [--watch-debug] [--watch-info] [--watch-sec] [--watch-warn] [--watch-error] [--watch-channel {cluster,audit,*}] [--version] [--verbose] [--concise] [-f {json,json-pretty,xml,xml-pretty,plain}] [--connect-timeout CLUSTER_TIMEOUT] [--block] [--period PERIOD] Ceph administration tool optional arguments: -h, --help request mon help -c CEPHCONF, --conf CEPHCONF ceph configuration file -i INPUT_FILE, --in-file INPUT_FILE input file, or \"-\" for stdin -o OUTPUT_FILE, --out-file OUTPUT_FILE output file, or \"-\" for stdout --setuser SETUSER set user file permission --setgroup SETGROUP set group file permission --id CLIENT_ID, --user CLIENT_ID client id for authentication --name CLIENT_NAME, -n CLIENT_NAME client name for authentication --cluster CLUSTER cluster name --admin-daemon ADMIN_SOCKET submit admin-socket commands (\"help\" for help -s, --status show cluster status -w, --watch watch live cluster changes --watch-debug watch debug events --watch-info watch info events --watch-sec watch security events --watch-warn watch warn events --watch-error watch error events --watch-channel {cluster,audit,*} which log channel to follow when using -w/--watch. One of ['cluster', 'audit', '*'] --version, -v display version --verbose make verbose --concise make less verbose -f {json,json-pretty,xml,xml-pretty,plain}, --format {json,json-pretty,xml,xml-pretty,plain} --connect-timeout CLUSTER_TIMEOUT set a timeout for connecting to the cluster --block block until completion (scrub and deep-scrub only) --period PERIOD, -p PERIOD polling period, default 1.0 second (for polling commands only) Local commands: =============== ping Send simple presence/life test to a mon may be 'mon.*' for all mons daemon {type.id|path} Same as --admin-daemon, but auto-find admin socket daemonperf {type.id | path} [stat-pats] [priority] [] [] daemonperf {type.id | path} list|ls [stat-pats] [priority] Get selected perf stats from daemon/admin socket Optional shell-glob comma-delim match string stat-pats Optional selection priority (can abbreviate name): critical, interesting, useful, noninteresting, debug List shows a table of all available stats Run times (default forever), once per seconds (default 1) Monitor commands: ================= osd pool application disable {--yes-i-really-mean-it} disables use of an application on pool osd pool application enable {--yes-i-really-mean-it} enable use of an application [cephfs,rbd,rgw] on pool osd pool application get {} {} {} get value of key of application on pool osd pool application rm removes application metadata key on pool osd pool application set sets application metadata key to on pool osd pool autoscale-status report on pool pg_num sizing recommendation and intent osd pool cancel-force-backfill [...] restore normal recovery priority of specified pool osd pool cancel-force-recovery [...] restore normal recovery priority of specified pool osd pool create {} {replicated|erasure} create pool {} {} {} {} {} {} {} osd pool deep-scrub [...] initiate deep-scrub on pool osd pool force-backfill [...] force backfill of specified pool first osd pool force-recovery [...] force recovery of specified pool first osd pool get size|min_size|pg_num|pgp_num|crush_rule| get pool parameter hashpspool|nodelete|nopgchange|nosizechange|write_fadvise_dontneed| noscrub|nodeep-scrub|hit_set_type|hit_set_period|hit_set_count|hit_set_ fpp|use_gmt_hitset|target_max_objects|target_max_bytes|cache_target_ dirty_ratio|cache_target_dirty_high_ratio|cache_target_full_ratio| cache_min_flush_age|cache_min_evict_age|erasure_code_profile|min_read_ recency_for_promote|all|min_write_recency_for_promote|fast_read|hit_ set_grade_decay_rate|hit_set_search_last_n|scrub_min_interval|scrub_ max_interval|deep_scrub_interval|recovery_priority|recovery_op_ priority|scrub_priority|compression_mode|compression_algorithm| compression_required_ratio|compression_max_blob_size|compression_min_ blob_size|csum_type|csum_min_block|csum_max_block|allow_ec_overwrites| fingerprint_algorithm|pg_autoscale_mode|pg_autoscale_bias|pg_num_min| target_size_bytes|target_size_ratio osd pool get-quota obtain object or byte limits for pool osd pool ls {detail} list pools osd pool mksnap make snapshot in osd pool rename rename to osd pool repair [...] initiate repair on pool osd pool rm {} {--yes-i-really-really-mean-it} {-- remove pool yes-i-really-really-mean-it-not-faking} osd pool rmsnap remove snapshot from osd pool scrub [...] initiate scrub on pool osd pool set size|min_size|pg_num|pgp_num|pgp_num_actual| set pool parameter to crush_rule|hashpspool|nodelete|nopgchange|nosizechange|write_fadvise_ dontneed|noscrub|nodeep-scrub|hit_set_type|hit_set_period|hit_set_ count|hit_set_fpp|use_gmt_hitset|target_max_bytes|target_max_objects| cache_target_dirty_ratio|cache_target_dirty_high_ratio|cache_target_ full_ratio|cache_min_flush_age|cache_min_evict_age|min_read_recency_ for_promote|min_write_recency_for_promote|fast_read|hit_set_grade_ decay_rate|hit_set_search_last_n|scrub_min_interval|scrub_max_interval| deep_scrub_interval|recovery_priority|recovery_op_priority|scrub_ priority|compression_mode|compression_algorithm|compression_required_ ratio|compression_max_blob_size|compression_min_blob_size|csum_type| csum_min_block|csum_max_block|allow_ec_overwrites|fingerprint_ algorithm|pg_autoscale_mode|pg_autoscale_bias|pg_num_min|target_size_ bytes|target_size_ratio {--yes-i-really-mean-it} osd pool set-quota max_objects|max_bytes set object or byte limit on pool osd pool stats {} obtain stats from all pools, or from specified pool Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 15:11:03 "},"1.Linux基础/1.7存储/ceph/05运维管理/04配置pg放置组.html":{"url":"1.Linux基础/1.7存储/ceph/05运维管理/04配置pg放置组.html","title":"04配置pg放置组","keywords":"","body":"放置组数配置 # ceph osd pool create {pool-name} pg_num 必须选择pg_num的值，因为它（当前）无法自动计算。以下是一些常用值： 少于5 OSDs设置pg_num为128 5-10 OSDs设置pg_num为512 10-50 OSDs设置pg_num为1024 如果您有超过50个OSD，则需要使用pgcalc 计算pg_num值 (OSDs * 100) Total PGs = ------------ pool size 结果应始终四舍五入到最接近的二次方,只有2的幂才能平衡放置组中对象的数量。其他值将导致数据在您的OSD中的不均匀分布。 例如，对于具有200个OSD和3个副本的池大小的群集，您可以按以下方式估计PG的数量： (200 * 100) ----------- = 6667. 最接近该数值的2的幂等为8192 3 放置组解析 放置组（PG）聚合池中的对象，因为在每个对象的基础上跟踪对象放置和对象元数据在计算上非常昂贵， 即，具有数百万个对象的系统无法在每个对象的基础上真实地跟踪放置。 Ceph客户端将计算对象应该在哪个放置组中。它通过散列对象ID并基于定义池中pg的数量和池的ID应用一个操作来实现这一点 放置组中的对象内容存储在一组OSD中。例如，在大小为2的复制池中，每个放置组将在两个OSD上存储对象. 如果OSD#2失败，另一个将被分配到放置组#1，并将填充OSD#1中所有对象的副本。如果池大小从2更改为3，则会为放置组分配一个附加的OSD，并接收放置组中所有对象的副本。 放置组不拥有OSD,它们与来自同一池甚至其他池的其他放置组共享OSD。如果OSD#2失败，放置组#2还必须使用OSD#3还原对象的副本。 当放置组的数量增加时，新的放置组将被分配OSD。挤压功能的结果也将更改，以前放置组中的某些对象将复制到新放置组中，并从旧放置组中删除。 放置组权衡 数据持久性和所有OSD之间的均匀分布需要更多的放置组，但是它们的数量应该减少到最小以节省CPU和内存。 数据持久性 单个OSD故障后，数据丢失的风险会增大，直到其中包含的数据被完全恢复。让我们想象一个场景，在单个放置组中导致永久的数据丢失: OSD故障（可看作磁盘故障），包含对象的所有副本丢失。对于放置组内的所有对象，副本的数量突然从3个下降到2个。 Ceph通过选择一个新的OSD来重新创建所有对象的第三个副本，从而开始恢复此放置组。 同一放置组中的另一个OSD在新OSD完全填充第三个拷贝之前失败。一些对象将只有一个幸存的副本。 Ceph会选择另一个OSD，并继续复制对象，以恢复所需的副本数量。 同一放置组中的第三个OSD在恢复完成之前失败。如果此OSD包含对象的唯一剩余副本，则它将永久丢失。 在一个包含10个OSD、在3个复制池中有512个放置组的集群中，CRUSH将给每个放置组三个OSD。 最终，每个OSD将托管(512 * 3)/ 10 ~= 150个放置组。 因此，当第一个OSD失败时，上述场景将同时启动所有150个安置组的恢复。 正在恢复的150个安置组可能均匀地分布在剩余的9个OSDs上。 因此，每个剩余的OSD都可能向其他所有OSD发送对象副本，并接收一些新对象来存储，因为它们成为了新的放置组的一部分。 完成此恢复所需的时间完全取决于Ceph集群的体系结构。假设每个OSD由一台机器上的1TB SSD托管， 所有这些OSD都连接到10Gb/s交换机，单个OSD的恢复在M分钟内完成。 如果每台机器有两个OSD、没有SSD、1Gb/s交换机，它至少会慢一个数量级。 在这种大小的集群中，放置组的数量对数据持f久性几乎没有影响。它可能是128或8192，恢复不会慢或快。 但是，将同一个Ceph集群增加到20个OSD而不是10个OSD可能会加快恢复速度，从而显著提高数据持久性。 每个OSD现在只参与~75个放置组，而不是在只有10个OSD时参与~150个，而且仍然需要剩下的19个OSD执行相同数量的对象副本才能恢复。 但是，以前10个OSD每个必须复制大约100GB，现在它们每个必须复制50GB。 如果网络是瓶颈，那么恢复的速度将是现在的两倍。换句话说，当OSDs的数量增加时，恢复速度会更快。 如果这个集群增长到40个OSD，那么每个OSD只能承载35个放置组。 如果OSD故障，除非它被另一个瓶颈阻塞，否则恢复将继续加快。 但是，如果这个集群增长到200个OSD，每个OSD只能承载~7个放置组。 如果一个OSD故障，那么在这些放置组中最多21个OSD（7*3）之间会发生恢复：恢复所需的时间将比有40个OSD时的集群长，这意味着放置组的数量应该增加。 无论恢复时间有多短，第二个OSD都有可能在恢复过程中失败。 在上述10个osd集群中，如果其中任何一个失败，那么~17个放置组（即~150/9个正在恢复的放置组）将只有一个幸存副本。 如果剩余的8个OSD中的任何一个失败，那么两个放置组的最后一个对象很可能会丢失（即大约17/8个放置组，只恢复了剩余的一个副本）。 当群集的大小增加到20个osd时，由于丢失3个osd而损坏的放置组的数量会下降。 丢失的第二个OSD将降级~4（即恢复~75/19个放置组），而不是~17，并且丢失的第三个OSD只有在包含幸存副本的四个OSD之一时才会丢失数据。 换句话说，如果在恢复时间范围内丢失一个OSD的概率为0.0001%，则在具有10个OSD的集群中，丢失的概率从17×10×0.0001%变为具有20个OSD的集群中的4×20×0.0001% 简而言之，更多的osd意味着更快的恢复和更低的导致永久性丢失放置组的级联故障风险。就数据持久性而言，512或4096个放置组在少于50个osd的集群中大致相当。 池中的对象分布 理想情况下，对象均匀分布在每个放置组中。由于CRUSH计算每个对象的放置组， 但实际上不知道该放置组中的每个OSD中存储了多少数据，因此放置组的数量与OSD的数量之间的比率可能会显著影响数据的分布。 例如，如果在一个三副本池中有十个OSD的单个放置组，那么只会使用三个OSD，因为CRUSH没有其他选择。 当有更多的放置组可用时，对象更可能均匀地分布在其中。CRUSH还尽一切努力将osd均匀地分布在所有现有的放置组中。 不均匀的数据分布可能是由osd和放置组之间的比率以外的因素造成的。 由于挤压不考虑对象的大小，一些非常大的对象可能会造成不平衡。 假设100万个4K对象（总共4GB）均匀分布在10个OSD上的1024个放置组中。他们将在每个OSD上使用4GB/10=400MB。 如果将一个400MB对象添加到池中，则支持放置该对象的放置组的三个OSD将被400MB+400MB=800MB填充，而其他七个OSD将仅被400MB占用。 内存、CPU和网络使用情况 对于每个放置组，osd和mon始终需要内存、网络和CPU，甚至在恢复期间需要更多。通过在放置组中聚集对象来共享此开销是它们存在的主要原因之一。 最小化放置组的数量可以节省大量资源。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 15:12:01 "},"1.Linux基础/1.7存储/ceph/05运维管理/05配置管理块存储.html":{"url":"1.Linux基础/1.7存储/ceph/05运维管理/05配置管理块存储.html","title":"05配置管理块存储","keywords":"","body":"rbd管理 创建rbd 1.创建池 [root@ceph01 ~]# ceph osd pool create rbd-demo-pool 64 64 pool 'rbd-demo-pool' created 2.设置配额 [root@ceph01 ~]# ceph osd pool set-quota rbd-demo-pool max_bytes 1G set-quota max_bytes = 1073741824 for pool rbd-demo-pool 3.关联rbd应用 [root@ceph01 ~]# ceph osd pool application enable rbd-demo-pool rbd enabled application 'rbd' on pool 'rbd-demo-pool' 4.初始化 rbd pool init rbd-demo-pool 5.创建rbd用户 语法格式 ceph auth get-or-create client.{ID} mon 'profile rbd' osd 'profile {profile name} [pool={pool-name}][, profile ...]' mgr 'profile rbd [pool={pool-name}]' 创建ID为qemu、对rbd-demo-pool池有读写权限的用户 ceph auth get-or-create client.qemu mon 'profile rbd' osd 'profile rbd pool=rbd-demo-pool' mgr 'profile rbd pool=rbd-demo-pool' -o /etc/ceph/ceph.client.qemu.keyring 6.创建rbd映像 在将块设备添加到节点之前，必须先在Ceph存储集群中为其创建映像。要创建块设备映像，请执行以下操作： # rbd create --size {megabytes} {pool-name}/{image-name} rbd create --size 1G rbd-demo-pool/rbd-demo-image 查看块设备映像 1.查看池内映像 # rbd ls {poolname} [root@ceph01 ~]# rbd ls rbd-demo-pool rbd-demo-image 2.查看块设备映像信息 [root@ceph01 ~]# rbd info rbd-demo-pool/rbd-demo-image rbd image 'rbd-demo-image': size 1 GiB in 256 objects order 22 (4 MiB objects) snapshot_count: 0 id: d3ef49824934 block_name_prefix: rbd_data.d3ef49824934 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten op_features: flags: create_timestamp: Mon Mar 1 15:48:05 2021 access_timestamp: Mon Mar 1 15:48:05 2021 modify_timestamp: Mon Mar 1 15:48:05 2021 块设备缩容 1.收缩大小为256M [root@ceph01 ~]# rbd resize --size 256M rbd-demo-pool/rbd-demo-image --allow-shrink Resizing image: 100% complete...done. 2.查看块设备映像信息 [root@ceph01 ~]# rbd info rbd-demo-pool/rbd-demo-image rbd image 'rbd-demo-image': size 256 MiB in 64 objects order 22 (4 MiB objects) snapshot_count: 0 id: d3ef49824934 block_name_prefix: rbd_data.d3ef49824934 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten op_features: flags: create_timestamp: Mon Mar 1 15:48:05 2021 access_timestamp: Mon Mar 1 15:48:05 2021 modify_timestamp: Mon Mar 1 15:48:05 2021 块设备扩容 扩容大小上限为池配额大小 扩容大小至1G [root@ceph01 ~]# rbd resize --size 1G rbd-demo-pool/rbd-demo-image --allow-shrink Resizing image: 100% complete...done. 查看块设备映像信息 [root@ceph01 ~]# rbd info rbd-demo-pool/rbd-demo-image rbd image 'rbd-demo-image': size 1 GiB in 256 objects order 22 (4 MiB objects) snapshot_count: 0 id: d3ef49824934 block_name_prefix: rbd_data.d3ef49824934 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten op_features: flags: create_timestamp: Mon Mar 1 15:48:05 2021 access_timestamp: Mon Mar 1 15:48:05 2021 modify_timestamp: Mon Mar 1 15:48:05 2021 删除块设备映像 # rbd rm {pool-name}/{image-name} [root@ceph01 ~]# rbd rm rbd-demo-pool/rbd-demo-image Removing image: 100% complete...done. 挂载块设备 1.删除原有yum源repo文件 rm -f /etc/yum.repos.d/*.repo 2.创建yum源文件（客户端） online curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo offline 下载以下文件上传至/etc/yum.repos.d/ Centos-7.repo epel-7.repo 3.配置ceph镜像源仓库 cat > /etc/yum.repos.d/ceph.repo 4.配置yum代理 适用于主机通过代理访问互联网场景 以下变量注意替换 username: 代理用户名 password: 代理用户密码 proxy_host: 代理IP地址 proxy_port: 代理端口 echo \"proxy=http://username:password@proxy_host:proxy_port\" >> /etc/yum.conf 5.安装ceph-common yum install -y ceph-common 6.拷贝配置文件 mkdir -p /etc/ceph 7.客户端创建挂载目录 mkdir -p /ceph chmod 777 /ceph 8.从服务端scp以下文件至客户端/etc/ceph下 /etc/ceph/ceph.client.qemu.keyring /etc/ceph/ceph.conf scp /etc/ceph/{ceph.conf,ceph.client.admin.keyring} ip:/etc/ceph/ 9.映射块设备 [root@localhost ~]# rbd map rbd-demo-pool/rbd-demo-image --name client.qemu /dev/rbd0 [root@localhost ~]# echo \"rbd-demo-pool/rbd-demo-image id=qemu,keyring=/etc/ceph/ceph.client.qemu.keyring\" >> /etc/ceph/rbdmap 10.格式化块设备 mkfs.ext4 -q /dev/rbd0 11.挂载使用 mount /dev/rbd0 /ceph 12.查看挂载 lsblk 13.查看块设备映射 [root@localhost ~]# rbd device list id pool namespace image snap device 0 rbd-demo-pool rbd-demo-image - /dev/rbd0 14.修改fstab，设置开机挂载 echo \"/dev/rbd0 /ceph ext4 defaults,noatime,_netdev 0 0\" >> /etc/fstab 15.配置开机自启动 vim /etc/init.d/rbdmap 填充以下内容 #!/bin/bash #chkconfig: 2345 80 60 #description: start/stop rbdmap ### BEGIN INIT INFO # Provides: rbdmap # Required-Start: $network # Required-Stop: $network # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Ceph RBD Mapping # Description: Ceph RBD Mapping ### END INIT INFO DESC=\"RBD Mapping\" RBDMAPFILE=\"/etc/ceph/rbdmap\" . /lib/lsb/init-functions #. /etc/redhat-lsb/lsb_log_message，加入此行后不正长 do_map() { if [ ! -f \"$RBDMAPFILE\" ]; then echo \"$DESC : No $RBDMAPFILE found.\" exit 0 fi echo \"Starting $DESC\" # Read /etc/rbdtab to create non-existant mapping newrbd= RET=0 while read DEV PARAMS; do case \"$DEV\" in \"\"|\\#*) continue ;; */*) ;; *) DEV=rbd/$DEV ;; esac OIFS=$IFS IFS=',' for PARAM in ${PARAMS[@]}; do CMDPARAMS=\"$CMDPARAMS --$(echo $PARAM | tr '=' ' ')\" done IFS=$OIFS if [ ! -b /dev/rbd/$DEV ]; then echo $DEV rbd map $DEV $CMDPARAMS [ $? -ne \"0\" ] && RET=1 newrbd=\"yes\" fi done 16.赋权 yum install redhat-lsb -y chmod +x /etc/init.d/rbdmap service rbdmap start chkconfig rbdmap on Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 15:18:14 "},"1.Linux基础/1.7存储/ceph/05运维管理/111卸载.html":{"url":"1.Linux基础/1.7存储/ceph/05运维管理/111卸载.html","title":"111卸载","keywords":"","body":"删除文件系统 删除文件系统 查看文件系统 [root@ceph01 ~]# ceph fs ls name: k8s-cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ] 删除文件系统 [root@ceph01 ~]# ceph fs fail k8s-cephfs k8s-cephfs marked not joinable; MDS cannot join the cluster. All MDS ranks marked failed. [root@ceph01 ~]# ceph fs rm k8s-cephfs --yes-i-really-mean-it 删除池 查看池 [root@ceph01 ~]# ceph osd pool ls ssd-demo-pool nvme-demo-pool rbd-demo-pool rbd-k8s-pool cephfs_data cephfs_metadata 删除池 ceph osd pool delete ssd-demo-pool ssd-demo-pool --yes-i-really-really-mean-it ceph osd pool delete nvme-demo-pool nvme-demo-pool --yes-i-really-really-mean-it ceph osd pool delete rbd-demo-pool rbd-demo-pool --yes-i-really-really-mean-it ceph osd pool delete rbd-k8s-pool rbd-k8s-pool --yes-i-really-really-mean-it ceph osd pool delete cephfs_data cephfs_data --yes-i-really-really-mean-it ceph osd pool delete cephfs_metadata cephfs_metadata --yes-i-really-really-mean-it 删除OSD ceph管理节点执行 #!/bin/bash osd_list=`ceph osd ls` for var in $osd_list; do ceph osd crush rm osd.$var ceph auth del osd.$var done ceph osd down all ceph osd out all ceph osd rm all 所有ceph osd节点执行 for i in `ls /var/lib/ceph/osd/`; do umount /var/lib/ceph/osd/$i done 对于umount: /var/lib/ceph/osd/ceph-*：目标忙。的情况，执行以下操作 [root@node2 ~]# fuser -mv /var/lib/ceph/osd/ceph-1 用户 进程号 权限 命令 /var/lib/ceph/osd/ceph-1: root kernel mount /var/lib/ceph/osd/ceph-1 ceph 5979 F.... ceph-osd [root@node2 ~]# kill -9 5979 [root@node2 ~]# fuser -mv /var/lib/ceph/osd/ceph-1 用户 进程号 权限 命令 /var/lib/ceph/osd/ceph-1: root kernel mount /var/lib/ceph/osd/ceph-1 [root@node2 ~]# umount /var/lib/ceph/osd/ceph-1 手动擦除盘上数据(所有数据节点) # 从DM中移除硬盘对应的编码 dmsetup remove_all # 格式化分区 yum install gdisk -y sgdisk -z /dev/ ceph管理节点执行 ceph-deploy disk zap /dev/ 卸载组件并清空目录 ceph-deploy purge ceph01 ceph02 ceph03 ceph-deploy purgedata ceph01 ceph02 ceph03 ceph-deploy forgetkeys Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 15:20:50 "},"1.Linux基础/1.7存储/raid/":{"url":"1.Linux基础/1.7存储/raid/","title":"raid","keywords":"","body":"Raid 简介 磁盘阵列（Redundant Arrays of Independent Drives，RAID），有“独立磁盘构成的具有冗余能力的阵列”之意。 磁盘阵列是由很多块独立的磁盘，组合成一个容量巨大的磁盘组，利用个别磁盘提供数据所产生加成效果提升整个磁盘系统效能。利用这项技术，将数据切割成许多区段，分别存放在各个硬盘上。 磁盘阵列还能利用同位检查（Parity Check）的观念，在数组中任意一个硬盘故障时，仍可读出数据，在数据重构时，将数据经计算后重新置入新硬盘中。 功能 RAID技术主要有以下三个基本功能： 通过对磁盘上的数据进行条带化，实现对数据成块存取，减少磁盘的机械寻道时间，提高了数据存取速度。 通过对一个阵列中的几块磁盘同时读取，减少了磁盘的机械寻道时间，提高数据存取速度。 通过镜像或者存储奇偶校验信息的方式，实现了对数据的冗余保护 分类 磁盘阵列其样式有三种，一是外接式磁盘阵列柜、二是内接式磁盘阵列卡，三是利用软件来仿真。 外接式磁盘阵列柜最常被使用大型服务器上，具可热交换（Hot Swap）的特性，不过这类产品的价格都很贵。 内接式磁盘阵列卡，因为价格便宜，但需要较高的安装技术，适合技术人员使用操作。硬件阵列能够提供在线扩容、动态修改阵列级别、自动数据恢复、驱动器漫游、超高速缓冲等功能。它能提供性能、数据保护、可靠性、可用性和可管理性的解决方案。阵列卡专用的处理单元来进行操作。 利用软件仿真的方式，是指通过网络操作系统自身提供的磁盘管理功能将连接的普通SCSI卡上的多块硬盘配置成逻辑盘，组成阵列。软件阵列可以提供数据冗余功能，但是磁盘子系统的性能会有所降低，有的降低幅度还比较大，达30%左右。因此会拖累机器的速度，不适合大数据流量的服务器。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 16:04:38 "},"1.Linux基础/1.7存储/raid/01-raid0.html":{"url":"1.Linux基础/1.7存储/raid/01-raid0.html","title":"01-raid0","keywords":"","body":"RAID 0 原理 数据分片至不同的磁盘上 磁盘数量 2块以上 冗余能力 不具有冗余能力 磁盘利用率 100% 适用场景 数据安全性要求不高 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 15:48:04 "},"1.Linux基础/1.7存储/raid/02-raid1.html":{"url":"1.Linux基础/1.7存储/raid/02-raid1.html","title":"02-raid1","keywords":"","body":"RAID 1 原理 把一个磁盘的数据镜像到另一个磁盘上 磁盘数量 偶数块（保证副本集非0） 冗余能力 具有冗余能力 磁盘利用率 50% 适用场景 保存关键性的重要数据：系统盘 实现raid1 1.虚拟机添加两块硬盘 2.安装raid管理工具mdadm yum install -y mdadm 3.查看磁盘情况 fdisk -l 4.创建raid1 -n表示副本集 mdadm -C /dev/md1 -n 2 -l 1 -a yes /dev/sd{b,c} 5.查看raid信息 cat /proc/mdstat 6.格式化 mkfs.ext4 -j -b 4096 /dev/md1 7.挂载 mkdir /mnt1 mount /dev/md1 /mnt1 echo \"/dev/md1 /mnt1 ext4 defaults 0 0\" >> /etc/fstab 8.写数据 mkdir /mnt1/abc && touch /mnt1/abc/123 9.模拟损坏其中一个磁盘块 mdadm /dev/md1 -f /dev/sdc 10.查看raid信息 11.新增磁盘设备，添加到md1 mdadm /dev/md1 -a /dev/sdd 12.查看raid信息 13.删除已损坏的硬盘 mdadm /dev/md1 -r /dev/sdc 14.关闭raid mdadm -S /dev/md1 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 15:52:03 "},"1.Linux基础/1.7存储/raid/03-raid01.html":{"url":"1.Linux基础/1.7存储/raid/03-raid01.html","title":"03-raid01","keywords":"","body":"RAID0+1 原理 把一个磁盘的数据镜像到另一个磁盘上 磁盘数量 至少4个硬盘 冗余能力 具有冗余能力 磁盘利用率 50% 适用场景 保存关键性的重要数据 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 15:53:41 "},"1.Linux基础/1.7存储/raid/04-raid5.html":{"url":"1.Linux基础/1.7存储/raid/04-raid5.html","title":"04-raid5","keywords":"","body":"raid5 原理 磁盘数量 至少3个硬盘 冗余能力 具有冗余能力 磁盘利用率 (N-1)/N，即只浪费一块磁盘用于奇偶校验 适用场景 保存关键性的重要数据 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 16:04:38 "},"1.Linux基础/1.7存储/raid/05-raid10.html":{"url":"1.Linux基础/1.7存储/raid/05-raid10.html","title":"05-raid10","keywords":"","body":"raid10 Raid 10是一个Raid 1与Raid0的组合体，它是利用奇偶校验实现条带集镜像， 所以它继承了Raid0的快速和Raid1的安全。 我们知道，RAID 1在这里就是一个冗余的备份阵列，而RAID 0则负责数据的读写阵列。 其实，下图只是一种RAID 10方式，更多的情况是从主通路分出两路，做Striping操作，即把数据分割， 而这分出来的每一路则再分两路，做Mirroring操作，即互做镜像 磁盘数量 至少4个硬盘 冗余能力 具有冗余能力 磁盘利用率 50% 适用场景 保存关键性的重要数据 注意一下Raid 10和Raid 01的区别 RAID01又称为RAID0+1，先进行条带存放（RAID0），再进行镜像（RAID1）。 RAID10又称为RAID1+0，先进行镜像（RAID1），再进行条带存放（RAID0）。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 16:06:27 "},"1.Linux基础/1.8网络/01-nat.html":{"url":"1.Linux基础/1.8网络/01-nat.html","title":"01-nat","keywords":"","body":" 清除nat规则 iptables -t nat -F 查看nat规则 iptables -t nat -nvL Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"1.Linux基础/1.8网络/02-路由配置.html":{"url":"1.Linux基础/1.8网络/02-路由配置.html","title":"02-路由配置","keywords":"","body":" windows route -p add 192.168.146.0 mask 255.255.255.0 192.168.121.1 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"1.Linux基础/1.8网络/03-wireshark.html":{"url":"1.Linux基础/1.8网络/03-wireshark.html","title":"03-wireshark","keywords":"","body":"keepalive及444状态码 keepalive 该配置官方文档给出的默认值为75s 官方文档地址 1、nginx keepalive配置方便起见配置为30s #配置于nginx.conf 中的 http{}内 keepalive_timeout 30s; 2、nginx server配置 server { listen 8089; location /123 { proxy_pass http://192.168.1.145:8080; } location / { index html/index.html; } } 3、开启wireshark监听虚拟网卡（nginx部署于本地vmware上的虚机，nat模式） 4、使用POSTMAN发送请求 5、wireshark过滤观察 keepalive与断开连接 444状态码 适用于屏蔽非安全请求或DDOS防御 1、nginx server配置 server { listen 8089; location /123 { proxy_pass http://192.168.1.145:8080; } location / { index html/index.html; } location /abc { return 444; } } 2、开启wireshark监听虚拟网卡（nginx部署于本地vmware上的虚机，nat模式） 3、发送请求 4、wireshark过滤观察 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:16 "},"1.Linux基础/1.8网络/04-shadows.html":{"url":"1.Linux基础/1.8网络/04-shadows.html","title":"04-shadows","keywords":"","body":"shadowsocks客户端 项目地址 1.下载安装包 windows 2.解压，运行 加压到本地目录 3.配置 配置服务端IP、端口、加密算法、服务端口 4、配置系统模式 在任务栏找到 Shadowsocks 图标，选取PAC模式 其他使用说明 5、测试是否可用 twitter Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:16 "},"1.Linux基础/1.9安全/":{"url":"1.Linux基础/1.9安全/","title":"1.9安全","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"1.Linux基础/1.9安全/01-禁ping.html":{"url":"1.Linux基础/1.9安全/01-禁ping.html","title":"01-禁ping","keywords":"","body":"禁ping echo \"net.ipv4.icmp_echo_ignore_all=1\" >> /etc/sysctl.conf sysctl -p Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"1.Linux基础/1.9安全/02-关闭ICMP_TIMESTAMP应答.html":{"url":"1.Linux基础/1.9安全/02-关闭ICMP_TIMESTAMP应答.html","title":"02-关闭ICMP_TIMESTAMP应答","keywords":"","body":"关闭ICMP_TIMESTAMP应答 iptables -I INPUT -p ICMP --icmp-type timestamp-request -m comment --comment \"deny ICMP timestamp\" -j DROP iptables -I INPUT -p ICMP --icmp-type timestamp-reply -m comment --comment \"deny ICMP timestamp\" -j DROP Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"1.Linux基础/1.9安全/03-锁定系统关键文件.html":{"url":"1.Linux基础/1.9安全/03-锁定系统关键文件.html","title":"03-锁定系统关键文件","keywords":"","body":"锁定系统关键文件 防止被篡改 chattr +i /etc/passwd /etc/shadow /etc/group /etc/gshadow /etc/inittab Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"1.Linux基础/1.9安全/04-ssh加固.html":{"url":"1.Linux基础/1.9安全/04-ssh加固.html","title":"04-ssh加固","keywords":"","body":"ssh加固 限制root用户直接登录 sed -i \"s#PermitRootLogin yes#PermitRootLogin no#g\" /etc/ssh/sshd_config systemctl restart sshd 修改允许密码错误次数 sed -i \"/MaxAuthTries/d\" /etc/ssh/sshd_config echo \"MaxAuthTries 3\" >> /etc/ssh/sshd_config systemctl restart sshd 关闭AgentForwarding和TcpForwarding sed -i \"/AgentForwarding/d\" /etc/ssh/sshd_config sed -i \"/TcpForwarding/d\" /etc/ssh/sshd_config echo \"AllowAgentForwarding no\" >> /etc/ssh/sshd_config echo \"AllowTcpForwarding no\" >> /etc/ssh/sshd_config systemctl restart sshd 关闭UseDNS sed -i \"/UseDNS/d\" /etc/ssh/sshd_config echo \"UseDNS no\" >> /etc/ssh/sshd_config systemctl restart sshd Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"1.Linux基础/1.9安全/05-升级sudo版本.html":{"url":"1.Linux基础/1.9安全/05-升级sudo版本.html","title":"05-升级sudo版本","keywords":"","body":"升级sudo版本 CVE-2021-3156等 sudo-1.9.7-3.el7.x86_64.rpm rpm -Uvh sudo-1.9.7-3.el7.x86_64.rpm 验证 sudo -V Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"1.Linux基础/1.9安全/06-设置会话超时.html":{"url":"1.Linux基础/1.9安全/06-设置会话超时.html","title":"06-设置会话超时","keywords":"","body":"设置会话超时（5分钟） echo \"export TMOUT=300\" >>/etc/profile . /etc/profile Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"1.Linux基础/1.9安全/07-隐藏系统版本信息.html":{"url":"1.Linux基础/1.9安全/07-隐藏系统版本信息.html","title":"07-隐藏系统版本信息","keywords":"","body":"隐藏系统版本信息 mv /etc/issue /etc/issue.bak mv /etc/issue.net /etc/issue.net.bak Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"1.Linux基础/1.9安全/08-禁止Control-Alt-Delete键盘重启系统命令.html":{"url":"1.Linux基础/1.9安全/08-禁止Control-Alt-Delete键盘重启系统命令.html","title":"08-禁止Control-Alt-Delete键盘重启系统命令","keywords":"","body":"禁止Control-Alt-Delete 键盘重启系统命令 rm -rf /usr/lib/systemd/system/ctrl-alt-del.target Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"1.Linux基础/1.9安全/09-密码加固.html":{"url":"1.Linux基础/1.9安全/09-密码加固.html","title":"09-密码加固","keywords":"","body":"密码加固 PASS_MAX_DAYS=`grep -e ^PASS_MAX_DAYS /etc/login.defs |awk '{print $2}'` if [ $PASS_MAX_DAYS -gt 90 ];then echo \"密码最长保留期限为：$PASS_MAX_DAYS, 更改为90天\" sed -i \"/^PASS_MAX_DAYS/d\" /etc/login.defs echo \"PASS_MAX_DAYS 90\" >> /etc/login.defs fi PASS_MIN_DAYS=`grep -e ^PASS_MIN_DAYS /etc/login.defs |awk '{print $2}'` if [ $PASS_MIN_DAYS -ne 1 ];then echo \"密码最段保留期限为：$PASS_MIN_DAYS, 更改为1天\" sed -i \"/^PASS_MIN_DAYS/d\" /etc/login.defs echo \"PASS_MIN_DAYS 1\" >> /etc/login.defs fi PASS_MIN_LEN=`grep -e ^PASS_MIN_LEN /etc/login.defs |awk '{print $2}'` if [ $PASS_MIN_LEN -lt 8 ];then echo \"密码最少字符为：$PASS_MIN_LEN, 更改为8\" sed -i \"/^PASS_MIN_LEN/d\" /etc/login.defs echo \"PASS_MIN_LEN 8\" >> /etc/login.defs fi PASS_WARN_AGE=`grep -e ^PASS_WARN_AGE /etc/login.defs |awk '{print $2}'` if [ $PASS_WARN_AGE -ne 7 ];then echo \"密码到期前$PASS_MIN_LEN天提醒, 更改为7\" sed -i \"/^PASS_WARN_AGE/d\" /etc/login.defs echo \"PASS_WARN_AGE 7\" >> /etc/login.defs fi Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"1.Linux基础/111.trouble shooting/cpu/cpu占用过高.html":{"url":"1.Linux基础/111.trouble shooting/cpu/cpu占用过高.html","title":"cpu占用过高","keywords":"","body":"CPU负载相关 排查所用工具集 top: 系统自带 ps: 系统自带 pstack：Linux 命令。可以查看某个进程的当前线程栈运行情况。 jstack：Java提供的命令。可以查看某个进程的当前线程栈运行情况。根据这个命令的输出可以定位某个进程的所有线程的当前运行状态、运行代码，以及是否死锁等等。 安装工具 通用进程排查工具 yum install gdb strace -y 排查java程序安装以下工具 yum install -y java-1.8.0-openjdk-devel.x86_64 CPU高负载排查过程 一个应用占用CPU很高，除了确实是计算密集型应用之外，通常原因都是出现了死循环。CPU负载过高解决问题过程 使用top命令定位异常进程（cpu使用率>100%），获取pid top - 00:32:54 up 1:14, 2 users, load average: 0.93, 0.79, 0.50 Tasks: 132 total, 2 running, 130 sleeping, 0 stopped, 0 zombie %Cpu(s): 23.8 us, 1.4 sy, 0.0 ni, 74.8 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 3861280 total, 3273408 free, 209176 used, 378696 buff/cache KiB Swap: 2097148 total, 2097148 free, 0 used. 3419296 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 64455 root 20 0 113288 1184 1008 R 100.0 0.0 0:04.44 sh 非java程序排查步骤 追踪该进程 strace -o output.txt -T -tt -e trace=all -p 64455 java程序排查步骤 查找该进程下的所有线程资源占用情况 [root@localhost ~]# ps -mp 64455 -o THREAD,tid,time USER %CPU PRI SCNT WCHAN USER SYSTEM TID TIME root 100 - - - - - - 00:39:36 root 100 19 - - - - 64455 00:39:36 获取异常线程16进制值 [root@localhost ~]# printf \"%x\\n\" 64455 fbc7 查看堆栈，获取线程调用信息 jstack | grep -C5 --color 参考文档 服务器CPU负载过高，如何定位问题 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:16 "},"1.Linux基础/skill.html":{"url":"1.Linux基础/skill.html","title":"skill","keywords":"","body":"技巧 linux读取移动硬盘数据 安装ntfs-3g ```shell script yum -y install ntfs-3g 或离线安装，[离线包下载地址](https://tuxera.com/opensource/ntfs-3g_ntfsprogs-2017.3.23.tgz) ```shell script yum -y install gcc tar -zxvf ntfs-3g_ntfsprogs-2017.3.23.tgz cd ntfs-3g_ntfsprogs-2017.3.23/ ./configure && make && make install 查询移动硬盘所在设备接口 ```shell script [root@node3 windows]# fdisk -l | grep NTFS WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. /dev/sdl1 2048 3907026943 1953512448 7 HPFS/NTFS/exFAT > 创建挂载点，挂载 ```shell script mkdir -p /ntfs mount -t ntfs-3g /dev/sdl1 /ntfs Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:06 "},"2.容器/docker/cli/cli.html":{"url":"2.容器/docker/cli/cli.html","title":"cli","keywords":"","body":" Table of Contents generated with DocToc docker源码阅读笔记 命令行 cobra实现 docker源码阅读笔记 命令行 基于cobra开发 cobra实现 operationSubCommands方法 解析命令行入参，返回参数数组： 若命令存在（未被移除 && 非隐藏类型命令）且不含子命令，依次放入数组中 func operationSubCommands(cmd *cobra.Command) []*cobra.Command { var cmds []*cobra.Command for _, sub := range cmd.Commands() { if sub.IsAvailableCommand() && !sub.HasSubCommands() { cmds = append(cmds, sub) } } return cmds } hasSubCommands方法 判断docker命令是否含有子命令或参数 调用operationSubCommands对返回的数组进行容量判断，长度大于0返回true（即含有子命令） 如docker ps不含有子命令，而docker save含有子命令（-o） func hasSubCommands(cmd *cobra.Command) bool { return len(operationSubCommands(cmd)) > 0 } FlagErrorFunc方法 判断docker命令合法性： func FlagErrorFunc(cmd *cobra.Command, err error) error { if err == nil { return nil } usage := \"\" if cmd.HasSubCommands() { usage = \"\\n\\n\" + cmd.UsageString() } return StatusError{ Status: fmt.Sprintf(\"%s\\nSee '%s --help'.%s\", err, cmd.CommandPath(), usage), StatusCode: 125, } } Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:06 "},"2.容器/docker/cmd/cmd.html":{"url":"2.容器/docker/cmd/cmd.html","title":"cmd","keywords":"","body":" Table of Contents generated with DocToc 清理容器 清理镜像 普通用户访问docker sudo groupadd docker #添加docker用户组 usermod -aG docker $USER newgrp docker #更新用户组 docker ps 清理容器 方式一： 显示所有的容器，过滤出Exited状态的容器，取出这些容器的ID， sudo docker ps -a|grep Exited|awk '{print $1}' 查询所有的容器，过滤出Exited状态的容器，列出容器ID，删除这些容器 sudo docker rm `docker ps -a|grep Exited|awk '{print $1}'` 方式二： 删除所有未运行的容器（已经运行的删除不了，未运行的就一起被删除了） sudo docker rm $(sudo docker ps -a -q) 方式三： 根据容器的状态，删除Exited状态的容器 sudo docker rm $(sudo docker ps -qf status=exited) 清理docker垃圾 docker image prune -a -f Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:06 "},"2.容器/docker/image/image.html":{"url":"2.容器/docker/image/image.html","title":"image","keywords":"","body":" Table of Contents generated with DocToc 定义转义符 环境变量 忽略文件 FROM RUN CMD RUN vs CMD LABEL MAINTAINER EXPOSE ENV ADD COPY COPY VS ADD ENTRYPOINT shell form vs exec form USER WORKDIR VOLUME SHELL HEALTHCHECK STOPSIGNAL ONBUILD 定义转义符 适用于windows平台 # escape=` FROM microsoft/nanoserver COPY testfile.txt c:\\ RUN dir c:\\ 环境变量 FROM busybox ENV foo /bar WORKDIR ${foo} # WORKDIR /bar ADD . $foo # ADD . /bar COPY \\$foo /quux # COPY $foo /quux ${variable_name}支持bash一些标准： ${variable:-word} variable为空则取word的值 ${variable:+word} variable非空则取word的值 支持环境变量得到docker指令如下： ADD COPY ENV EXPOSE FROM LABEL STOPSIGNAL USER VOLUME WORKDIR 忽略文件 当执行构建build时docker-cli会先在指定的上下文目录中，寻找.dockerignore文件，docker-cli根据文件内容，排除context的路基目录或文件，随后再将信息发送给docker-daemon 例子如下： # comment */temp* */*/temp* temp? 不忽略的话，会全部提交过去，如果当前上下文目录下文件较多/大，会影响镜像的build速度 FROM FROM可以在一个Dockerfile出现多次 ARG与FROM交互 ARG CODE_VERSION=latest FROM base:${CODE_VERSION} CMD /code/run-app FROM extras:${CODE_VERSION} CMD /code/run-extras ARG生命周期 在FROM之前声明的ARG位于构建阶段之外，因此不能在FROM之后的任何指令中使用它。若要使用第一个FROM之前声明的ARG的默认值，请使用构建阶段中没有值的ARG指令 ARG VERSION=latest FROM busybox:$VERSION ARG VERSION RUN echo $VERSION > image_version RUN 格式一： RUN (shell form, the command is run in a shell, which by default is /bin/sh -c on Linux or cmd /S /C on Windows) 格式二： RUN [\"executable\", \"param1\", \"param2\"] /bin/sh替换为/bin/bash RUN [\"/bin/bash\", \"-c\", \"echo hello\"] docker-deamon执行与shell执行 shell执行并返回结果 Step 1/2 : FROM alpine ---> 961769676411 Step 2/2 : RUN [ \"sh\", \"-c\", \"cat ~/.bash_profile\" ] ---> Running in f6a08aee1953 cat: can't open '/root/.bash_profile': No such file or directory The command 'sh -c cat ~/.bash_profile' returned a non-zero code: 1 docker-deamon执行并返回结果 Step 1/2 : FROM alpine ---> 961769676411 Step 2/2 : RUN [ \"cat ~/.bash_profile\" ] ---> Running in 59d4ac8f5ff7 OCI runtime create failed: container_linux.go:345: starting container process caused \"exec: \\\"cat ~/.bash_profile\\\": stat cat ~/.bash_profile: no such file or directory\": unknown CMD 格式一： CMD [\"executable\",\"param1\",\"param2\"] (exec form, this is the preferred form) 格式二： CMD [\"param1\",\"param2\"] (as default parameters to ENTRYPOINT) 格式三： CMD command param1 param2 (shell form) CMD只能出现一次，最后的CMD指令会覆盖之前的指令 解析规则 默认解析为shell指令 FROM ubuntu CMD echo \"This is a test.\" | wc - 实际解析为 FROM ubuntu CMD /bin/sh -c echo \"This is a test.\" | wc - 以下格式必须使绝对径，并且命令用 \"\"引用 FROM ubuntu CMD [\"/usr/bin/wc\",\"--help\"] RUN vs CMD RUN 生效于镜像build时 Step 1/2 : FROM alpine ---> 961769676411 Step 2/2 : RUN cat ~/.bash_profile ---> Running in 446a0b3c52ce cat: can't open '/root/.bash_profile': No such file or directory The command '/bin/sh -c cat ~/.bash_profile' returned a non-zero code: 1 CMD生效于镜像启动为容器时 Step 1/2 : FROM alpine ---> 961769676411 Step 2/2 : CMD cat ~/.bash_profile ---> Running in 8b4e2c4c810f Removing intermediate container 8b4e2c4c810f ---> 92382df2a644 Successfully built 92382df2a644 Successfully tagged alpine01:latest docker run -idt alpine01 返回容器ID 4542bb3760f4036c8044be744fbf5c948045bbfe4216aa7ae719e596a41dd859 docker logs -f 4542bb3760f4036c8044be744fbf5c948045bbfe4216aa7ae719e596a41dd859 cat: can't open '/root/.bash_profile': No such file or directory LABEL 格式： LABEL = = = ... 样例： LABEL \"com.example.vendor\"=\"ACME Incorporated\" LABEL com.example.label-with-value=\"foo\" LABEL version=\"1.0\" LABEL description=\"This text illustrates \\ that label-values can span multiple lines.\" 声明为一行： LABEL multi.label1=\"value1\" multi.label2=\"value2\" other=\"value3\" LABEL multi.label1=\"value1\" \\ multi.label2=\"value2\" \\ other=\"value3\" MAINTAINER 最新版已被移除，可以使用Label指令代替 LABEL maintainer=\"SvenDowideit@home.org.au\" EXPOSE 仅作说明用途，不会实际开放端口 ENV 格式一： ENV 格式二： ENV = ... ADD 格式一： ADD [--chown=:] ... 格式二： ADD [--chown=:] [\"\",... \"\"] 支持通配符： ADD hom* /mydir/ # adds all files starting with \"hom\" ADD hom?.txt /mydir/ # ? is replaced with any single character, e.g., \"home.txt\" The is an absolute path, or a path relative to WORKDIR, into which the source will be copied inside the destination container 转义特殊字符： ADD arr[[]0].txt /mydir/ # copy a file named \"arr[0].txt\" to /mydir/ 指定文件所属及权限： ADD --chown=55:mygroup files* /somedir/ ADD --chown=bin files* /somedir/ ADD --chown=1 files* /somedir/ ADD --chown=10:11 files* /somedir/ If the container root filesystem does not contain either /etc/passwd or /etc/group files and either user or group names are used in the --chown flag, the build will fail on the ADD operation. Using numeric IDs requires no lookup and will not depend on container root filesystem content. ADD规则： 必须位于build 上下文路径中，不可以使用相对路径（如ADD ../something /something） 尽量以\"/\"结尾（ADD http://192.168.1.2:80/file1 /root/file2,会解析为将file1下载到/root下并命名为file2） 当为目录时，则复制目录下的全部内容，包括文件系统元数据但不包含该目录 为可识别的压缩类型文件时（identity, gzip, bzip2 or xz，与文件名无关），会自动被解压 If doesn’t exist, it is created along with all missing directories in its path. COPY 格式一： COPY [--chown=:] ... 格式二： COPY [--chown=:] [\"\",... \"\"] COPY VS ADD COPY不会自动解压缩 COPY支持为URL类型 ENTRYPOINT 格式一： ENTRYPOINT [\"executable\", \"param1\", \"param2\"] 格式二： ENTRYPOINT command param1 param2 shell form vs exec form 两者的区别如下： shell form vs exec form shell form本质为交由shell执行（默认/bin/sh） exec form本质为交由docker-daemon执行 USER 指定镜像启动后的容器内程序所属用户,默认root启动 USER [:] or USER [:] WORKDIR 指定构建阶段的目录上下文 VOLUME 映射docker宿主机与容器目录 SHELL 多用于windows平台 The SHELL instruction allows the default shell used for the shell form of commands to be overridden. The default shell on Linux is [\"/bin/sh\", \"-c\"], and on Windows is [\"cmd\", \"/S\", \"/C\"]. The SHELL instruction must be written in JSON form in a Dockerfile. HEALTHCHECK 健康检测端点，判断容器内服务状态： HEALTHCHECK --interval=5m --timeout=3s \\ CMD curl -f http://localhost/ || exit 1 可选配置： // 每30s检测一次 --interval=DURATION (default: 30s) // 超时响应时间（超过30s未响应，代表不健康） --timeout=DURATION (default: 30s) // 容器启动多久后开启检测(取决于容器启动速度) --start-period=DURATION (default: 0s) // 重试次数 --retries=N (default: 3) 退出码： 0: success - the container is healthy and ready for use 1: unhealthy - the container is not working correctly STOPSIGNAL 退出信号，默认SIGTERM（强制退出） STOPSIGNAL 9 可修改该值实现程序的平滑退出，值可以为信号名也可以为数字 ONBUILD 相当于触发器，下次该镜像作为基础镜像时被触发 The ONBUILD instruction adds to the image a trigger instruction to be executed at a later time, when the image is used as the base for another build. ONBUILD [INSTRUCTION] ONBUILD不可触发的指令如下： FROM ONBUILD MAINTAINER docker build image Dockerfile指令 创建目录 mkdir -p /docker/simple 创建dockerfile FROM alpine # 设置变量 ENV NGINX_VERSION 1.16.1 # 修改源 RUN echo \"http://mirrors.aliyun.com/alpine/latest-stable/main/\" > /etc/apk/repositories && \\ echo \"http://mirrors.aliyun.com/alpine/latest-stable/community/\" >> /etc/apk/repositories && \\ # 安装需要的软件 apk update && \\ apk add --no-cache ca-certificates && \\ apk add --no-cache curl bash tree tzdata && \\ cp -rf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime && \\ # 编译安装nginx GPG_KEYS=B0F4253373F8F6F510D42178520A9993A1C052F8 \\ && CONFIG=\"\\ --prefix=/opt/nginx \\ --sbin-path=/usr/sbin/nginx \\ --modules-path=/usr/lib/nginx/modules \\ --conf-path=/opt/nginx/conf/nginx.conf \\ --error-log-path=/var/log/nginx/error.log \\ --http-log-path=/var/log/nginx/access.log \\ --pid-path=/var/run/nginx.pid \\ --lock-path=/var/run/nginx.lock \\ --http-client-body-temp-path=/var/cache/nginx/client_temp \\ --http-proxy-temp-path=/var/cache/nginx/proxy_temp \\ --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp \\ --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp \\ --http-scgi-temp-path=/var/cache/nginx/scgi_temp \\ --user=nginx \\ --group=nginx \\ --with-http_ssl_module \\ --with-http_realip_module \\ --with-http_addition_module \\ --with-http_sub_module \\ --with-http_gunzip_module \\ --with-http_gzip_static_module \\ --with-http_random_index_module \\ --with-http_secure_link_module \\ --with-http_stub_status_module \\ --with-http_auth_request_module \\ --with-http_xslt_module=dynamic \\ --with-http_image_filter_module=dynamic \\ --with-http_geoip_module=dynamic \\ --with-stream \\ --with-stream_ssl_module \\ --with-stream_ssl_preread_module \\ --with-stream_realip_module \\ \" \\ && addgroup -S nginx \\ && adduser -D -S -h /var/cache/nginx -s /sbin/nologin -G nginx nginx \\ && apk add --no-cache --virtual .build-deps \\ gcc \\ libc-dev \\ make \\ openssl-dev \\ pcre-dev \\ zlib-dev \\ linux-headers \\ curl \\ gnupg \\ libxslt-dev \\ gd-dev \\ geoip-dev \\ && curl -fSL http://nginx.org/download/nginx-$NGINX_VERSION.tar.gz -o nginx.tar.gz \\ && mkdir -p /usr/src \\ && tar -zxC /usr/src -f nginx.tar.gz \\ && rm nginx.tar.gz \\ && cd /usr/src/nginx-$NGINX_VERSION \\ && ./configure $CONFIG --with-debug \\ && make -j$(getconf _NPROCESSORS_ONLN) \\ && mv objs/nginx objs/nginx-debug \\ && mv objs/ngx_http_xslt_filter_module.so objs/ngx_http_xslt_filter_module-debug.so \\ && mv objs/ngx_http_image_filter_module.so objs/ngx_http_image_filter_module-debug.so \\ && ./configure $CONFIG \\ && make -j$(getconf _NPROCESSORS_ONLN) \\ && make install \\ && rm -rf /opt/nginx/html/ \\ && mkdir /opt/nginx/conf/conf.d/ \\ && mkdir -p /usr/share/nginx/html/ \\ && install -m644 html/index.html /usr/share/nginx/html/ \\ && install -m644 html/50x.html /usr/share/nginx/html/ \\ && install -m755 objs/nginx-debug /usr/sbin/nginx-debug \\ && install -m755 objs/ngx_http_xslt_filter_module-debug.so /usr/lib/nginx/modules/ngx_http_xslt_filter_module-debug.so \\ && install -m755 objs/ngx_http_image_filter_module-debug.so /usr/lib/nginx/modules/ngx_http_image_filter_module-debug.so \\ && ln -s ../../usr/lib/nginx/modules /opt/nginx/modules \\ && strip /usr/sbin/nginx* \\ && strip /usr/lib/nginx/modules/*.so \\ && rm -rf /usr/src/nginx-$NGINX_VERSION \\ \\ # Bring in gettext so we can get `envsubst`, then throw # the rest away. To do this, we need to install `gettext` # then move `envsubst` out of the way so `gettext` can # be deleted completely, then move `envsubst` back. && apk add --no-cache --virtual .gettext gettext \\ && mv /usr/bin/envsubst /tmp/ \\ \\ && runDeps=\"$( \\ scanelf --needed --nobanner /usr/sbin/nginx /usr/lib/nginx/modules/*.so /tmp/envsubst \\ | awk '{ gsub(/,/, \"\\nso:\", $2); print \"so:\" $2 }' \\ | sort -u \\ | xargs -r apk info --installed \\ | sort -u \\ )\" \\ && apk add --no-cache --virtual .nginx-rundeps $runDeps \\ && apk del .build-deps \\ && apk del .gettext \\ && mv /tmp/envsubst /usr/local/bin/ \\ \\ # forward request and error logs to docker log collector && ln -sf /dev/stdout /var/log/nginx/access.log \\ && ln -sf /dev/stderr /var/log/nginx/error.log # 开放80端口 EXPOSE 80 STOPSIGNAL SIGTERM # 启动nginx命令 CMD [\"nginx\", \"-g\", \"daemon off;\"] 构建镜像 docker build -t nginx:1.16.1 . 11/16/2019 1:55:59 PM 批量导出 docker images |awk '{print $1}' |sed -n '2,$p' |xargs docker save -o k8s.tar docker multi-stage 创建目录 mkdir -p /docker/multi-stage 创建golang程序 cat > /docker/multi-stage/rancher.go 创建dockerfile 参考官方样例 cat > /docker/multi-stage/Dockerfile 构建镜像 docker build -t rancher-demo . 构建信息 镜像大小 运行容器 docker run --rm --name rancher -idt rancher-demo 查看日志 docker logs -f rancher 11/16/2019 1:19:36 PM Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:06 "},"2.容器/docker/install/install.html":{"url":"2.容器/docker/install/install.html","title":"install","keywords":"","body":" 安装 升级内核 在线安装 配置yum源 配置yum代理 清理旧版本docker 安装docker 配置docker 关闭selinux 调整系统参数 配置阿里云加速 启动 安装 升级内核 建议升级内核 ，以便使用新特性 在线安装 配置yum源 yum可用跳过 CentOS 7 rm -f /etc/yum.repos.d/* curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo 配置yum代理 宿主机可直接访问互联网情况跳过 配置代理 vi /etc/yum.conf 文件最后添加以下内容 proxy=http://username:password@host:port username: http代理账号 password: http代理密码 host: http代理主机（ip或域名） port: http代理端口 清理旧版本docker ```shell script yum remove docker -y \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine rm -rf /etc/systemd/system/docker.service.d rm -rf /var/lib/docker rm -rf /var/run/docker rm -rf /usr/local/docker rm -rf /etc/docker ### 安装docker 以下安装方式：二选一 > 安装最新版 ```shell script yum -y install yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum makecache fast yum -y install docker-ce 安装指定版本 查看可选版本 ```shell script [root@localhost ~]# yum list docker-ce --showduplicates|sort -r updates: mirrors.aliyun.com Loading mirror speeds from cached hostfile Loaded plugins: fastestmirror extras: mirrors.aliyun.com docker-ce.x86_64 3:20.10.7-3.el7 docker-ce-stable docker-ce.x86_64 3:20.10.6-3.el7 docker-ce-stable docker-ce.x86_64 3:20.10.5-3.el7 docker-ce-stable docker-ce.x86_64 3:20.10.4-3.el7 docker-ce-stable docker-ce.x86_64 3:20.10.3-3.el7 docker-ce-stable docker-ce.x86_64 3:20.10.2-3.el7 docker-ce-stable docker-ce.x86_64 3:20.10.1-3.el7 docker-ce-stable docker-ce.x86_64 3:20.10.0-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.9-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.8-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.7-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.6-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.5-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.4-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.3-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.2-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.15-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.14-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.1-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.13-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.12-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.11-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.10-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.0-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.9-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.8-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.7-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.6-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.5-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.4-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.3-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.2-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.1-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.0-3.el7 docker-ce-stable docker-ce.x86_64 18.06.3.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.06.2.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.06.1.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.06.0.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.03.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 18.03.0.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.12.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.12.0.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.09.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.09.0.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.06.2.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.06.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.06.0.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.03.3.ce-1.el7 docker-ce-stable docker-ce.x86_64 17.03.2.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.03.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.03.0.ce-1.el7.centos docker-ce-stable 安装指定版本 ```shell script yum install -y docker-ce-19.03.15-3.el7 ## 配置docker ### 关闭selinux ```shell script setenforce 0 sed -i \"s#SELINUX=enforcing#SELINUX=disabled#g\" /etc/selinux/config 调整系统参数 ```shell script sed -i ':a;$!{N;ba};s@# docker limit BEGIN.*# docker limit END@@' /etc/security/limits.conf cat > /etc/security/limits.conf docker limit BEGIN soft nofile 65535 hard nofile 65535 soft nproc 65535 hard nproc 65535docker limit END EOF sed -i \"/user.max_user_namespaces/d\" /etc/sysctl.conf echo \"user.max_user_namespaces=15000\" >> /etc/sysctl.conf sysctl -p ulimit -u 65535 ulimit -n 65535 echo 'net.ipv4.ip_forward = 1' >> /etc/sysctl.conf echo 'net.bridge.bridge-nf-call-arptables = 1' >> /etc/sysctl.conf echo 'net.bridge.bridge-nf-call-ip6tables = 1' >> /etc/sysctl.conf echo 'net.bridge.bridge-nf-call-iptables = 1' >> /etc/sysctl.conf echo 'vm.max_map_count = 262144' >> /etc/sysctl.conf echo 'vm.swappiness = 1' >> /etc/sysctl.conf echo 'fs.inotify.max_user_instances = 524288' >> /etc/sysctl.conf See https://imroc.io/posts/kubernetes/troubleshooting-with-kubernetes-network/ sed -r -i \"s@#{0,}?net.ipv4.tcp_tw_recycle ?= ?(0|1)@net.ipv4.tcp_tw_recycle = 0@g\" /etc/sysctl.conf sed -r -i \"s@#{0,}?net.ipv4.ip_forward ?= ?(0|1)@net.ipv4.ip_forward = 1@g\" /etc/sysctl.conf sed -r -i \"s@#{0,}?net.bridge.bridge-nf-call-arptables ?= ?(0|1)@net.bridge.bridge-nf-call-arptables = 1@g\" /etc/sysctl.conf sed -r -i \"s@#{0,}?net.bridge.bridge-nf-call-ip6tables ?= ?(0|1)@net.bridge.bridge-nf-call-ip6tables = 1@g\" /etc/sysctl.conf sed -r -i \"s@#{0,}?net.bridge.bridge-nf-call-iptables ?= ?(0|1)@net.bridge.bridge-nf-call-iptables = 1@g\" /etc/sysctl.conf sed -r -i \"s@#{0,}?vm.max_map_count ?= ?(0|1)@vm.max_map_count = 262144@g\" /etc/sysctl.conf sed -r -i \"s@#{0,}?vm.swappiness ?= ?(0|1)@vm.swappiness = 1@g\" /etc/sysctl.conf sed -r -i \"s@#{0,}?fs.inotify.max_user_instances ?= ?(0|1)@fs.inotify.max_user_instances = 524288@g\" /etc/sysctl.conf awk ' !x[$0]++{print > \"/etc/sysctl.conf\"}' /etc/sysctl.conf > 配置`docker daemon` ```shell script mkdir -p /etc/docker cat /etc/docker/daemon.json { \"log-opts\": { \"max-size\": \"10m\", \"max-file\":\"3\" }, \"userland-proxy\": false, \"live-restore\": true, \"default-ulimits\": { \"nofile\": { \"Hard\": 65535, \"Name\": \"nofile\", \"Soft\": 65535 } }, \"default-address-pools\": [ { \"base\": \"172.80.0.0/16\", \"size\": 24 }, { \"base\": \"172.90.0.0/16\", \"size\": 24 } ], \"no-new-privileges\": false, \"default-gateway\": \"\", \"default-gateway-v6\": \"\", \"default-runtime\": \"runc\", \"default-shm-size\": \"64M\", \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF 配置阿里云加速 可选 调整/etc/docker/daemon.json 添加如下内容 ```shell script \"registry-mirrors\": [\"https://jz73200c.mirror.aliyuncs.com\"] 重启 ```shell sudo systemctl daemon-reload sudo systemctl restart docker 启动 shell script systemctl daemon-reload systemctl enable docker --now Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/docker/network/network.html":{"url":"2.容器/docker/network/network.html","title":"network","keywords":"","body":" Table of Contents generated with DocToc complete network communication between two containers via veth-paris & network namespaces. complete network communication between two containers via veth-paris & network namespaces. 1、安装pipework yum install git bridge-utils -y git clone https://github.com/jpetazzo/pipework cd pipework cp pipework /usr/bin/ 2、配置pipework 修改物理网卡配置 修改前： TYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no BOOTPROTO=static DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6INIT=yes IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_FAILURE_FATAL=no IPV6_ADDR_GEN_MODE=stable-privacy NAME=ens33 UUID=5b8e5f27-6e21-4a48-97eb-05cca1056a98 DEVICE=ens33 IPADDR=192.168.40.136 NETMASK=255.255.255.0 GATEWAY=192.168.40.2 PREFIX=24 ONBOOT=yes ZONE= 修改后： TYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no BOOTPROTO=none DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6INIT=yes IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_FAILURE_FATAL=no IPV6_ADDR_GEN_MODE=stable-privacy NAME=ens33 UUID=5b8e5f27-6e21-4a48-97eb-05cca1056a98 DEVICE=ens33 #IPADDR=192.168.40.136 #NETMASK=255.255.255.0 #GATEWAY=192.168.40.2 #PREFIX=24 ONBOOT=yes ZONE= BRIDGE=br0 创建网桥，宿主机IP为136（原物理网卡ens33 IP地址） cat >> /etc/sysconfig/network-scripts/ifcfg-br0 重启网络 systemctl restart network 查看宿主机网络 ip a 创建容器A，并配置独立IP 180 docker run -itd --net=none --name testA busybox /bin/sh pipework br0 testA 192.168.40.180/24@192.168.40.2 创建容器B，并配置独立IP 190 docker run -itd --net=none --name testB busybox /bin/sh pipework br0 testB 192.168.40.190/24@192.168.40.2 查看容器testA IP地址 docker exec -it testA ip a 查看容器testB IP地址 docker exec -it testB ip a 宿主机ping testA testB 网络 容器A、容器B测试网络连通性 docker exec -it testA ping 192.168.40.190 -c 3 docker exec -it testB ping 192.168.40.180 -c 3 11/18/2019 7:56:37 PM Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/docker/security/security.html":{"url":"2.容器/docker/security/security.html","title":"security","keywords":"","body":" 容器安全 概述 1 主机安全配置 1.1 为docker挂载单独存储目录 1.2 容器宿主机加固 1.3 更新Docker到最新版本 1.4 只有受信任的用户才能控制Docker守护进程 2.docker守护进程配置 2.1 不适用不安全的镜像仓库 2.2 不使用aufs存储驱动程序 2.3 Docker守护进程配置TLS身份认证 2.4 配置合适的ulimit 2.5 启用用户命名空间 2.6 使用默认cgroup 2.7 设置容器的默认空间大小 2.8 启用docker客户端命令的授权 2.9 配置集中和远程日志记录 2.10 禁用旧仓库版本（v1）上的操作 2.11 启用实时恢复 2.12 禁用userland代理 2.13 应用守护进程范围的自定义seccomp配置文件 2.14 生产环境中避免实验性功能 2.15 限制容器获取新的权限 3.docker守护程序文件配置 3.1 设置docker文件的所有权为root:root 3.2 设置docker.service文件权限为644或更多限制性 3.3 设置docker.socket文件的所有权为root:root 3.4 设置docker.socket文件权限为644或更多限制性 3.5 设置/etc/docker目录所有权为root:root 3.6 设置/etc/docker目录权限为755或更多限制性 3.7 设置仓库证书文件所有权为root:root 3.8 设置仓库证书文件权限为444或更多限制性 3.9 设置TLS CA证书文件所有权为root:root 3.10 设置TLS CA证书文件权限为444或更多限制性 3.11 设置docker服务器证书文件所有权为root:root 3.12 设置Docker服务器证书文件权限为400或更多限制 3.13 设置docker.sock文件所有权为root:docker 3.14 设置docker.sock文件权限为660或更多限制性 3.15 设置docker.json文件所有权为root:root 3.16 设置docker.json文件权限为644或更多限制性 4.容器镜像和构建文件 4.1 创建容器的用户 4.2 容器使用可信的基础镜像 4.3 容器中不安装没有必要的软件包 4.4 扫描镜像漏洞并且构建包含安全补丁的镜像 4.5 启用docker内容信任 4.6 将HEALTHCHECK说明添加到容器镜像 4.7 在dockerfile中使用copy而不是add 4.8 涉密信息不存储在dockerfile 4.9 仅安装已经验证的软件包 4.10 正确设置容器上的CPU优先级 4.11 Linux内核功能在容器内受限 4.12 不使用特权容器 4.13 敏感的主机系统目录未挂载在容器上 4.14 SSH不在容器中运行 4.15 特权端口禁止映射到容器内 4.16 只映射必要的端口 4.17 确保容器的内存使用合理 4.18 设置容器的根文件系统为只读 4.19 确保进入容器的流量绑定到特定的主机接口 4.20 容器重启策略on-failure设置为5 4.21 确保主机的进程命名空间不共享 4.22 主机的IPC命令空间不共享 4.23 主机设备不直接共享给容器 4.24 设置装载传播模式不共享 4.25 设置主机的UTS命令空间不共享 4.26 docker exec命令不能使用特权选项 4.27 docker exec命令不能与user选项一起使用 4.28 检查容器运行时状态 4.29 限制使用PID cgroup 4.30 不要使用Docker的默认网桥docker0 4.31 任何容器内不能安装Docker套接字 5.Docker安全操作 5.1 避免镜像泛滥 5.2 避免容器泛滥 最佳实践 安装 配置 文件权限调整 参考文档 容器安全 基于Docker 19.03.8 概述 云原生时代下，容器广受欢迎，因为它能简化应用或服务及其所有依赖项的构建、封装与推进，而且这种简化涵盖整个生命周期，并且跨越不同的环境和部署目标。 然而，容器安全依然面临着一些挑战。本文从几个角度切入，剖析容器加固常用方式方法。 在审查Docker安全性时，有四个主要方面需要考虑: 内核的内在安全性及其对名称空间和cgroup的支持 Docker守护进程本身的攻击面 容器配置文件中的漏洞，要么是默认的，要么是用户自定义的 内核的强化安全特性以及它们如何与容器交互 1 主机安全配置 1.1 为docker挂载单独存储目录 描述 默认安装情况下，所有Docker容器及数据、元数据存储于/var/lib/docker下 审计方式 Docker依赖于/var/lib/docker作为默认数据目录，该目录存储所有相关文件，包括镜像文件。 该目录可能会被恶意的写满，导致Docker、甚至主机可能无法使用。因此，建议为Docker存储目录配置独立挂载点（最好为独立数据盘） 修复建议 docker宿主机增加数据盘/dev/sdb ```shell script [root@localhost ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 20G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 19G 0 part ├─centos-root 253:0 0 17G 0 lvm / └─centos-swap 253:1 0 2G 0 lvm [SWAP] sdb 8:16 0 30G 0 disk sr0 11:0 1 4.4G 0 rom > 格式化数据盘 ```shell script [root@localhost ~]# mkfs.ext4 /dev/sdb mke2fs 1.42.9 (28-Dec-2013) /dev/sdb is entire device, not just one partition! Proceed anyway? (y,n) y Filesystem label= OS type: Linux Block size=4096 (log=2) Fragment size=4096 (log=2) Stride=0 blocks, Stripe width=0 blocks 1966080 inodes, 7864320 blocks 393216 blocks (5.00%) reserved for the super user First data block=0 Maximum filesystem blocks=2155872256 240 block groups 32768 blocks per group, 32768 fragments per group 8192 inodes per group Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000 Allocating group tables: done Writing inode tables: done Creating journal (32768 blocks): done Writing superblocks and filesystem accounting information: done 配置/dev/sdb挂载点为/var/lib/docker 该步骤建议安装docker之后进行 ```shell script echo \"/dev/sdb /var/lib/docker ext4 defaults 0 0\" >> /etc/fstab > 重启主机测试是否生效 ```shell script [root@localhost ~]# reboot [root@localhost ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 20G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 19G 0 part ├─centos-root 253:0 0 17G 0 lvm / └─centos-swap 253:1 0 2G 0 lvm [SWAP] sdb 8:16 0 30G 0 disk /var/lib/docker sr0 11:0 1 4.4G 0 rom [root@localhost ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE harbor.wl.com/public/alpine latest d6e46aa2470d 6 months ago 5.57MB 1.2 容器宿主机加固 分析 容器在Linux主机上运行，容器宿主机可以运行一个或多个容器。 加强主机以缓解主机安全配置错误是非常重要的 审计方式 确保遵守主机的安全规范。询问系统管理员当前主机系统符合哪个安全标准。确保主机系统实际符合主机制定的安全规范 修复建议 参考Linux主机安全加固规范。 1.3 更新Docker到最新版本 描述 Docker软件频繁发布更新，旧版本可能存在安全漏洞 审计 查看release 与本地版本比较 ```shell script docker version - 隐患分析 不要盲目升级`docker`版本，评估升级是否会对现有系统产生影响，充分测试其兼容性（如与`k8s kubeadm`兼容性） - 修复建议 ```shell script #安装一些必要的系统工具 yum -y install yum-utils device-mapper-persistent-data lvm2 #添加软件源信息 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo #更新 yum 缓存 yum makecache fast #安装docker-ce yum -y install docker-ce # 或更新 yum -y update docker-ce 1.4 只有受信任的用户才能控制Docker守护进程 描述 Docker守护进程需要root权限。对于添加到Docker组的用户， 为其提供了完整的root访问权限。 隐患分析 Docker允许在宿主机和访客容器之间共享目录，而不会限制容器的访问权限。 这意味着可以启动容器并将主机上的/目录映射到容器。 容器将能够不受任何限制地更改您的主机文件系统。 简而言之，这意味着您只需作为Docker组的成员即可获得较高的权限，然后在主机上启动具有映射/目录的容器。 审计方式 ```shell script [root@localhost ~]# yum install glibc-common -y -q [root@localhost ~]# getent group docker docker:x:994: - 结果判定 查看`审计`步骤中的返回值是否含有非信任用户 - 修复建议 从`docker`组中删除任何不受信任的用户。另外，请勿在主机上创建敏感目录到容器卷的映射 ## 2.`docker`守护进程配置 ### 2.1 不适用不安全的镜像仓库 - 描述 `Docker`在默认情况下，私有仓库被认为是安全的 - 隐患分析 镜像仓库建议使用`TLS`。 在`/etc/docker/certs.d//`目录下，将镜像仓库的`CA`证书副本放置在`Docker`主机上。 不安全的镜像仓库是没有有效的镜像仓库证书或不使用`TLS`的镜像仓库。不应该在生产环境中使用任何不安全的镜像仓库。 不安全的镜像仓库中的镜像可能会被篡改，从而导致生产系统可能受到损害。 此外，如果镜像仓库被标记为不安全，则`docker pull`，`docker push`和`docker push`命令并不能发现， 那样用户可能无限期地使用不安全的镜像仓库而不会发现。 - 审计方式 ```shell script [root@localhost ~]# cat /etc/docker/daemon.json |grep insecure-registries \"insecure-registries\":[\"gcr.azk8s.cn\",\"dockerhub.azk8s.cn\",\"quay.azk8s.cn\",\"5twf62k1.mirror.aliyuncs.com\",\"registry.docker-cn.com\",\"registry-1.docker.io\"], 修复建议 使用ssl签名的镜像仓库（如配置ssl证书的harbor） 2.2 不使用aufs存储驱动程序 描述 aufs存储驱动程序是较旧的存储驱动程序。 它基于Linux内核补丁集，不太可能合并到主版本Linux内核中。 aufs驱动会导致一些严重的内核崩溃。aufs在Docker中只是保留了历史遗留支持,现在主要使用overlay2和devicemapper。 而且最重要的是，在许多使用最新Linux内核的发行版中，aufs不再被支持 审计方式 ```shell script [root@node105 ~]# docker info |grep \"Storage Driver:\" Storage Driver: overlay2 - 修复建议 默认安装情况下存储驱动为`overlay2`，避免使用`aufs`作为存储驱动 ### 2.3 `Docker`守护进程配置`TLS`身份认证 - 描述 可以让`Docker`守护进程监听特定的`IP`和端口以及除默认`Unix`套接字以外的任何其他`Unix`套接字。 配置`TLS`身份验证以限制通过`IP`和端口访问`Docker`守护进程。 - 隐患分析 默认情况下，`Docker`守护程序绑定到非联网的`Unix`套接字，并以`root`权限运行。 如果将默认的`Docker`守护程序更改为绑定到`TCP`端口或任何其他`Unix`套接字，那么任何有权访问该端口或套接字的人都可以完全访问`Docker`守护程序，进而可以访问主机系统。 因此，不应该将`Docker`守护程序绑定到另一个`IP`/端口或`Unix`套接字。 如果必须通过网络套接字暴露`Docker`守护程序，请为守护程序配置`TLS`身份验证 - 审计方法 ```shell script [root@localhost ~]# systemctl status docker|grep /usr/bin/dockerd ├─1061 /usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock 修复建议 生产环境下避免开启tcp监听，若避免不了，执行以下操作。 生成CA私钥和公共密钥 ```shell script mkdir -p /root/docker cd /root/docker openssl genrsa -aes256 -out ca-key.pem 4096 openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem > 创建一个服务端密钥和证书签名请求(`CSR`) `192.168.235.128`为当前主机`IP`地址 ```shell script openssl genrsa -out server-key.pem 4096 openssl req -subj \"/CN=192.168.235.128\" -sha256 -new -key server-key.pem -out server.csr 用CA来签署公共密钥 ```shell script echo subjectAltName = DNS:192.168.235.128,IP:192.168.235.128 >> extfile.cnf echo extendedKeyUsage = serverAuth >> extfile.cnf > 生成`key` ```shell script openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem \\ -CAcreateserial -out server-cert.pem -extfile extfile.cnf 创建客户端密钥和证书签名请求 ```shell script openssl genrsa -out key.pem 4096 openssl req -subj '/CN=client' -new -key key.pem -out client.csr > 修改`extfile.cnf` ```shell script echo extendedKeyUsage = clientAuth > extfile-client.cnf 生成签名私钥 ```shell script openssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem \\ -CAcreateserial -out cert.pem -extfile extfile-client.cnf > 将`Docker`服务停止，然后修改`docker`服务文件 停服务 ```shell script systemctl stop docker 编辑配置文件 ```shell script vi /etc/systemd/system/docker.service 替换`ExecStart=/usr/bin/dockerd`为以下 ```shell script ExecStart=/usr/bin/dockerd --tlsverify --tlscacert=/root/docker/ca.pem --tlscert=/root/docker/server-cert.pem --tlskey=/root/docker/server-key.pem -H unix:///var/run/docker.sock -H tcp://192.168.235.128:2375 重启 ```shell script systemctl daemon-reload systemctl start docker > 测试`tls` ```shell script docker --tlsverify --tlscacert=/root/docker/ca.pem --tlscert=/root/docker/cert.pem --tlskey=/root/docker/key.pem -H=192.168.235.128:2375 version 2.4 配置合适的ulimit 描述 什么是ulimit ulimit主要是用来限制进程对资源的使用情况的，它支持各种类型的限制，常用的有： 内核文件的大小限制 进程数据块的大小限制 Shell进程创建文件大小限制 可加锁内存大小限制 常驻内存集的大小限制 打开文件句柄数限制 分配堆栈的最大大小限制 CPU占用时间限制用户最大可用的进程数限制 Shell进程所能使用的最大虚拟内存限制 隐患分析 ulimit提供对shell可用资源的控制。设置系统资源控制可以防止资源耗尽带来的问题，如fork炸弹。 有时候合法的用户和进程也可能过度使用系统资源，导致系统资源耗尽。 为Docker守护程序设置默认ulimit将强制执行所有容器的ulimit。 不需要单独为每个容器设置ulimit。 但默认的ulimit可能在容器运行时被覆盖。 因此，要控制系统资源，需要自定义默认的ulimit 审计 确保含有--default-ulimit参数 ```shell script [root@localhost ~]# ps -ef|grep dockerd root 65353 1 0 03:02 ? 00:00:00 /usr/bin/dockerd --tlsverify --tlscacert=/root/docker/ca.pem --tlscert=/root/docker/server-cert.pem --tlskey=/root/docker/server-key.pem -H unix:///var/run/docker.sock -H tcp://192.168.235.128:2375 - 修复建议 > 调整参数`LimitNOFILE`、`LimitNPROC` ```shell script sed -i \"s#LimitNOFILE=infinity#LimitNOFILE=20480:40960#g\" /etc/systemd/system/docker.service sed -i \"s#LimitNPROC=infinity#LimitNPROC=1024:2048#g\" /etc/systemd/system/docker.service 重启 ```shell script systemctl daemon-reload systemctl restart docker > 启动一个容器测试 ```shell script [root@localhost ~]# docker run -idt --name ddd harbor.wl.com/public/alpine sh 15eebdabbb8bd59366348ae95a89d79100370b9c9381b070fdfbb0119b516400 查看容器PID ```shell script [root@localhost ~]# ps -ef|grep 15eebdabbb8bd59366348ae95a89d79100370b9c9381b070fdfbb0119b516400|grep -v grep|awk '{print $2}' 80060 > 查看`limit` ```shell script [root@localhost ~]# cat /proc/80060/limits Limit Soft Limit Hard Limit Units Max cpu time unlimited unlimited seconds Max file size unlimited unlimited bytes Max data size unlimited unlimited bytes Max stack size 8388608 unlimited bytes Max core file size unlimited unlimited bytes Max resident set unlimited unlimited bytes Max processes 1024 2048 processes Max open files 20480 40960 files Max locked memory 65536 65536 bytes Max address space unlimited unlimited bytes Max file locks unlimited unlimited locks Max pending signals 3795 3795 signals Max msgqueue size 819200 819200 bytes Max nice priority 0 0 Max realtime priority 0 0 Max realtime timeout unlimited unlimited us 2.5 启用用户命名空间 描述 在Docker守护程序中启用用户命名空间支持，可对用户进行重新映射。该建议对镜像中没有指定用户是有帮助的。如果在容器镜像中已经 定义了非root运行，可跳过此建议。 隐患分析 Docker守护程序中对Linux内核用户命名空间支持为Docker主机系统提供了额外的安全性。 它允许容器具有独特的用户和组ID，这些用户和组ID在主机系统所使用的传统用户和组范围之外。 例如，root用户希望有容器内的管理权限，可映射到主机系统上的非root的UID上 审计 如果容器进程以root身份运行，则不符合安全要求 ```shell script [root@localhost ~]# ps -ef|grep 15eebdabbb8b root 80060 73608 0 04:03 ? 00:00:00 containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/15eebdabbb8bd59366348ae95a89d79100370b9c9381b070fdfbb0119b516400 -address /var/run/docker/containerd/containerd.sock -containerd-binary /usr/bin/containerd -runtime-root /var/run/docker/runtime-runc -systemd-cgroup root 111259 1482 0 07:08 pts/0 00:00:00 grep --color=auto 15eebdabbb8b - 修复建议 > 修改系统参数 ```shell script sed -i \"/user.max_user_namespaces/d\" /etc/sysctl.conf echo \"user.max_user_namespaces=15000\" >> /etc/sysctl.conf sysctl -p 编辑配置文件 ```shell script vi /etc/systemd/system/docker.service `ExecStart=/usr/bin/dockerd`添加参数`--userns-remap=default` > 重载服务 ```shell script systemctl daemon-reload systemctl restart docker 启动一个容器 ```shell script [root@localhost ~]# docker run -idt --name ccc alpine > 查看容器内进程用户 [root@localhost ~]# ps -p $(docker inspect --format='{{.State.Pid}}' $(docker ps |grep ccc|awk '{print $1}')) -o pid,user PID USER 2535 100000 ### 2.6 使用默认`cgroup` - 描述 查看`--cgroup-parent`选项允许设置用于所有容器的默认`cgroup parent`。 如果没有特定用例,则该设置应保留默认值。 - 隐患分析 系统管理员可定义容器应运行的`cgroup`。 若系统管理员没有明确定义`cgroup`，容器也会在`docker cgroup`下运行。 应该监测和确认使用情况。通过加到与默认不同的`cgroup`，导致不合理地共享资源，从而可能会主机资源耗尽 - 审计方式 ```shell script ps -ef|grep dockerd 确保--cgroup-parent参数未设置或设置为适当的非默认cgroup 修复建议 如无特殊需求，默认值即可 2.7 设置容器的默认空间大小 描述 在某些情况下，可能需要大于10G（容器默认存储大小）的容器空间。需要仔细选择空间的大小 隐患分析 守护进程重启时可以增加容器空间的大小。用户可以通过设置默认容器空间值来进行扩大，但不允许缩小。 设立该值的时候需要谨慎，防止设置不当带来空间耗尽的情况 审计方式 ```shell script ps -ef|grep dockerd 执行上述命令，它不应显示任何`--storage-opt dm.basesize`参数 - 修复建议 如无特殊需求，默认值即可 ### 2.8 启用`docker`客户端命令的授权 - 描述 使用本机`Docker`授权插件或第三方授权机制与`Docker`守护程序来管理对`Docker`客户端命令的访问。 - 隐患分析 `Docker`默认是没有对客户端命令进行授权管理的功能。 任何有权访问`Docker`守护程序的用户都可以运行任何`Docker`客户端命令。 对于使用`Docker`远程`API`来调用守护进程的调用者也是如此。 如果需要细粒度的访问控制，可以使用授权插件并将其添加到`Docker`守护程序配置中。 使用授权插件，`Docker`管理员可以配置更细粒度访问策略来管理对`Docker`守护进程的访问。 `Docker`的第三方集成可以实现他们自己的授权模型，以要求`Docker`的本地授权插件 （即`Kubernetes`，`Cloud Foundry`，`Openshift`）之外的`Docker`守护进程的授权。 - 审计方式 ```shell script ps -ef|grep dockerd 或 cat /etc/docker/daemon.json|grep userland-proxy 如果使用Docker本地授权，可使用--authorization-plugin参数加载授权插件。 修复建议 如无特殊需求，默认值即可 2.9 配置集中和远程日志记录 描述 Docker现在支持各种日志驱动程序。存储日志的最佳方式是支持集中式和远程日志记录 审计方式 运行docker info并确保日志记录驱动程序属性被设置为适当的。 ```shell script [root@localhost ~]# docker info --format '{{.LoggingDriver}}' json-file - 修复建议 > 配置`json-file`驱动 ```shell script [root@localhost ~]# cat /etc/docker/daemon.json { \"log-driver\":\"json-file\", \"log-opts\":{ \"max-size\":\"50m\", \"max-file\":\"3\" } } 重启 ```shell script systemctl daemon-reload systemctl restart docker ### 2.10 禁用旧仓库版本（v1）上的操作 - 描述 最新的`Docker`镜像仓库是`v2`。遗留镜像仓库版本`v1`上的所有操作都应受到限制 - 隐患分析 `Docker`镜像仓库`v2`在`v1`中引入了许多性能和安全性改进。 它支持容器镜像来源验证和其他安全功能。因此，对`Docker v1`仓库的操作应该受到限制 - 审计方式 ```shell script ps -ef|grep dockerd 上面的命令应该列出--disable-legacy-registry作为传递给Docker守护进程的选项。 修复建议 注意：17.12+版本已移除，无需配置 编辑配置文件 ```shell script vi /etc/systemd/system/docker.service `ExecStart=/usr/bin/dockerd`添加参数`--userns-remap=default` > 重载服务 ```shell script systemctl daemon-reload systemctl restart docker 2.11 启用实时恢复 描述 live-restore参数可以支持无守护程序的容器运行。 它确保Docker在关闭或恢复时不会停止容器，并在重新启动后重新连接到容器。 隐患分析 可用性作为安全一个重要的属性。 在Docker守护进程中设置--live-restore标志可确保当Docker守护进程不可用时容器执行不会中断。 这也意味着当更新和修复Docker守护进程而不会导致容器停止工作。 审计方式 ```shell script [root@localhost ~]# docker info --format '{{.LiveRestoreEnabled}}' false - 修复建议 > 编辑文件 ```shell script mkdir -p /etc/docker/ vi /etc/docker/daemon.json 添加如下内容 \"live-restore\": true 重载服务 ```shell script systemctl daemon-reload systemctl restart docker ### 2.12 禁用`userland`代理 - 描述 当容器端口需要被映射时，`Docker`守护进程都会启动用于端口转发的`userland-proxy`方式。如果使用了`DNAT`方式，该功能可以被禁用 - 隐患分析 `Docker`引擎提供了两种机制将主机端口转发到容器,`DNAT`和`userland-proxy`。 在大多数情况下，`DNAT`模式是首选，因为它提高了性能，并使用本地`Linux iptables`功能而需要附加组件。 如果`DNAT`可用，则应在启动时禁用`userland-proxy`以减少安全风险。 - 审计方法 ```shell script ps -ef|grep dockerd 或 cat /etc/docker/daemon.json|grep userland-proxy 确保userland-proxy配置为false 修复建议 编辑文件 ```shell script mkdir -p /etc/docker/ vi /etc/docker/daemon.json 添加如下内容 \"userland-proxy\": false, > 重载服务 ```shell script systemctl daemon-reload systemctl restart docker 2.13 应用守护进程范围的自定义seccomp配置文件 描述 如果需要，您可以选择在守护进程级别自定义seccomp配置文件，并覆盖Docker的默认seccomp配置文件 隐患分析 大量系统调用暴露于每个用户级进程，其中许多系统调用在整个生命周期中都未被使用。 大多数应用程序不需要所有的系统调用，因此可以通过减少可用的系统调用来增加安全性。 可自定义seccomp配置文件，而不是使用Docker的默认seccomp配置文件。 如果Docker的默认配置文件够用的话，则可以选择忽略此建议 审计 ```shell script [root@localhost ~]# docker info --format '{{.SecurityOptions}}' - 修复建议 错误配置的`seccomp`配置文件可能会中断的容器运行。`Docker`默认的策略兼容性很好，可以解决一些基本的安全问题。 所以，在[重写默认值](https://docs.docker.com/engine/security/seccomp/) 时，你应该非常小心 ### 2.14 生产环境中避免实验性功能 - 描述 避免生产环境中的实验性功`-Experimental` - 隐患分析 `Docker`实验功能现在是一个运行时`Docker`守护进程标志, 其作为运行时标志传递给`Docker`守护进程，激活实验性功能。 实验性功能现在虽然比较稳定，但是一些功能可能没有大规模经使用，并不能保证`API`的稳定性，所以不建议在生产环境中使用 - 审计方法 ```shell script [root@localhost ~]# docker version --format '{{.Server.Experimental}}' false 修复建议 不要将--Experimental作为运行时参数传递给Docker守护进程 2.15 限制容器获取新的权限 描述 默认情况下，限制容器通过suid或sgid位获取附加权限 隐患分析 一个进程可以在内核中设置no_new_priv。 它支持fork，clone和execve。 no_new_priv确保进程或其子进程不会通过suid或sgid位获得任何其他特权。 这样，很多危险的操作就降低安全风险。在守护程序级别进行设置可确保默认情况下，所有新容器不能获取新的权限。 审计方法 ```shell script ps -ef|grep dockerd 或 cat /etc/docker/daemon.json|grep no-new-privileges 确保`no-new-privileges`配置为`false` - 修复建议 > 编辑文件 ```shell script mkdir -p /etc/docker/ vi /etc/docker/daemon.json 添加如下内容 \"no-new-privileges\": false 重载服务 ```shell script systemctl daemon-reload systemctl restart docker ## 3.`docker`守护程序文件配置 ### 3.1 设置`docker`文件的所有权为`root:root` - 描述 - 隐患分析 `docker.service`文件包含可能会改变`Docker`守护进程行为的敏感参数。 因此，它应该由`root`拥有和归属，以保持文件的完整性。 - 审计方式 ```shell script systemctl show -p FragmentPath docker.service|sed \"s/FragmentPath=//\"|xargs -n1 ls -l 返回值应为 -rw-r--r-- 1 root root 1157 Apr 26 08:04 /etc/systemd/system/docker.service 修复建议 若所属用户非root:root，修改授权 ```shell script systemctl show -p FragmentPath docker.service|sed \"s/FragmentPath=//\"|xargs -n1 chown root:root ### 3.2 设置`docker.service`文件权限为644或更多限制性 - 描述 验证`docker.service`文件权限是否正确设置为`644`或更多限制 - 隐患分析 `docker.service`文件包含可能会改变`Docker`守护进程行为的敏感参数。 因此，它应该由`root`拥有和归属，以保持文件的完整性。 - 审计方式 ```shell script [root@localhost ~]# systemctl show -p FragmentPath docker.service|sed \"s/FragmentPath=//\"|xargs -n1 stat -c %a 644 修复建议 若权限非644，修改授权 ```shell script systemctl show -p FragmentPath docker.service|sed \"s/FragmentPath=//\"|xargs -n1 chmod 644 ### 3.3 设置`docker.socket`文件的所有权为`root:root` - 描述 验证`docker.socket`文件所有权和组所有权是否正确设置为`root` - 隐患分析 `docker.socket`文件包含可能会改变`Docker`远程`API`行为的敏感参数。 因此，它应该拥有`root`权限，以保持文件的完整性。 - 审计方式 ```shell script systemctl show -p FragmentPath docker.socket|sed \"s/FragmentPath=//\"|xargs -n1 ls -l 返回值应为 -rw-r--r-- 1 root root 197 Mar 10 2020 /usr/lib/systemd/system/docker.socket 修复建议 若所属用户非root:root，修改授权 ```shell script systemctl show -p FragmentPath docker.socket|sed \"s/FragmentPath=//\"|xargs -n1 chown root:root ### 3.4 设置`docker.socket`文件权限为`644`或更多限制性 - 描述 验证`docker.socket`文件权限是否正确设置为`644`或更多限制 - 隐患分析 `docker.socket`文件包含可能会改变`Docker`远程`API`行为的敏感参数。 因此，它应该拥有`root`权限，以保持文件的完整性。 - 审计方式 ```shell script [root@localhost ~]# systemctl show -p FragmentPath docker.socket|sed \"s/FragmentPath=//\"|xargs -n1 stat -c %a 644 修复建议 若权限非644，修改授权 ```shell script systemctl show -p FragmentPath docker.socket|sed \"s/FragmentPath=//\"|xargs -n1 chmod 644 ### 3.5 设置`/etc/docker`目录所有权为`root:root` - 描述 验证`/etc/docker`目录所有权和组所有权是否正确设置为`root:root` - 隐患分析 除了各种敏感文件之外，`/etc/docker`目录还包含证书和密钥。 因此，它应该由`root:root`拥有和归组来维护目录的完整性。 - 审计方式 ```shell script [root@localhost ~]# stat -c %U:%G /etc/docker root:root 修复建议 若所属用户非root:root，修改授权 ```shell script chown root:root /etc/docker ### 3.6 设置`/etc/docker`目录权限为`755`或更多限制性 - 描述 验证`/etc/docker`目录权限是否正确设置为`755` - 隐患分析 除了各种敏感文件之外，`/etc/docker`目录还包含证书和密钥。 因此，它应该由`root:root`拥有和归组来维护目录的完整性。 - 审计方式 ```shell script [root@localhost ~]# stat -c %a /etc/docker 755 修复建议 若所属用户非root:root，修改授权 ```shell script chmod 755 /etc/docker ### 3.7 设置仓库证书文件所有权为`root:root` - 描述 验证所有仓库证书文件（通常位于`/etc/docker/certs.d/` 目录下）均由`root`拥有并归组所有 - 隐患分析 `/etc/docker/certs.d/`目录包含`Docker`镜像仓库证书。 这些证书文件必须由`root`和其组拥有，以维护证书的完整性 - 审计方式 ```shell script [root@localhost ~]# stat -c %U:%G /etc/docker/certs.d/* root:root 修复建议 若所属用户非root:root，修改授权 ```shell script chown root:root /etc/docker/certs.d/* ### 3.8 设置仓库证书文件权限为`444`或更多限制性 - 描述 验证所有仓库证书文件（通常位于`/etc/docker/certs.d/` 目录下）所有权限是否正确设置为`444` - 隐患分析 `/etc/docker/certs.d/`目录包含`Docker`镜像仓库证书。 这些证书文件必须具有`444`权限，以维护证书的完整性。 - 审计方式 ```shell script [root@localhost ~]# stat -c %a /etc/docker/certs.d/* 755 修复建议 若权限非444，修改授权 ```shell script chmod 444 /etc/docker/certs.d/* ### 3.9 设置`TLS CA`证书文件所有权为`root:root` - 描述 验证`TLS CA`证书文件均由`root`拥有并归组所有 - 隐患分析 `TLS CA`证书文件应受到保护，不受任何篡改。它用于指定的`CA`证书验证。 因此，它必须由`root`拥有，以维护`CA`证书的完整性。 - 审计方式 ```shell script [root@localhost ~]# ls /etc/docker/certs.d/*/* |xargs -n1 stat -c %U:%G root:root root:root root:root 修复建议 若所属用户非root:root，修改授权 ```shell script chown root:root /etc/docker/certs.d// ### 3.10 设置`TLS CA`证书文件权限为`444`或更多限制性 - 描述 验证所有仓库证书文件（通常位于`/etc/docker/certs.d/` 目录下）所有权限是否正确设置为`444` - 隐患分析 `TLS CA`证书文件应受到保护，不受任何篡改。它用于指定的`CA`证书验证。 这些证书文件必须具有`444`权限，以维护证书的完整性。 - 审计方式 ```shell script [root@localhost ~]# stat -c %a /etc/docker/certs.d/*/* 644 644 644 修复建议 若权限非444，修改授权 ```shell script chmod 444 /etc/docker/certs.d// ### 3.11 设置`docker`服务器证书文件所有权为`root:root` - 描述 验证`Docker`服务器证书文件（与`--tlscert`参数一起传递的文件）是否由`root`和其组拥有 - 隐患分析 `Docker`服务器证书文件应受到保护，不受任何篡改。它用于验证`Docker`服务器。 因此，它必须由`root`拥有以维护证书的完整性。 - 审计方式 **注意:** `/root/docker`替换为docker服务端实际证书存放目录 ```shell script [root@localhost ~]# ls -l /root/docker total 44 -rw-r--r-- 1 root root 3326 Apr 26 02:55 ca-key.pem -rw-r--r-- 1 root root 1980 Apr 26 02:56 ca.pem -rw-r--r-- 1 root root 17 Apr 26 02:57 ca.srl -rw-r--r-- 1 root root 1801 Apr 26 02:57 cert.pem -rw-r--r-- 1 root root 1582 Apr 26 02:57 client.csr -rw-r--r-- 1 root root 30 Apr 26 02:57 extfile-client.cnf -rw-r--r-- 1 root root 86 Apr 26 02:56 extfile.cnf -rw-r--r-- 1 root root 3243 Apr 26 02:57 key.pem -rw-r--r-- 1 root root 1862 Apr 26 02:56 server-cert.pem -rw-r--r-- 1 root root 1594 Apr 26 02:56 server.csr -rw-r--r-- 1 root root 3243 Apr 26 02:56 server-key.pem 修复建议 若所属用户非root:root，修改授权 ```shell script chown root:root /root/docker/* ### 3.12 设置`Docker`服务器证书文件权限为`400`或更多限制 - 描述 验证`Docker`服务器证书文件（与`--tlscert`参数一起传递的文件）权限是否为`400` - 隐患分析 `Docker`服务器证书文件应受到保护，不受任何篡改。它用于验证`Docker`服务器。 因此，它必须由`root`拥有以维护证书的完整性。 - 审计方式 **注意:** `/root/docker`替换为docker服务端实际证书存放目录 ```shell script [root@localhost ~]# ls -l /root/docker total 44 -rw-r--r-- 1 root root 3326 Apr 26 02:55 ca-key.pem -rw-r--r-- 1 root root 1980 Apr 26 02:56 ca.pem -rw-r--r-- 1 root root 17 Apr 26 02:57 ca.srl -rw-r--r-- 1 root root 1801 Apr 26 02:57 cert.pem -rw-r--r-- 1 root root 1582 Apr 26 02:57 client.csr -rw-r--r-- 1 root root 30 Apr 26 02:57 extfile-client.cnf -rw-r--r-- 1 root root 86 Apr 26 02:56 extfile.cnf -rw-r--r-- 1 root root 3243 Apr 26 02:57 key.pem -rw-r--r-- 1 root root 1862 Apr 26 02:56 server-cert.pem -rw-r--r-- 1 root root 1594 Apr 26 02:56 server.csr -rw-r--r-- 1 root root 3243 Apr 26 02:56 server-key.pem 修复建议 若权限非400，修改授权 ```shell script chmod 400 /root/docker/* ### 3.13 设置`docker.sock`文件所有权为`root:docker` - 描述 验证`docker.sock`文件由`root`拥有，而用户组为`docker`。 - 隐患分析 `Docker`守护进程以`root`用户身份运行。 因此，默认的`Unix`套接字必须由`root`拥有。 如果任何其他用户或进程拥有此套接字，那么该非特权用户或进程可能与`Docker`守护进程交互。 另外，这样的非特权用户或进程可能与容器交互，这样非常不安全。 另外，`Docker`安装程序会创建一个名为`docker`的用户组。 可以将用户添加到该组，然后这些用户将能够读写默认的`Docker Unix`套接字。 `docker`组成员由系统管理员严格控制。 如果任何其他组拥有此套接字，那么该组的成员可能会与`Docker`守护进程交互。。 因此，默认的`Docker Unix`套接字文件必须由`docker`组拥有权限，以维护套接字文件的完整性 - 审计 ```shell script [root@localhost ~]# stat -c %U:%G /var/run/docker.sock root:docker 修复建议 若所属用户非root:docker，修改授权 ```shell script chown root:docker /var/run/docker.sock ### 3.14 设置`docker.sock`文件权限为`660`或更多限制性 - 描述 验证`docker`套接字文件是否具有`660`或更多限制的权限 - 隐患分析 只有`root`和`docker`组的成员允许读取和写入默认的`Docker Unix`套接字。 因此，`Docker`套接字文件必须具有`660`或更多限制的权限 - 审计 ```shell script [root@localhost ~]# stat -c %a /var/run/docker.sock 660 修复建议 若权限非660，修改授权 ```shell script chmod 660 /var/run/docker.sock ### 3.15 设置`docker.json`文件所有权为`root:root` - 描述 验证`docker.json`文件由`root`归属。 - 隐患分析 `docker.json`文件包含可能会改变`Docker`守护程序行为的敏感参数。 因此，它应该由`root`拥有，以维护文件的完整性 - 审计 ```shell script [root@localhost ~]# stat -c %U:%G /etc/docker/daemon.json root:root 修复建议 若所属用户非root:root，修改授权 ```shell script chown root:root /etc/docker/daemon.json ### 3.16 设置`docker.json`文件权限为`644`或更多限制性 - 描述 验证`docker.json`文件权限是否正确设置为`644`或更多限制 - 隐患分析 `docker.json`文件包含可能会改变`Docker`守护程序行为的敏感参数。 因此，它应该由`root`拥有，以维护文件的完整性 - 审计方式 ```shell script [root@localhost ~]# stat -c %a /etc/docker/daemon.json 644 修复建议 若权限非644，修改授权 ```shell script chmod 644 /etc/docker/daemon.json ## 4.容器镜像和构建文件 ### 4.1 创建容器的用户 - 描述 为容器镜像的`Dockerfile`中的容器创建非`root`用户 - 隐患分析 如果可能，指定非`root`用户身份运行容器是个很好的做法。 虽然用户命名空间映射可用，但是如果用户在容器镜像中指定了用户，则默认情况下容器将作为该用户运行，并且不需要特定的用户命名空间重新映射。 - 审计方式 ```shell script [root@localhost ~]# docker ps |grep ccc|awk '{print $1}'|xargs -n1 docker inspect --format='{{.Id}}:User={{.Config.User}}' 4e53c86daf89a1bac0ed178d043663d2af162ca813ff17864ebdb964d8233459:User= 上述命令应该返回容器用户名或用户ID。 如果为空，则表示容器以root身份运行 修复建议 确保容器镜像的Dockerfile包含以下指令：USER 其中用户名或ID是指可以在容器基础镜像中找到的用户。 如果在容器基础镜像中没有创建特定用户，则在USER指令之前添加useradd命令以添加特定用户。 例如，在Dockerfile中创建用户： RUN useradd -d /home/username -m -s /bin/bash username USER username 注意: 如果镜像中有容器不需要的用户，请考虑删除它们。 删除这些用户后，提交镜像，然后生成新的容器实例以供使用。 4.2 容器使用可信的基础镜像 描述 确保容器镜像是从头开始编写的，或者是基于通过安全仓库下载的另一个已建立且可信的基本镜像 隐患分析 官方存储库是由Docker社区或供应商优化的Docker镜像。 可能还存在其他不安全的公共存储库。 在从Docker和第三方获取容器镜像时，需谨慎使用。 审计方式 1.检查Docker主机以查看执行以下命令使用的Docker镜像： ```shell script docker images 这将列出当前可用于`Docker`主机的所有容器镜像。 访谈系统管理员并获取证据，证明镜像列表是通过安全的镜像仓库获到的，也可简单的从镜像的`TAG`名称来判断是否为可信镜像。 > 2.检查镜像信息 对于在`Docker`主机上找到的每个`Docker`镜像，检查镜像的构建方式，以验证是否来自可信来源： ```shell script docker history 修复建议 中间件等应用使用官方镜像 构建镜像时选用alpine、CentOS等官方镜像 从源头杜绝不安全镜像 4.3 容器中不安装没有必要的软件包 描述 容器往往是操作系统的最简的版本，不要安装任何不需要的软件。 隐患分析 安装不必要的软件可能会增加容器的攻击风险。因此，除了容器的真正需要的软件之外，不要安装其他多余的软件。 审计方式 1.通过执行以下命令列出所有运行的容器实例： ```shell script docker ps > 对于每个容器实例，执行以下或等效的命令 ```shell script docker exec rpm -qa rpm -qa命令可根据容器镜像系统类型进行相应变更 修复建议 中间件等应用使用官方镜像 构建镜像时选用alpine、CentOS等官方精简后的镜像 从源头杜绝安装没有必要的软件包 4.4 扫描镜像漏洞并且构建包含安全补丁的镜像 描述 应该经常扫描镜像以查找漏洞。重建镜像安装最新的补丁。 隐患分析 安全补丁可以解决软件的安全问题。可以使用镜像漏洞扫描工具来查找镜像中的任何类型的漏洞，然后检查可用的补丁以减轻这些漏洞。 修补程序将系统更新到最新的代码库。此外，如果镜像漏洞扫描工具可以执行二进制级别分析，而不仅仅是版本字符串匹配，则会更好 审计方式 1.通过执行以下命令列出所有运行的容器实例 ```shell script docker ps --quiet > 2.对于每个容器实例，执行下面的或等效的命令来查找容器中安装的包的列表,确保安装各种受影响软件包的安全更新。 ```shell script docker exec rpm -qa 修复建议 定期更新基础镜像版本tag（或使用latest版本镜像，每日执行构建）及镜像内必须软件版本 4.5 启用docker内容信任 描述 默认情况下禁用内容信任，为了安全起见，可以启用 隐患分析 内容信任为向远程Docker镜像仓库发开和接收的数据提供了使用数字签名的能力。 这些签名允许客户端验证特定镜像标签的完整性和发布者。这确保了容器镜像的来源的合法性。 审计方式 4.6 将HEALTHCHECK说明添加到容器镜像 描述 在Docker容器镜像中添加HEALTHCHECK指令以对正在运行的容器执行运行状况检查。 安全出发点 安全性最重要的一个特性就是可用性。将HEALTHCHECK指令添加到容器镜像可确保Docker引擎定期检查运行的容器实例是否符合该指令， 以确保实例仍在运行。根据报告的健康状况，Docker引擎可以退出非工作容器并实例化新容器。 审计 运行以下命令，并确保Docker镜像对HEALTHCHECK指令设置 ```shell script [root@localhost ~]# docker inspect --format='{{.Config.Healthcheck}}' 8a2fb25a19f5 应当返回设置值而非`nil` - 修复建议 按照`Docker`文档，并使用`HEALTHCHECK`指令重建容器镜像。 ### 4.7 在`dockerfile`中使用`copy`而不是`add` - 描述 在`Dockerfile`中使用`COPY`指令而不是`ADD`指令 - 隐患分析 `COPY`指令只是将文件从本地主机复制到容器文件系统。 `ADD`指令可能会从远程`URL`下载文件并执行诸如解包等操作。 因此，`ADD`指令增加了从`URL`添加恶意文件的风险 - 审计 步骤 1：运行以下命令获取镜像列表 ```shell script docker images 步骤 2：对上述列表中的每个镜像执行以下命令，并查找任何ADD指令： for i in `docker images --quiet`;do docker history $i |grep ADD > /dev/null if [ $? -eq 0 ];then echo \"imageID: $i has 'ADD' direct...\" fi done 修复建议 在Dockerfile中使用COPY指令 4.8 涉密信息不存储在dockerfile 描述 不要在Dockerfile中存储任何涉密信息 隐患分析 通过使用Docker历史命令，可以查看各种工具和实用程序。 通常情况，镜像发布者提供Dockerfile来构建镜像。所以，Dockerfile中的涉密信息可能会被暴露并被恶意利用。 审计方式 第 1 步：运行以下命令以获取镜像列表： ```shell script docker images 第 2 步：对上面列表中的每个镜像运行以下命令，并查找是否有涉密信息： ```shell script docker history 如果有权访问镜像的Dockerfile，请确认没有涉密信息（不应该有涉密的信息，如用户账号，私钥证书等。） 修复建议 不要在Dockerfile中存储任何类型的涉密信息 4.9 仅安装已经验证的软件包 描述 在将软件包安装到镜像中之前，验证软件包可靠性 隐患分析 验证软件包的可靠性对于构建安全的容器镜像至关重要。不合法的软件包可能具有恶意或者存在一些可能被利用的已知漏洞 审计方式 第 1 步：运行以下命令以获取镜像列表： ```shell script docker images 第 2 步：对上面列表中的每个镜像运行以下命令，并查看软件包的合法性 ```shell script docker history 若可以访问镜像的Dockerfile，请验证是否检查了软件包的合法性 修复建议 使用GPG密钥下载和验证您所选择的软件包或任何其他安全软件包分发机制 4.10 正确设置容器上的CPU优先级 描述 默认情况下，Docker主机上的所有容器均可共享资源。通过使用Docker主机的资源管理功能（如CPU共享），可以控制容器可能占用的主机CPU资源 隐患分析 默认情况下CPU时间在容器间平均分配。 如果需要，为了控制容器实例之间的CPU时间，可以使用CPU共享功能。 CPU共享允许将一个容器优先于另一个容器，并禁止较低优先级的容器更频繁占用CPU资源。可确保高优先级的容器更好地运行 审计方式 ```shell script [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{{.Id}}:CpuShares={{.HostConfig.CpuShares}}' 83243cce85b85f9091b4c3bd7ff981762ff91c50e42ca36f2a5f47502ff00377:CpuShares=0 748901568eafe1d3c21bb8e544278ed36af019281d485eb74be39b41ca549605:CpuShares=0 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:CpuShares=0 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:CpuShares=0 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:CpuShares=0 cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:CpuShares=0 如果上述命令返回`0`或`1024`，则表示`CPU`无限制。 如果上述命令返回非`1024`值以外的非零值，则表示`CPU`已经限制。 - 修复建议 管理容器之间的`CPU`份额。为此，请使用`--cpu-shares`参数启动容器 ### 4.11 `Linux`内核功能在容器内受限 - 描述 默认情况下，`Docker`使用一组受限制的`Linux`内核功能启动容器。 这意味着可以将任何进程授予所需的功能，而不是`root`访问。 使用`Linux`内核功能，这些进程不必以`root`用户身份运行。 - 隐患分析 `Docker`支持添加和删除功能，允许使用非默认配置文件。 这可能会使`Docker`通过移除功能更加安全，或者通过增加功能来减少安全性。 因此，建议除去容器进程明确要求的所有功能。 例如，容器进程通常不需要如下所示的功能：`NET_ADMIN`、`SYS_ADMIN`、 `SYS_MODULE` - 审计方式 ```shell script [root@localhost ~]# docker ps --quiet |xargs docker inspect --format '{{.Id}}:CapAdd={{.HostConfig.CapAdd}} CapDrop={{.HostConfig.CapDrop}}' 7121e891641679fda571e67a0e9953d263feca2508b013c70ae2546f6336b1a0:CapAdd= CapDrop= bb3875c107daa062f2eccb10bd48ad54954cecd7d51a5eba385335f377b7aae9:CapAdd= CapDrop= 7a3a2c9e524a9d44ae857abd52447f86940dd49e1947291e7985b98e3c6a309a:CapAdd= CapDrop= 0780c27f8eb858e172e6a7458d2b2221130e6dde0f64887d396ad5bc350a4a64:CapAdd= CapDrop= 验证添加和删除的Linux内核功能是否符合每个容器实例的容器进程所需的功能 修复建议 只添加必须功能特性 ```shell script docker run -dit --cap-drop=all --cap-add={\"NET_ADMIN\", \"SYS_ADMIN\"} centos /bin/bash 默认情况下，以下功能可用于容器: `AUDIT_WRITE`、`CHOWN`、`DAC_OVERRIDE`、`FOWNER`、`FSETID`、`KILL`、`MKNOD`、`NET_BIND_SERVICE`、`NET_RAW`、 `SETFCAP`、`SETGID`、`SETPCAP`、`SETUID`、`SYS_CHROOT` > Linux kernel capabilities机制介绍 默认情况下，`Docker`启动具有一组受限功能的容器。 `capabilities`机制将二进制`root/no-root`二分法转换为细粒度的访问控制系统。 只需要绑定`1024`以下端口的进程(比如`web`服务器)不需要以`root`身份运行:它们只需要被授予`net_bind_service`能力即可。 对于通常需要根特权的几乎所有特定领域，还有许多其他功能。 典型的服务器以`root`身份运行多个进程，包括`SSH`守护进程、`cron`守护进程、日志守护进程、内核模块、网络配置工具等。 容器是不同的，因为几乎所有这些任务都由容器周围的基础设施处理: - `SSH`访问通常由运行在`Docker`宿主机管理 - 必要时，`cron`应该作为一个用户进程运行，专门为需要调度服务的应用程序定制，而不是作为一个平台范围的工具 - 日志管理通常也交给`Docker`，或者像`Loggly`或`Splunk`这样的第三方服务 - 硬件管理是不相关的，这意味着您永远不需要在容器中运行`udevd`或等效的守护进程 - 网络管理也都在宿主机上设置，除非特殊需求。这意味着容器不需要执行`ifconfig`、`route`或`ip`命令（当然，除非容器被专门设计成路由器或防火墙） 这意味这大部分情况下，容器完全不需要真正的`root`权限。因此，容器可以运行一个减少的`capabilities`集，容器中的`root`也比真正的`root`拥有更少的`capabilities`,比如： - 完全禁止任何`mount`操作 - 禁止访问络`socket` - 禁止访问一些文件系统的操作，比如创建新的设备`node`等等 - 禁止模块加载 这意味这就算攻击者在容器中取得了`root`权限，也很难造成严重破坏 这不会影响到普通的`web`应用程序，但会大大减少恶意用户的攻击。默认情况下，`Docker`会删除所有需要的功能，使用`allowlist`而不是`denylist`方法 运行`Docker`容器的一个主要风险是，给容器的默认功能集和挂载可能会提供不完全的隔离 `Docker`支持添加和删除`capabilities`功能，允许使用非默认配置文件。这可能会使`Docker`通过删除功能而变得更安全，或者通过增加功能而变得更不安全。对于用户来说，最佳实践是删除除其流程显式需要的功能之外的所有功能 **简言之：`Linux Kernel capabilities`提供更细粒度的`root`权限控制** ### 4.12 不使用特权容器 - 描述 使用`--privileged`标志将所有`Linux`内核功能提供给容器，从而覆盖`-cap-add`和`-cap-drop`标志。若无必须请不要使用 - 隐患分析 `--privileged`标志给容器提供所有功能,并且还提升了`cgroup`控制器执行的所有限制。 换句话说，容器可以做几乎主机可以做的一切。这个标志存在允许特殊用例,就像在`Docker`中运行`Docker`一样 - 审计方式 ```shell script [root@localhost ~]# docker ps --quiet |xargs docker inspect --format '{{.Id}}:Privileged={{.HostConfig.Privileged}}' 7121e891641679fda571e67a0e9953d263feca2508b013c70ae2546f6336b1a0:Privileged=false bb3875c107daa062f2eccb10bd48ad54954cecd7d51a5eba385335f377b7aae9:Privileged=false 7a3a2c9e524a9d44ae857abd52447f86940dd49e1947291e7985b98e3c6a309a:Privileged=false 0780c27f8eb858e172e6a7458d2b2221130e6dde0f64887d396ad5bc350a4a64:Privileged=false 确保Privileged为false 修复措施 不要运行带有--privileged标志的容器。例如，不要启动如下容器： ```shell script docker run -idt --privileged centos /bin/bash ### 4.13 敏感的主机系统目录未挂载在容器上 - 描述 不应允许将敏感的主机系统目录（如下所示）作为容器卷进行挂载，特别是在读写模式下。 ```shell script boot dev etc lib lib64 proc run sbin sys usr var 隐患分析 如果敏感目录以读写方式挂载，则可以对这些敏感目录中的文件进行更改。 这些更改可能会降低安全性，且直接影响Docker宿主机 审计方式 ```shell script [root@localhost ~]# docker ps --quiet |xargs docker inspect --format '{{.Id}}:Volumes={{.Mounts}}' 7121e891641679fda571e67a0e9953d263feca2508b013c70ae2546f6336b1a0:Volumes=[map[Destination:/config Driver:local Mode: Name:800e943d52c78312b2d6dd53bed41999fd5f7780af5098f688a894fb74f4360f Propagation: RW:true Source:/var/lib/docker/volumes/800e943d52c78312b2d6dd53bed41999fd5f7780af5098f688a894fb74f4360f/_data Type:volume]] bb3875c107daa062f2eccb10bd48ad54954cecd7d51a5eba385335f377b7aae9:Volumes=[map[Destination:/var/lib/postgresql/data Driver:local Mode: Name:774546bf5c3dcfe5f90a60012c5f1f2bdeb57a5908cdc1922b3dc75550ceeaa4 Propagation: RW:true Source:/var/lib/docker/volumes/774546bf5c3dcfe5f90a60012c5f1f2bdeb57a5908cdc1922b3dc75550ceeaa4/_data Type:volume]] 7a3a2c9e524a9d44ae857abd52447f86940dd49e1947291e7985b98e3c6a309a:Volumes=[map[Destination:/usr/src/redmine/config/configuration.yml Mode: Propagation:rprivate RW:true Source:/cephfs/redmine/config/configuration.yml Type:bind] map[Destination:/usr/src/redmine/files Mode: Propagation:rprivate RW:true Source:/cephfs/redmine/files Type:bind] map[Destination:/usr/src/redmine/app/models/attachment.rb Mode: Propagation:rprivate RW:true Source:/cephfs/redmine/config/attachment.rb Type:bind] map[Destination:/usr/src/redmine/config.ru Mode: Propagation:rprivate RW:true Source:/cephfs/redmine/config/config.ru Type:bind]] 0780c27f8eb858e172e6a7458d2b2221130e6dde0f64887d396ad5bc350a4a64:Volumes=[map[Destination:/var/lib/mysql Mode: Propagation:rprivate RW:true Source:/cephfs/redmine/mysql Type:bind]] - 修复建议 不要将主机敏感目录挂载在容器上，尤其是在读写模式下 ### 4.14 `SSH`不在容器中运行 - 描述 `SSH`服务不应该在容器内运行 - 隐患分析 在容器内运行`SSH`可以增加安全管理的复杂性 难以管理`SSH`服务器的访问策略和安全合规性 难以管理各种容器的密钥和密码 难以管理`SSH`服务器的安全升级 可以在不使用`SSH`情况下对容器进行`shell`访问，避免不必要地增加安全管理的复杂性。 - 审计方式 ```shell script for i in `docker ps --quiet`;do docker exec $i ps -el|grep sshd >/dev/null if [ $? -eq 0 ]; then echo \"container : $i run sshd...\" fi done 返回值如下，说明下面几个容器内部运行ssh服务 ```shell script container : 0781479bef1b run sshd... container : fea9d4d5708a run sshd... container : 38bb65479056 run sshd... container : 212fec812c01 run sshd... - 修复建议 卸载容器内部`ssh`服务或重新构建不含有`ssh`的镜像，运行容器 ### 4.15 特权端口禁止映射到容器内 - 描述 低于`1024`的`TCP/IP`端口号被认为是特权端口，由于各种安全原因，普通用户和进程不允许使用它们。 - 隐患分析 默认情况下，如果用户没有明确声明容器端口进行主机端口映射，`Docker`会自动地将容器端口映射到主机上的`49153-65535`中。 但是，如果用户明确声明它，`Docker`可以将容器端口映射到主机上的特权端口。 这是因为容器使用不限制特权端口映射的`NET_BIND_SERVICE Linux`内核功能来执行。 特权端口接收和发送各种敏感和特权的数据。允许`Docker`使用它们可能会带来严重的影响 - 审计方式 通过执行以下命令列出容器的所有运行实例及其端口映射 ```shell script [root@localhost ~]# docker ps --quiet |xargs docker inspect --format '{{.Id}}:Ports={{.NetworkSettings.Ports}}' 7121e891641679fda571e67a0e9953d263feca2508b013c70ae2546f6336b1a0:Ports=map[6060/tcp:[map[HostIp:0.0.0.0 HostPort:6060]] 6061/tcp:] bb3875c107daa062f2eccb10bd48ad54954cecd7d51a5eba385335f377b7aae9:Ports=map[5432/tcp:[map[HostIp:0.0.0.0 HostPort:5432]]] 7a3a2c9e524a9d44ae857abd52447f86940dd49e1947291e7985b98e3c6a309a:Ports=map[3000/tcp:[map[HostIp:0.0.0.0 HostPort:4000]]] 0780c27f8eb858e172e6a7458d2b2221130e6dde0f64887d396ad5bc350a4a64:Ports=map[3306/tcp:[map[HostIp:0.0.0.0 HostPort:3316]]] 查看列表，并确保容器端口未映射到低于1024的主机端口号 修复建议 启动容器时，不要将容器端口映射到特权主机端口。另外，确保没有容器在Docker文件中特权端口映射声明 4.16 只映射必要的端口 描述 容器镜像的Dockerfile定义了在容器实例上默认要打开的端口。端口列表可能与在容器内运行的应用程序相关 隐患分析 一个容器可以运行在Dockerfile文件中为其镜像定义的端口，也可以任意传递运行时参数以打开一个端口列表。 此外，Dockerfile文件可能会进行各种更改，暴露的端口列表可能与在容器内运行的应用程序不相关。 推荐做法是不要打开不需要的端口 审计方式 ```shell script [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{{.Id}}:Ports={{.NetworkSettings.Ports}}' 83243cce85b85f9091b4c3bd7ff981762ff91c50e42ca36f2a5f47502ff00377:Ports=map[80/tcp:[map[HostIp:192.168.235.128 HostPort:18080]]] 748901568eafe1d3c21bb8e544278ed36af019281d485eb74be39b41ca549605:Ports=map[80/tcp:[map[HostIp:0.0.0.0 HostPort:8080]]] 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:Ports=map[] 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:Ports=map[] 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:Ports=map[] cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:Ports=map[] 查看列表，并确保映射的端口是容器真正需要的端口 ### 4.17 确保容器的内存使用合理 - 描述 默认情况下，`Docker`主机上的所有容器均等共享资源。 通过使用`Docker`主机的资源管理功能，例如内存限制，您可以控制容器可能消耗的内存量 - 隐患分析 默认情况下，容器可以使用主机上的所有内存。 您可以使用内存限制机制来防止由于一个容器消耗了所有主机资源而导致拒绝服务，以致同一主机上的其他容器无法执行预期功能 - 审计方式 ```shell script [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{{.Id}}:Memory={{.HostConfig.Memory}}' 83243cce85b85f9091b4c3bd7ff981762ff91c50e42ca36f2a5f47502ff00377:Memory=0 748901568eafe1d3c21bb8e544278ed36af019281d485eb74be39b41ca549605:Memory=0 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:Memory=0 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:Memory=0 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:Memory=0 cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:Memory=0 如果上述命令返回0，则表示内存无限制。如果上述命令返回非零值，则表示已有内存限制策略 修复建议 建议使用--momery参数运行容器，例如可以如下运行一个容器 ```shell script docker run -idt --memory 256m centos ### 4.18 设置容器的根文件系统为只读 - 描述 通过使用`Docker`运行的只读选项，容器的根文件系统应被视为`只读镜像`。 这样可以防止在容器运行时写入容器的根文件系统 - 隐患分析 启用此选项会迫使运行时的容器明确定义其数据写入策略，可减少安全风险， 因为容器实例的文件系统不能被篡改或写入，除非它对文件系统文件夹和目录具有明确的读写权限。 - 审计方式 ```shell script [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{{.Id}}:ReadonlyRootfs={{.HostConfig.ReadonlyRootfs}}' 83243cce85b85f9091b4c3bd7ff981762ff91c50e42ca36f2a5f47502ff00377:ReadonlyRootfs=false 748901568eafe1d3c21bb8e544278ed36af019281d485eb74be39b41ca549605:ReadonlyRootfs=false 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:ReadonlyRootfs=false 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:ReadonlyRootfs=false 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:ReadonlyRootfs=false cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:ReadonlyRootfs=false 如果上述命令返回true，则表示容器的根文件系统是只读的。 如果上述命令返回false，则意味着容器的根文件系统是可写的 修复建议 在容器的运行时添加一个只读标志以强制容器的根文件系统以只读方式装入 ```shell script docker run -read-only 在容器的运行时启用只读选项，包括但不限于如下： > 1.使用`--tmpfs` 选项为非持久数据写入临时文件系统 ```shell script docker run -idt --read-only --tmpfs \"/run\" --tmpfs \"/tmp\" centos bash 2.启用Docker rw在容器的运行时载入，以便将容器数据直接保存在Docker主机文件系统上 ```shell script docker run -idt --read-only -v /opt/app/data:/run/app/data:rw centos > 3.在容器运行期间，将容器数据传输到容器外部，以便保持容器数据。包括托管数据库，网络文件共享和 API。 ### 4.19 确保进入容器的流量绑定到特定的主机接口 - 描述 默认情况下，`Docker`容器可以连接到外部，但外部无法连接到容器。 每个传出连接都源自主机自己的`IP`地址。所以只允许通过主机上的特定外部接口访问容器服务 - 隐患分析 如果主机上有多个网络接口，则容器可以接受任何网络接一上公开端口的连接，这可能不安全。 很多时候，特定的端口暴露在外部，并且在这些端口上运行诸如入侵检测，入侵防护，防火墙，负载均衡等服务以筛选传入的公共流量。 因此，只允许来自特定外部接口的传入连接 - 审计方式 通过执行以下命令列出容器的所有运行实例及其端口映射 ```shell script [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{{.Id}}:Ports={{.NetworkSettings.Ports}}' 748901568eafe1d3c21bb8e544278ed36af019281d485eb74be39b41ca549605:Ports=map[80/tcp:[map[HostIp:0.0.0.0 HostPort:8080]]] 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:Ports=map[] 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:Ports=map[] 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:Ports=map[] cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:Ports=map[] 查看列表并确保公开的容器端口与特定接口绑定，而不是通配符IP地址- 0.0.0.0 例如，如果上述命令返回是不安全的，并且容器可以接受指定端口8080上的任何主机接口上的连接 修复建议 将容器端口绑定到所需主机端口上的特定主机接口。 ```shell script [root@localhost ~]# docker run -idt --name=nginx2 -p 192.168.235.128:18080:80 --network=nginx-net nginx:1.14-alpine 83243cce85b85f9091b4c3bd7ff981762ff91c50e42ca36f2a5f47502ff00377 [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{{.Id}}:Ports={{.NetworkSettings.Ports}}' 83243cce85b85f9091b4c3bd7ff981762ff91c50e42ca36f2a5f47502ff00377:Ports=map[80/tcp:[map[HostIp:192.168.235.128 HostPort:18080]]] 748901568eafe1d3c21bb8e544278ed36af019281d485eb74be39b41ca549605:Ports=map[80/tcp:[map[HostIp:0.0.0.0 HostPort:8080]]] 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:Ports=map[] 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:Ports=map[] 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:Ports=map[] cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:Ports=map[] ### 4.20 容器重启策略`on-failure`设置为`5` - 描述 在`docker run`命令中使用`--restart`标志，可以指定重启策略，以便在廿出时确定是否重启容器。 基于安全考虑，应该设置重启尝试次数限制为5次 - 隐患分析 如果无限期地尝试启动容器，可能会导致主机上的拒绝服务。 这可能是一种简单的方法来执行分布式拒绝服务攻击，特别是在同一主机上有多个容器时。 此外，忽略容器的廿出状态并始终尝试重新启动容器导致未调查容器终止的根本原因。 如果一个容器被终止，应该做的是去调查它重启的原因，而不是试图无限期地重启它。 因此，建议使用故障重启策略并将其限制为最多 5 次重启尝试 - 审计方式 ```shell script [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{{.Id}}:RestartPolicyName={{.HostConfig.RestartPolicy.Name}} MaximumRetryCount={{.HostConfig.RestartPolicy.MaximumRetryCount}}' 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:RestartPolicyName=no MaximumRetryCount=0 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:RestartPolicyName=no MaximumRetryCount=0 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:RestartPolicyName=no MaximumRetryCount=0 d35fd7bd5e90e6aebc237368453361f632f775490da3c1d28011b9f7e43ff75c:RestartPolicyName=no MaximumRetryCount=0 cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:RestartPolicyName=no MaximumRetryCount=0 修复建议 如果一个容器需要自己重新启动，可以如下设置： ```shell script docker run -idt --restart=on-failure:5 nginx ### 4.21 确保主机的进程命名空间不共享 - 描述 进程`ID（PID）`命名空间隔离进程`ID`空间，这意味着不同`PID`命名空间中的进程可以具有相同的`PID`。这就是容器和主机之间的进程级隔离 - 隐患分析 `PID`名称空间提供了进程的隔离。`PID`命名空间删除了系统进程的视图，并允许重用包括`PID`的进程`ID`。 如果主机的`PID`名称空间与容器共享，它基本上允许容器内的进程查看主机上的所有进程。 这就打破了主机和容器之间进程级别隔离的优点。若访问容器最终可以知道主机系统上运行的所有进程，甚至可以从容器内杀死主机系统进程。 这可能是灾难性的。因此，不要将容器与主机的进程名称空间共享 - 审计方式 ```shell script [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{{.Id}}:PidMode={{.HostConfig.PidMode}}' 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:PidMode= 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:PidMode= 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:PidMode= d35fd7bd5e90e6aebc237368453361f632f775490da3c1d28011b9f7e43ff75c:PidMode= cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:PidMode= 如果上述命令返回host，则表示主机PID名称空间与容器共享，存在安全风险 修复建议 不要使用--pid=host参数启动容器。例如，不要启动一个容器，如下所示 ````shell script docker run -idt --pid=host centos ### 4.22 主机的`IPC`命令空间不共享 - 描述 `IPC（POSIX / Sys IPC）`命名空间提供命名共享内存段，信号量和消息队列的分离。因此主机上的`IPC`命名空间不应该与容器共享，并且应该保持独立。 - 隐患分析 `IPC`命名空间提供主机和容器之间的`IPC`分离。 如果主机的`IPC`名称空间与容器共享，它允许容器内的进程查看主机系统上的所有`IPC`。 这打破了主机和容器之间`IPC`级别隔离的好处。可通过访问容器操纵主机`IPC`。 这可能是灾难性的。 因此，不要将主机的`IPC`名称空间与容器共享 - 审计方式 ```shell script [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{{.Id}}:IpcMode={{.HostConfig.IpcMode}}' 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:IpcMode=private 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:IpcMode=private 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:IpcMode=private d35fd7bd5e90e6aebc237368453361f632f775490da3c1d28011b9f7e43ff75c:IpcMode=private cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:IpcMode=private 如果上述命令返回host，则意味着主机IPC命名空间与容器共享。 修复建议 不要使用--ipc=host参数启动容器。 例如，不要启动如下容器 ```shell script docker run -idt --ipc=host centos - 说明 共享内存段用于加速进程间通信。 它通常被高性能应用程序使用。 如果这些应用程序被容器化为多个容器，则可能需要共享容器的`IPC`名称空间以实现高性能。 在这种情况下，您仍然应该共享容器特定的`IPC`命名空间而不是整个主机`IPC`命名空间。 可以将容器的`IPC`名称空间与另一个容器共享，如下所示： ```shell script docker run -idt --ipc=container:e43299eew043243284 centos 4.23 主机设备不直接共享给容器 描述 主机设备可以在运行时直接共享给容器。 不要将主机设备直接共享给容器，特别是对不受信任的容器 隐患分析 选项--device 将主机设备共享给容器，因此容器可以直接访问这些主机设备。 不允许容器以特权模式运行以访问和操作主机设备默认情况下，容器将能够读取，写入和mknod这些设备。 此外，容器可能会从主机中删除设备。 因此，不要直接将主机设备共享给容器。如果必须的将主机设备共享给容器，请适当地使用共享权限： ```shell script w -> write r -> read m -> mknod - 审计方式 ```shell script [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{{.Id}}:Devices={{.HostConfig.Devices}}' 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:Devices=[] 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:Devices=[] 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:Devices=[] d35fd7bd5e90e6aebc237368453361f632f775490da3c1d28011b9f7e43ff75c:Devices=[] cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:Devices=[] 验证是否需要从容器中访问主机设备，并且正确设置所需的权限。如果上述命令返回[]，则容器无权访问主机设备 修复建议 不要将主机设备直接共享于容器。如果必须将主机设备共享给容器，请使用正确的一组权限，以下为错误示范 ```shell script docker run --interactive --tty --device=/dev/tty0:/dev/tty0:rwm centos bash ### 4.24 设置装载传播模式不共享 - 描述 装载传播模式允许在容器上以`shared`、`private`和`slave`模式挂载数据卷。只有必要的时候才使用共享模式 - 隐患分析 共享模式下挂载卷不会限制任何其他容器的安装并对该卷进行更改。 如果使用的数据卷对变化比较敏感，则这可能是灾难性的。最好不要将安装传播模式设置为共享 - 审计方式 ```shell script [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{.Id}:Propagation={{range $mnt:=.Mounts}}{{json $mnt.Propagation}}{{end}}' {.Id}:Propagation= {.Id}:Propagation= {.Id}:Propagation= {.Id}:Propagation= {.Id}:Propagation= 上述命令将返回已安装卷的传播模式。除非需要，不应将传播模式设置为共享。 修复建议 不建议以共享模式传播中安装卷。例如，不要启动容器，如下所示 ```shell script docker run --volume=/hostPath:/containerPath:shared ### 4.25 设置主机的`UTS`命令空间不共享 - 描述 `UTS`命名空间提供两个系统标识符的隔离：主机名和`NIS`域名。 它用于设置在该名称空间中运行进程可见的主机名和域名。 在容器中运行的进程通常不需要知道主机名和域名。因此，名称空间不应与主机共享 - 隐患分析 与主机共享`UTS`命名空间提供了容器可更改主机的主机名。这是不安全的 - 审计方式 ```shell script [root@localhost ~]# docker ps --quiet --all |xargs docker inspect --format '{{.Id}}:UTSMode={{.HostConfig.UTSMode}}' 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:UTSMode= 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:UTSMode= 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:UTSMode= d35fd7bd5e90e6aebc237368453361f632f775490da3c1d28011b9f7e43ff75c:UTSMode= cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:UTSMode= 如果上述命令返回host，则意味着主机UTS名称空间与容器共享，是不符合要求的。 如果上述命令不返回任何内容，则主机的UTS名称空间不共享 修复建议 不要使用--uts=host参数启动容器。例如，不要启动如下容器： ```shell script docker run -idt --uts=host alpine ### 4.26 `docker exec`命令不能使用特权选项 - 描述 不要使用`--privileged`选项来执行`docker exec` - 隐患分析 在`docker exec`中使用`--privileged`选项可为命令提供扩展的`Linux`功能。这可能会造成不安全的情况 - 修复建议 在`docker exec`命令中不要使用`--privileged`选项 ### 4.27 `docker exec`命令不能与`user`选项一起使用 - 描述 不要使用`--user`选项执行`docker exec` - 隐患分析 在`docker exec`中使用`--user`选项以该用户身份在容器内执行该命令。这可能会造成不安全的情况。 例如，假设你的容器是以`tomcat`用户（或任何其他非`root`用户）身份运行的， 那么可以使用`--user=root`选项以`root`用户身份运行命令，这是非常危险的 - 修复建议 在`docker exec`命令中不要使用`--user`选项 ### 4.28 检查容器运行时状态 - 描述 如果容器镜像没有定义`HEALTHCHECK`指令，请在容器运行时使用`--health-cmd`参数来检查容器运行状况 - 隐患分析 可用性是安全一个重要特性。如果您用的容器镜像没有预定义的`HEALTHCHECK`指令， 请使用`--health-cmd`参数在运行时检查容器运行状况。根据报告的健康状况，可以采取必要的措施 - 审计方式 运行以下命令并确保所有容器都报告运行状况 ```shell script [root@localhost ~]# docker ps --quiet |xargs -n1 docker inspect --format='{{.State.Health.Status}}' Template parsing error: template: :1:8: executing \"\" at : map has no entry for key \"Health\" Template parsing error: template: :1:8: executing \"\" at : map has no entry for key \"Health\" Template parsing error: template: :1:8: executing \"\" at : map has no entry for key \"Health\" Template parsing error: template: :1:8: executing \"\" at : map has no entry for key \"Health\" 修复建议 添加--health-cmd参数 ```shell script [root@localhost ~]# docker run --name=test -d \\ --health-cmd='stat /etc/passwd || exit 1' \\ --health-interval=2s \\ busybox:1.31.1 sleep 1d 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc [root@localhost ~]# 1.31.1 -bash: 1.31.1: command not found [root@localhost ~]# sleep 2; docker inspect --format='{{.State.Health.Status}}' test healthy [root@localhost ~]# docker exec test rm /etc/passwd [root@localhost ~]# sleep 2; docker inspect --format='{{.State.Health.Status}}' test unhealthy ``` 4.29 限制使用PID cgroup 描述 在容器运行时使用--pids-limit标志 隐患分析 攻击者可以在容器内发射fork炸弹。 这个fork炸弹可能会使整个系统崩溃，并需要重新启动主机以使系统重新运行。 PIDs cgroup --pids-limit将通过限制在给定时间内可能发生在容器内的fork数来防止这种攻击 审计分析 运行以下命令并确保PidsLimit未设置为0或-1。 PidsLimit为0或-1意味着任何数量的进程可以同时在容器内分叉。 ```shell script [root@localhost ~]# docker ps --quiet | xargs docker inspect --format='{{.Id}}:PidsLi mit={{.HostConfig.PidsLimit}}' 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:PidsLimit= d35fd7bd5e90e6aebc237368453361f632f775490da3c1d28011b9f7e43ff75c:PidsLimit= cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:PidsLimit= - 修复建议 升级内核至`4.3+`，添加`--pids-limit参数`，如 ```shell script docker run -idt --name=box --pids-limit=100 busybox:1.31.1 4.30 不要使用Docker的默认网桥docker0 描述 不要使用Docker的默认bridge docker0。 使用Docker的用户定义的网络进行容器联网 隐患分析 Docker将以桥模式创建的虚拟接口连接到名为docker0的公共桥。 这种默认网络模型易受ARP欺骗和MAC洪泛攻击的攻击，因为没有应用过滤 审计方式 运行以下命令，并验证容器是否在用户定义的网络上，而不是默认的docker0网桥 ```shell script [root@localhost ~]# docker network ls --quiet|xargs docker network inspect --format='{{.Name}}.{{.Options}}'|grep docker0 bridge.map[com.docker.network.bridge.default_bridge:true com.docker.network.bridge.enable_icc:true com.docker.network.bridge.enable_ip_masquerade:true com.docker.network.bridge.host_binding_ipv4:0.0.0.0 com.docker.network.bridge.name:docker0 com.docker.network.driver.mtu:1500] 若返回值不为空，说明使用`docker0`网桥 - 修复建议 **使用自定义网桥** > 关于自定义网桥与默认docker0网桥的主要区别 - 自定义网桥自动提供容器间的`DNS`解析 默认网桥通过`IP`地址实现容器间的寻址，也可通过`--link`参数实现容器`DNS`解析（容器A名称->容器A IP地址），但不推荐`--link`方式 - 自定义网桥提供更好的隔离 如果宿主机上所有容器没有指定`--network`参数，那它们将使用默认网桥`docker0`，并可以无限制的互相通信，存在一定安全隐患。 而自定义网桥提供了的网络隔离，只有相同网络域（network）内的容器才能相互访问 > 创建自定义网桥 ```shell script docker network create nginx-net 运行测试用例 ```shell script [root@localhost ~]# docker run -idt --name=nginx --network=nginx-net nginx:1.14-alpine [root@localhost ~]# docker run -idt --name=box --network=nginx-net busybox:1.31.1 [root@localhost ~]# docker exec box wget nginx -S Connecting to nginx (172.18.0.2:80) HTTP/1.1 200 OK Server: nginx/1.14.2 Date: Sat, 01 May 2021 07:06:59 GMT Content-Type: text/html Content-Length: 612 Last-Modified: Wed, 10 Apr 2019 01:08:42 GMT Connection: close ETag: \"5cad421a-264\" Accept-Ranges: bytes ### 4.31 任何容器内不能安装`Docker`套接字 - 描述 `docker socket`不应该安装在容器内 - 隐患分析 如果`Docker`套接字安装在容器内，它将允许在容器内运行的进程执行`Docker`命令，这有效地允许完全控制主机 - 审计方式 ```shell script [root@localhost ~]# docker ps --quiet | xargs docker inspect --format='{{.Id}}:Volumes={{.Mounts}}'|grep docker.sock 上述命令将返回docker.sock作为卷映射到容器的任何实例 修复建议 确保没有容器将docker.sock作为卷 5.Docker安全操作 5.1 避免镜像泛滥 描述 不要在同一主机上保留大量容器镜像，根据需要仅使用标记的镜像。 隐患分析 标记镜像有助于从latest退回到生产中镜像的特定版本。 如果实例化了未使用或旧标签的镜像，则可能包含可能被利用的漏洞。 此外，如果您无法从系统中删除未使用的镜像，并且存在各种此类冗余和未使用的镜像，主机文件空间可能会变满，从而导致拒绝服务。 审计方式 1.通过执行以下命令列出当前实例化的所有镜像ID ```shell script [root@localhost ~]# docker images --quiet | xargs docker inspect --format='{{.Id}}:Image={{.Config.Image}}' sha256:d6e46aa2470df1d32034c6707c8041158b652f38d2a9ae3d7ad7e7532d22ebe0:Image=sha256:3543079adc6fb5170279692361be8b24e89ef1809a374c1b4429e1d560d1459c > 2.通过执行以下命令列出系统中存在的所有镜像 ```shell script [root@localhost ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE harbor.wl.com/public/alpine latest d6e46aa2470d 6 months ago 5.57MB 3.比较步骤1和步骤2中的镜像ID列表，找出当前未实例化的镜像。如果发现未使用或旧镜像，请与系统管理员讨论是否需要在系统上保留这些镜像 修复建议 保留您实际需要的一组镜像，并建立工作流程以从主机中删除陈旧的镜像。 此外，使用诸如按摘要的功能从镜像仓库中获取特定镜像。 对于无用镜像，应予以删除 5.2 避免容器泛滥 描述 不要在同一主机上保留大量无用容器 隐患分析 容器的灵活性使得运行多个应用程序实例变得很容易，并间接导致存在于不同安全补丁级别的Docker镜像。 因此，避免容器泛滥，并将主机上的容器数量保持在可管理的总量上 审计方式 1.查找主机上的容器总数 ```shell script [root@localhost ~]# docker info --format '{{.Containers}}' 1 > 2.执行以下命令以查找主机上实际正在运行或处于停止状态的容器总数。 ```shell script [root@localhost ~]# docker info --format '{{.ContainersStopped}}' 0 [root@localhost ~]# docker info --format '{{.ContainersRunning}}' 1 如果主机上保留的容器数量与主机上实际运行的容器数量之间的差异很大（比如说 25 或更多）， 那么请清理无用容器（确保stopped无用再进行清理）。 修复建议 定期检查每个主机的容器清单，并使用以下命令清理已停止的容器 [root@localhost ~]# docker container prune WARNING! This will remove all stopped containers. Are you sure you want to continue? [y/N] y Total reclaimed space: 0B 最佳实践 安装 安装更新CentOS 7最新稳定版 降低低版本操作系统可能存在的安全漏洞 安装更新最新稳定版内核 香港镜像源 更新高版本内核以支持docker新特性、降低内核导致的安全漏洞风险 安装最新稳定版docker-ce docker-ce 二进制下载地址 docker-ce 镜像源 配置 配置limit参数 ```shell script ulimit -HSn 65536 cat >/etc/security/limits.conf soft nofile 65536 hard nofile 65536 soft noproc 10240 hard noproc 10240 EOF ``` 配置内核参数 ```shell script cat >/etc/sysctl.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-arptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 user.max_user_namespaces=15000 EOF sysctl -p > 配置`docker daemon` ```shell script mkdir -p /etc/docker cat /etc/docker/daemon.json { \"log-opts\": { \"max-size\": \"5m\", \"max-file\":\"3\" }, \"userland-proxy\": false, \"live-restore\": true, \"default-ulimits\": { \"nofile\": { \"Hard\": 64000, \"Name\": \"nofile\", \"Soft\": 64000 } }, \"default-address-pools\": [ { \"base\": \"172.80.0.0/16\", \"size\": 24 }, { \"base\": \"172.90.0.0/16\", \"size\": 24 } ], \"no-new-privileges\": false, \"default-gateway\": \"\", \"default-gateway-v6\": \"\", \"default-runtime\": \"runc\", \"default-shm-size\": \"64M\", \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl daemon-reload systemctl restart docker 文件权限调整 shell script chmod 755 /etc/docker chown root:root /etc/docker chmod 660 /var/run/docker.sock chown root:docker /var/run/docker.sock systemctl show -p FragmentPath docker.socket|sed \"s/FragmentPath=//\"|xargs -n1 chmod 644 systemctl show -p FragmentPath docker.service|sed \"s/FragmentPath=//\"|xargs -n1 chmod 644 systemctl show -p FragmentPath docker.socket|sed \"s/FragmentPath=//\"|xargs -n1 chown root:root systemctl show -p FragmentPath docker.service|sed \"s/FragmentPath=//\"|xargs -n1 chown root:root if [ -d /etc/docker/certs.d/ ];then chmod 444 /etc/docker/certs.d/*; fi if [ -d /etc/docker/certs.d/ ];then chown root:root /etc/docker/certs.d/*; fi if [ -f /etc/docker/daemon.json ];then chown root:root /etc/docker/daemon.json; fi 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/docker/storage/storage.html":{"url":"2.容器/docker/storage/storage.html","title":"storage","keywords":"","body":" Table of Contents generated with DocToc volums 管理方式 使用场景 管理volume Bind mounts volums与Bind mounts对比 关于volums与Bind mounts使用说明 tmpfs 存储驱动 可写入层 写时拷贝 overlay2 几种数据持久方案区别 volume：对宿主机文件系统之上的 容器可写入层 进行读写操作 Bind mounts：宿主机与容器以映射的方式共享目录，对宿主机文件系统进行读写操作 tmpfs：数据存储于内存中，不持久化 volums Volumes方式的数据存储于docker宿主机的/var/lib/docker/volumes/下（Linux系统下），由于linux严格的权限管理，非docker进程无法修改该目录下数据，具有很好的隔离性，volums为目前最好的docker容器数据持久化的方式。 管理方式 由docker进行创建管理，创建方式为两种： 1、手动创建 docker volume create 2、自动创建 随容器/服务的创建而被创建 使用场景 1、多容器运行时共享数据 2、当Docker主机不能保证具有给定的目录或文件结构时。卷可以帮助您将Docker主机的配置与容器运行时解耦。（例如A环境具有/data目录，当迁移至B环境不具有/data目录） 3、存储容器数据于远程主机 4、备份、迁移容器数据等至远程主机，备份目录 /var/lib/docker/volumes/ 管理volume #1、手动创建 docker volume create my-volume #查看volume docker inspect my-volume #2、创建启动容器时指定volume docker run -d --name test-nginx --mount source=myvol2,target=/app nginx:latest #查看volume列表(默认在创建启动容器时自动创建了myvol2) docker volume ls #接下来，测试删除容器后volume是否会随之被删除(很显然删除容器对volume并未产生影响，需手动删除) #手动删除volume volume指定为只读方式 #创建启动容器，指定volumew名为nginx-vol（实际路径为/var/lib/docker/volumes/nginx-vol），对应容器内/usr/share/nginx/html目录，并且为只读状态（容器内禁止写操作） docker run -d --name=nginxtest --mount source=nginx-vol,destination=/usr/share/nginx/html,readonly nginx:latest #查看本地volume情况(nginx容器内的数据已挂载出来) #访问容器内，验证可读属性（） docker container exec -it nginxtest bash cd /usr/share/nginx/html echo 1 >> 50x.html Bind mounts Bind mounts方式理论上可以在宿主机任意位置持久化数据，显然非docker进程可以修改这部分数据，隔离性较差。 使用场景 1、容器与宿主机共享配置文件等（如dns、时区等） 2、共享源代码或打包后的应用程序（例如：宿主机maven打包java程序，只需挂载target/目录，容器内即可访问到打包好的应用程序） 当然，该方式仅适用于开发环境，安全考虑并不推荐使用于生产环境 3、Docker主机的文件或目录结构与容器所需的绑定挂载一致时。（例如容器内读取配置文件目录为/etc/redis.conf,而宿主机/etc/redis.conf并不存在，则需要匹配路径进行挂载） volums与Bind mounts对比 Volumes方式更容易备份、迁移 Volumes可以通过docker命令行指令或api进行管理 Volumes适用于windows与linux环境 Volumes多容器分享数据更安全（非docker进程无法修改该部分数据） Volume drivers可以实现加密传输数据持久化至远程主机。 新volume的内容可以由容器预先填充 关于volums与Bind mounts使用说明 如果你挂载一个空的volums到容器的/data目录，并且容器内/data下数据非空，则容器内的/data数据会拷贝到新挂载的卷上；相似的，如果你挂载了宿主机不存在的volums至容器内部，这个不存在的目录则会自动创建 如果你使用bind mount或挂载一个非空的volum到容器的/data目录，并且容器内/data下数据非空，则容器内的/data部分数据会被覆盖（相同目录/文件名） tmpfs tmpfs方式数据存储宿主机系统内存中，并且不会持久化到宿主机的文件系统中（磁盘） 使用场景 存储敏感数据（swarm利用tmpfs存放secrets于内存中） 数据共享 与volume、bind mounts方式不同，tmpfs无法跨容器共享数据，即仅适用于单机模式。 环境依赖说明 tmpfs仅支持Linux环境下docker使用 使用方式 #nginx容器内/app下的数据将写入至宿主机内存之中。 #1、方式一 docker run -d \\ -it \\ --name tmptest \\ --mount type=tmpfs,destination=/app \\ nginx:latest #2、方式二 docker run -d \\ -it \\ --name tmptest \\ --tmpfs /app \\ nginx:latest 验证数据是否存储于内存中 #1、分配虚拟机8G内存 #2、启动nginx容器，指定--tmpfs方式存储数据 #3、查看内存占用 docker ps #bf5605056391为容器ID ps -ef|grep bf5605056391 #11298为容器进程PID top -p 11298 #4、拷贝918M大小文件至容器/app下 ，观察内存变化 docker cp CentOS-7-x86_64-Minimal-1810.iso bf5605056391:/app/ #5、查看内存占用 free -h #6、多次拷贝观察宿主机内存变化（文件名注意修改，避免被覆盖） docker cp CentOS-7-x86_64-Minimal-1810.iso bf5605056391:/app/CentOS-7-x86_64-Minimal-1810.iso2 docker cp CentOS-7-x86_64-Minimal-1810.iso bf5605056391:/app/CentOS-7-x86_64-Minimal-1810.iso2 docker cp CentOS-7-x86_64-Minimal-1810.iso bf5605056391:/app/CentOS-7-x86_64-Minimal-1810.iso4 #7、内存变化（大约900M左右的递减，与文件大小相匹配） #8、让我们看下容器内部/app下存储了什么(nothing,说明容器内写入/app的数据都被写入到了内存里，/app下也并不会持久化) #9、如果我们通过不断向容器内/app下写入数据会怎样？(竟然没有出现crash相关情况发生，初步猜测最开始存放的数据被回收释放) #10、测试删除容器，内存是否会被释放 docker container rm -f bf5605056391 验证tmpfs存储大小限制 #1、启动nginx容器，1073741824为字节数（等于1GB） 换算地址 http://www.elecfans.com/tools/zijiehuansuan.html 官方配置说明 docker run -d \\ -it \\ --name tmptest \\ --mount type=tmpfs,destination=/app,tmpfs-size=1073741824 \\ nginx:latest #2、不断拷贝数据到容器内部，观察内存大小 竟然没有生效，难道是配置错了？ #3、删除容器，重启配置(官方提供信息指出默认单位是字节，可实际看来怎么是MB，难道理解有误？) docker run -d \\ -it \\ --name tmptest \\ --mount type=tmpfs,destination=/app,tmpfs-size=1024 \\ nginx:latest #4、删除容器，重启配置(1GB大小限制未生效，难道单位问题？) docker run -d \\ -it \\ --name tmptest \\ --mount type=tmpfs,destination=/app,tmpfs-size=1GB \\ nginx:latest #5、删除容器，重启配置(1G大小限制未生效) docker run -d \\ -it \\ --name tmptest \\ --mount type=tmpfs,destination=/app,tmpfs-size=1G \\ nginx:latest #6、删除容器，重启配置，限制大小10M(？？？竟然也未生效？，难道第三步测试有误) docker run -d \\ -it \\ --name tmptest \\ --mount type=tmpfs,destination=/app,tmpfs-size=10 \\ nginx:latest 仔细观察第三步测试，拷贝到容器内的文件名一样，果然测试有误 #7、看来这个配置项并未生效 存储驱动 以下内容基于overlay2 overlay2为docker存储驱动的一种，负责容器读写（不会修改镜像原始数据）镜像数据 可写入层 writable layer 官方文档描述如下： When you start a container, a thin writable container layer is added on top of the other layers. Any changes the container makes to the filesystem are stored here. Any files the container does not change do not get copied to this writable layer. This means that the writable layer is as small as possible. 假设现在有镜像A，B构建文件如下 #A镜像构建文件（构建镜像名为：imageA，拷贝内容为hello.sh、app.py） FROM ubuntu:18.04 COPY ./hello.py ./hello.sh /app/ CMD python /app/app.py #B镜像构建文件（构建镜像名为：imageB） FROM imageB CMD /bin/bash /app/hello.sh #hello.sh内容 #!/bin/sh echo \"Hello world\" #hello.py内容 print('hello python!') 目录结构及文件内容如下 构建镜像ab docker build -t imagea -f Dockerfile.A . docker build -t imageb -f Dockerfile.B . 查看两者层级 很显然，A镜像红框内的3个层级分别由以下构建命令生成，其他层级由基础镜像ubuntu:18.04生成 FROM ubuntu:18.04 COPY ./hello.py ./hello.sh /app/ CMD python /app/app.py 而B镜像对比A镜像之多出一个层级313438ff3ff4，即以下构建指令生成的层，该层即为可写入层，镜像B与镜像A的区别存储于该层 而镜像B与镜像A相同的层级指向同一系统存储地址， 由于该层仅为一条shell指令故大小可以忽略不计，即构建B镜像理论上对宿主机磁盘的占用忽略不计，由此可看见docker的分层结构相当节省存储空间 Any changes the container makes to the filesystem are stored here CMD /bin/bash /app/hello.sh 测试以上结论是否正确 构建镜像CD，配置文件内容为如下： #Dockerfile.C FROM imageb COPY CentOS-7-x86_64-Minimal-1804.iso / #Dockerfile.D FROM imagec CMD /bin/bash /app/hello.py 此时磁盘空间大小 文件大小为906M，理论上构建CD镜像后，磁盘剩余空间在14031-906=13125M左右 docker build -t imagec -f Dockerfile.C . docker build -t imaged -f Dockerfile.D . 很显然测试结果验证了以上的结论（当然官方已经说明了，这里仅仅是测试而已），我们再看下镜像cd的层级 测试删除镜像C，对磁盘空间有何影响 分析：由于镜像C与镜像D有相同分层，且镜像D比镜像C多一个层级，即可以理解为D内数据包含C数据，删除镜像C对宿主机磁盘空间无任何影响 由此可见，验证了我们的猜测：删除镜像C对宿主机磁盘空间无任何影响（存在镜像D包含镜像C所有数据内容） 测试生成镜像E，构建内容为镜像C的内容，宿主机磁盘空间是否变化。 分析：因为E的所有数据内容都已存在（镜像D持有），故磁盘空间不会发生变化 #Dockerfile.E FROM imageb COPY CentOS-7-x86_64-Minimal-1804.iso / docker build -t imagee -f Dockerfile.E . 显然猜测成立 测试删除镜像D、E，对磁盘空间有何影响 分析：由于镜像C、镜像D、镜像E均持有COPY CentOS-7-x86_64-Minimal-1804.iso /层级，镜像C已经删除，此时再删除镜像D、镜像E， COPY CentOS-7-x86_64-Minimal-1804.iso /层级无其他镜像引用，导致磁盘空间释放906M 猜测成立！ 结论如下： docker分层架构在很大程度上节省了磁盘存储开销（镜像文件一般较大），相同层级只存储一份 删除镜像时，只会删除与其他镜像非同层级数据 copy-on-write 写时拷贝 官方对Copy-on-write的说明: Copy-on-write is a strategy of sharing and copying files for maximum efficiency. If a file or directory exists in a lower layer within the image, and another layer (including the writable layer) needs read access to it, it just uses the existing file. The first time another layer needs to modify the file (when building the image or running the container), the file is copied into that layer and modified. This minimizes I/O and the size of each of the subsequent layers. These advantages are explained in more depth below 总结为：容器需要读写底层（镜像的层级）数据时，会将文件或目录拷贝到容器`可写层`进行读写，而非全部拷贝（显然镜像的某些层级包含的文件在容器整个运行周期中可能并不会用到）。 测试写时拷贝 #构建镜像F #Dockerfile.F FROM centos COPY CentOS-7-x86_64-Minimal-1804.iso / docker build -t imagef -f Dockerfile.F . 分析：运行时并未读写其他层级数据，容器大小忽略不计 官方对Copy-on-write优势的说明: Not only does copy-on-write save space, but it also reduces start-up time. When you start a container (or multiple containers from the same image), Docker only needs to create the thin writable container layer 总结：节省存储空间、容器启动快 overlay2 overlay2可理解为连接container (upperdir)与image (lowerdir)的纽带，类比显卡驱动等 容器读取文件 官方列举三个场景 The file does not exist in the container layer: If a container opens a file for read access and the file does not already exist in the container (upperdir) it is read from the image (lowerdir). This incurs very little performance overhead. #如果容器层不存在该文件，将从镜像层读取，官方表示性能损耗较小。 The file only exists in the container layer: If a container opens a file for read access and the file exists in the container (upperdir) and not in the image (lowerdir), it is read directly from the container. #如果容器层存在该文件，将直接从容器层读取 The file exists in both the container layer and the image layer: If a container opens a file for read access and the file exists in the image layer and the container layer, the file’s version in the container layer is read. Files in the container layer (upperdir) obscure files with the same name in the image layer (lowerdir). #如果容器层、镜像层均存在该文件，优先读取容器层文件的版本 容器修改文件|目录 官方列举几个场景： Writing to a file for the first time: The first time a container writes to an existing file, that file does not exist in the container (upperdir). The overlay/overlay2 driver performs a copy_up operation to copy the file from the image (lowerdir) to the container (upperdir). The container then writes the changes to the new copy of the file in the container layer. #第一次修改容器层不存在的文件时，overlay驱动执行`copy_up`操作，将文件从镜像层拷贝到容器层，然后容器将\"更改\"写入容器层中文件的新副本 However, OverlayFS works at the file level rather than the block level. This means that all OverlayFS copy_up operations copy the entire file, even if the file is very large and only a small part of it is being modified. This can have a noticeable impact on container write performance. However, two things are worth noting: 1、The copy_up operation only occurs the first time a given file is written to. Subsequent writes to the same file operate against the copy of the file already copied up to the container. 2、OverlayFS only works with two layers. This means that performance should be better than AUFS, which can suffer noticeable latencies when searching for files in images with many layers. This advantage applies to both overlay and overlay2 drivers. overlayfs2 is slightly less performant than overlayfs on initial read, because it must look through more layers, but it caches the results so this is only a small penalty. #只有第一次文件拷贝为全量拷贝，以后读写操作均操作容器层上的文件副本 Deleting files and directories: 1、When a file is deleted within a container, a whiteout file is created in the container (upperdir). The version of the file in the image layer (lowerdir) is not deleted (because the lowerdir is read-only). However, the whiteout file prevents it from being available to the container. 2、When a directory is deleted within a container, an opaque directory is created within the container (upperdir). This works in the same way as a whiteout file and effectively prevents the directory from being accessed, even though it still exists in the image (lowerdir). #当删除容器内的某一文件（假如/usr/bin/telnet）或目录（/boot）时，对应镜像层的文件不会被删除（只读性），虽然镜像层依然存在该文件，但容器层已服务对其进行读写 Renaming directories: Calling rename(2) for a directory is allowed only when both the source and the destination path are on the top layer. Otherwise, it returns EXDEV error (“cross-device link not permitted”). Your application needs to be designed to handle EXDEV and fall back to a “copy and unlink” strategy. #重命名容器内目录：（不太理解） 只有当源路径和目标路径都位于顶层时，才允许为目录调用rename(2)。 否则，它将返回EXDEV错误(“不允许跨设备链接”)。您的应用程序需要设计成能够处理EXDEV并返回到“复制和断开链接”策略。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/docker/thin/thin.html":{"url":"2.容器/docker/thin/thin.html","title":"thin","keywords":"","body":"镜像体积小的优势：传输快、加载快、 1、根据场景选取基础镜像 jdk -> 选取openjdk镜像作为基础镜像 非不是centos ubuntu等操作系统镜像 2、利用multistage-build Use multistage builds. For instance, you can use the maven image to build your Java application, then reset to the tomcat image and copy the Java artifacts into the correct location to deploy your app, all in the same Dockerfile. This means that your final image doesn’t include all of the libraries and dependencies pulled in by the build, but only the artifacts and the environment needed to run them. #以下整个流程在一个Dockerfile内实现 1、选取maven基础镜像进行打包JAVA程序 2、拷贝Jar至tomcat基础镜像内（spring boot的话直接jdk基础镜像） 3、发布 multistage-build样例 FROM golang:1.7.3 WORKDIR /go/src/github.com/alexellis/href-counter/ RUN go get -d -v golang.org/x/net/html COPY app.go . RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app . FROM alpine:latest RUN apk --no-cache add ca-certificates WORKDIR /root/ COPY --from=0 /go/src/github.com/alexellis/href-counter/app . CMD [\"./app\"] 显然，只是把多个步骤合并到同一Dockfile呢，降低构造镜像成本。 3、减少层级 场景一 合并命令，每一行命令均产生一个层级 #合并前 RUN apt-get -y update RUN apt-get install -y python #合并后 RUN apt-get -y update && apt-get install -y python 场景二 #制作适合自己的基础镜像（适用于多个application场景，并且基础层级相同较多的） Docker only needs to load the common layers once, and they are cached. This means that your derivative images use memory on the Docker host more efficiently and load more quickly 场景 由于测试镜像的话，可能需要安装一些测试软件等，保证两者的区别处于镜像最高层级（还是为了充分复用相同层级） To keep your production image lean but allow for debugging, consider using the production image as the base image for the debug image. Additional testing or debugging tooling can be added on top of the production image 场景四 制作镜像时，打上tag标签 When building images, always tag them with useful tags which codify version information, intended destination (prod or test, for instance), stability, or other information that is useful when deploying the application in different environments. Do not rely on the automatically-created latest tag 4、程序数据持久化问题 #避免将数据写入容器内部，这样不仅会增加容器体积，并且I/O读写效率比挂载模式要低 Avoid storing application data in your container’s writable layer using storage drivers. This increases the size of your container and is less efficient from an I/O perspective than using volumes or bind mounts. Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/addons/addons.html":{"url":"2.容器/k8s/addons/addons.html","title":"addons","keywords":"","body":" 负载均衡器Porter 配置热重载 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/application/helm.html":{"url":"2.容器/k8s/application/helm.html","title":"helm","keywords":"","body":" Table of Contents generated with DocToc helm 离线安装tiller helm 安装 curl -O https://storage.googleapis.com/kubernetes-helm/helm-v2.14.3-linux-amd64.tar.gz tar zxvf helm-v2.12.1-linux-amd64.tar.gz chmod +x linux-amd64/helm mv linux-amd64/helm /usr/local/bin 添加helm service account 并添加到clusteradmin 这个clusterrole上 kubectl create serviceaccount --namespace=kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller 安装tiller helm init -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.14.3 --stable-repo-url http://mirror.azure.cn/kubernetes/charts/ --service-account tiller --override spec.selector.matchLabels.'name'='tiller',spec.selector.matchLabels.'app'='helm' --output yaml | sed 's@apiVersion: extensions/v1beta1@apiVersion: apps/v1@' | kubectl apply -f - 查看 kubectl get pods -n kube-system | grep tiller 离线安装tiller 搭建本地仓储 http://10.16.48.44/ docker pull fishead/gcr.io.kubernetes-helm.tiller:v2.11.0 上传导入 helm init --upgrade --service-account tiller --tiller-image fishead/gcr.io.kubernetes-helm.tiller:v2.11.0 --stable-repo-url http://10.16.48.44/ Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/base/intro.html":{"url":"2.容器/k8s/base/intro.html","title":"intro","keywords":"","body":"基本概念 容器的本质 一个视图被隔离、资源受限的进程 容器里PID=1的进程就是应用本身 管理虚拟机=管理基础设施 管理容器=直接管理应用本身 什么是kubernetes 云时代的操作系统 以此类推，容器镜像->操作系统的软件安装包 容器 -> 系统进程 Pod -> 线程组（一个pod可以包含多个容器） pod 为什么需要pod? 首先容器是单进程模型，容器的PID=1的进程应为应用本身进程 其次同一容器内启动多个进程，难以管理进程生命周期，也起不到隔离作用（ns,cgroup） 尽可能遵循: 一个容器只做一件事 同一pod网络共享 通过infra容器实现： 同一pod内的容器挂载infra容器（k8s.gcr.io/pause）的网络命名空间，实现网络共享 同一pod内的容器通信地址为localhost 同一pod内的容器共享同一ip地址，端口不可重复 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/changelog/changelog.html":{"url":"2.容器/k8s/changelog/changelog.html","title":"changelog","keywords":"","body":"k8s版本更新说明 发布记录 2017 年 6 月 29 日，Kubernetes 1.7 发布 2017 年 9 月 28 日，Kubernetes 1.8 发布 2017 年 12 月 15 日，Kubernetes 1.9 发布 2018 年 3 月 26 日，Kubernetes 1.10 发布 2018 年 6 月 27 日，Kubernetes 1.11 发布 2018 年 9 月 27 日，Kubernetes 1.12 发布 2018 年 12 月 3 日，Kubernetes 1.13 发布 2019 年 3 月 26 日，Kubernetes 1.14 发布 2019 年 6 月 20 日，Kubernetes 1.15 发布 2019 年 9 月 19 日，Kubernetes 1.16 发布 2019 年 12 月 10 日，Kubernetes 1.17 发布 2020 年 3 月 25 日，Kubernetes 1.18 发布 2020 年 8 月 26 日，Kubernetes 1.19 发布1.2 1.3 1.4 1.5 1.6 1.7 2017年6月29日，kuberentes1.7发布。该版本的kubernetes在安全性、存储和可扩展性方面有了很大的提升。 这些新特性中包含了安全性更高的加密的secret、pod间通讯的网络策略，限制kubelet访问的节点授权程序以及客户端/服务器TLS证书轮换。 对于那些在Kubernetes上运行横向扩展数据库的人来说，这个版本有一个主要的特性，可以为StatefulSet添加自动更新并增强DaemonSet的更新。我们还宣布了对本地存储的Alpha支持，以及用于更快地缩放StatefulSets的突发模式。 此外，对于高级用户，此发行版中的API聚合允许使用用于自定义的API与API server同时运行。其他亮点包括支持可扩展的准入控制器，可插拔云供应商程序和容器运行时接口（CRI）增强功能。 新功能 安全 Network Policy API提升为稳定版本。用户可以通过使用网络插件实现的网络策略来控制哪些Pod之间能够互相通信。 节点授权和准入控制插件是新增加的功能，可以用于限制kubelet可以访问的secret、pod和其它基于节点的对象。 加密的Secret和etcd中的其它资源，现在是alpha版本。 Kubelet TLS bootstrapping现在支持客户端和服务器端的证书轮换。 由API server存储的审计日志现在更具可定制性和可扩展性，支持事件过滤和webhook。它们还为系统审计提供更丰富的数据。 有状态负载 StatefulSet更新是1.7版本的beta功能，它允许使用包括滚动更新在内的一系列更新策略自动更新诸如Kafka，Zookeeper和etcd等有状态应用程序。 StatefulSets现在还支持对不需要通过Pod管理策略进行排序的应用程序进行快速扩展和启动。这可以是主要的性能改进。 本地存储（alpha）是有状态应用程序最常用的功能之一。用户现在可以通过标准的PVC/PV接口和StatefulSet中的StorageClass访问本地存储卷。 DaemonSet——为每个节点创建一个Pod，现在有了更新功能，在1.7中增加了智能回滚和历史记录功能。 新的StorageOS Volume插件可以使用本地或附加节点存储中以提供高可用的集群范围的持久卷。 可扩展性 运行时的API聚合是此版本中最强大的扩展功能，允许高级用户将Kubernetes风格的预先构建的第三方或用户创建的API添加到其集群中。 容器运行时接口（CRI）已经增强，可以使用新的RPC调用从运行时检索容器度量。 CRI的验证测试已经发布，与containerd进行了Alpha集成，现在支持基本的生命周期和镜像管理。 其它功能 引入了对外部准入控制器的Alpha支持，提供了两个选项，用于向API server添加自定义业务逻辑，以便在创建对象和验证策略时对其进行修改。 基于策略的联合资源布局提供Alpha版本，用于根据自定义需求（如法规、定价或性能）为联合（federated）集群提供布局策略。 弃用 第三方资源（TPR）已被自定义资源定义（Custom Resource Definitions，CRD）取代，后者提供了一个更清晰的API，并解决了TPR测试期间引发的问题和案例。如果您使用TPR测试版功能，则建议您迁移，因为它将在Kubernetes 1.8中被移除。 1.8 1.9 1.10 1.11 1.12 1.13 1.14 1.15 1.16 1.17 1.18 1.19 1.20 参考文献 开源中文手册 k8s中文社区 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/cmd/cmd.html":{"url":"2.容器/k8s/cmd/cmd.html","title":"cmd","keywords":"","body":"禁止/允许调度 允许调度 kubectl patch node -p \"{\\\"spec\\\":{\\\"unschedulable\\\":false}}\" 修改默认sc kubectl patch storageclass -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' 节点标签 # kubectl label nodes key=value kubectl label nodes ceph01 role=storage-node 驱逐业务容器 kubectl drain --ignore-daemonsets --delete-local-data 清理Evicted状态pod ``shell script for ns inkubectl get ns | awk 'NR>1{print $1}'` do kubectl get pods -n ${ns} | grep Evicted | awk '{print $1}' | xargs kubectl delete pod -n ${ns} done ### 配置 > 修改`kubernetes`限制节点`pod`数量 **默认100** 编辑 ```shell vim /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf 调整修改 Environment=\"KUBELET_NODE_MAX_PODS=--max-pods=600\" ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS $KUBELET_NODE_MAX_PODS 重启kubelet systemctl daemon-reload systemctl restart kubelet 删除命名空间 reason ceph-csi注意替换 curl -H \"Content-Type: application/json\" -XPUT -d '{\"apiVersion\":\"v1\",\"kind\":\"Namespace\",\"metadata\":{\"name\":\"ceph-csi\"},\"spec\":{\"finalizers\":[]}}' http://localhost:8001/api/v1/namespaces/ceph-csi/finalize 变更default StorageClass kubectl patch storageclass -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' 检查证书是否过期 kubeadm alpha certs check-expiration 或 openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text |grep ' Not ' 手动更新证书 kubeadm alpha certs renew Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/faq/faq.html":{"url":"2.容器/k8s/faq/faq.html","title":"faq","keywords":"","body":"1.什么是K8s？ k8s是一套开源的容器编排系统，负责管理容器部署、扩缩容及负载均衡 2.Kubernetes与Docker有什么关系？ docker是一种容器运行时，基于命名空间与控制组技术实现进程级虚拟化。 k8s管理容器生命周期的编排系统。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/ingress/ingress.html":{"url":"2.容器/k8s/ingress/ingress.html","title":"ingress","keywords":"","body":"nginx ingress 配置手册 可以通过下面三种方式配置nginx k8s ConfigMap配置项方式 k8s Annotations注解方式 注解方式-常用配置 注解的key与value取值为字符串类型. 布尔或数字等类型必须加引号如: \"true\", \"false\", \"100\" body体大小 设置每个location读取客户端请求体的缓冲区大小。如果请求体大于缓冲区，则整个请求体或仅其部分被写入一个临时文件。 默认情况下，缓冲区大小等于两个内存页。这在x86、其他32位平台和x86-64上是8K。在其他64位平台上通常是16K。 ```shell script nginx.ingress.kubernetes.io/client-body-buffer-size: 1M 对应原生`nginx`配置 ```shell script Syntax: client_body_buffer_size size; Default: client_body_buffer_size 8k|16k; Context: http, server, location Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/install/":{"url":"2.容器/k8s/install/","title":"install","keywords":"","body":" kubeadm kubeadm rke Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/install/binary.html":{"url":"2.容器/k8s/install/binary.html","title":"binary","keywords":"","body":"二进制安装 编译源码 安装golang 下载解压配置 ```shell script tar zxvf go1.16.5.linux-amd64.tar.gz -C /usr/local/ cat >> ~/.bash_profile ### 编译全部组件 ```shell script unzip kubernetes-1.18.6.zip cd kubernetes-1.18.6 yum install -y rsync rm -rf _output make -j4 安装k8s主节点 安装etcd 下载解压etcd etcd-v3.3.9-linux-amd64.tar.gz ```shell script tar -zxvf etcd-v3.3.9-linux-amd64.tar.gz sudo cp etcd-v3.3.9-linux-amd64/{etcd,etcdctl} /usr/bin/ > 配置服务 ```shell script sudo tee /usr/lib/systemd/system/etcd.service 创建目录 ```shell script mkdir /var/lib/etcd > 启动 ```shell script sudo systemctl daemon-reload sudo systemctl enable etcd.service --now 查看集群状态 shell script [root@localhost ~]# etcdctl cluster-health member 8e9e05c52164694d is healthy: got healthy result from http://localhost:2379 cluster is healthy 上传启动 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/install/k8s-kubeadm.html":{"url":"2.容器/k8s/install/k8s-kubeadm.html","title":"k8s-kubeadm","keywords":"","body":" Table of Contents generated with DocToc 单机 HA 所有节点安装kubeadm和kubelet 配置负载均衡 创建集群 新worker节点加入集群 单机 配置yum源 升级内核 安装docker 关闭防火墙 systemctl stop firewalld && systemctl disable firewalld iptables -F && iptables -X && iptables -F -t nat && iptables -X -t nat && iptables -P FORWARD ACCEPT 关闭swap swapoff -a sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab sysctl -p 设置内核参数 modprobe br_netfilter cat 修改Linux 资源配置文件,调高ulimit最大打开数和systemctl管理的服务文件最大打开数 echo \"* soft nofile 655360\" >> /etc/security/limits.conf echo \"* hard nofile 655360\" >> /etc/security/limits.conf echo \"* soft nproc 655360\" >> /etc/security/limits.conf echo \"* hard nproc 655360\" >> /etc/security/limits.conf echo \"* soft memlock unlimited\" >> /etc/security/limits.conf echo \"* hard memlock unlimited\" >> /etc/security/limits.conf echo \"DefaultLimitNOFILE=1024000\" >> /etc/systemd/system.conf echo \"DefaultLimitNPROC=1024000\" >> /etc/systemd/system.conf 配置k8s源 cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum clean all && yum makecache 安装依赖 yum install -y epel-release conntrack ipvsadm ipset jq sysstat curl iptables libseccomp yum-utils device-mapper-persistent-data lvm2 安装指定版本 查看版本列表 ```shell script yum list kubeadm --showduplicates|sort -r > 安装指定版本`kubeadm` 安装`1.18.6`版本 ```shell script version=1.18.6-0 yum install -y kubelet-$version kubeadm-$version kubectl-$version --disableexcludes=kubernetes 安装命令补全 ```shell script yum install -y bash-completion source /usr/share/bash-completion/bash_completion source > ~/.bashrc > 启动`kubelet` ```shell script systemctl enable kubelet --now 下载k8s相关镜像 ``shell script for i inkubeadm config images list 2>/dev/null |sed 's/k8s.gcr.io\\///g'`; do docker pull registry.aliyuncs.com/google-containers/${i} docker tag registry.aliyuncs.com/google-containers/${i} k8s.gcr.io/${i} docker rmi registry.aliyuncs.com/google-containers/${i} done 修改kubelet配置 sed -i \"s;KUBELET_EXTRA_ARGS=;KUBELET_EXTRA_ARGS=\\\"--fail-swap-on=false\\\";g\" /etc/sysconfig/kubelet 启动kubelet systemctl enable --now kubelet 初始化集群 k8sversion=`kubeadm version -o yaml|grep gitVersion|sed 's#gitVersion:##g'|sed 's/ //g'` echo \"k8s version: $k8sversion\" kubeadm init --kubernetes-version=$k8sversion --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap 配置授权 mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 安装网络 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml ### HA [参考地址1](https://www.kubernetes.org.cn/5551.html) [参考地址2](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/) - 架构图 ![](images/kubeadm-ha-topology-stacked-etcd.svg) - 节点说明 | hostname | IP地址 | 应用| | :----: | :----: | :----:| | node1 | 172.16.145.160 | docker kubelet kubeadm kubectl keepalived harproxy control-plane| | node2 | 172.16.145.161 | docker kubelet kubeadm kubectl keepalived harproxy control-plane| | node3 | 172.16.145.162 | docker kubelet kubeadm kubectl keepalived harproxy control-plane| | | 172.16.145.200 | | 172.16.145.200为虚拟IP - [所有节点升级内核](/linux/update/kernel.md) - [所有节点安装docker](/os/virtaul/containerontainer/docker/docker-install-online.md) 添加Host解析 cat >> /etc/hosts /etc/sysconfig/modules/ipvs.modules > /etc/docker/daemon.json /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum clean all && yum makecache 安装依赖 yum install -y epel-release conntrack jq sysstat curl iptables libseccomp yum-utils device-mapper-persistent-data lvm2 安装kubelet等 yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes 安装命令补全 yum install -y bash-completion source /usr/share/bash-completion/bash_completion source > ~/.bashrc 修改kubelet配置 sed -i \"s;KUBELET_EXTRA_ARGS=;KUBELET_EXTRA_ARGS=\\\"--fail-swap-on=false\\\";g\" /etc/sysconfig/kubelet 启动kubelet systemctl enable --now kubelet 下载k8s相关镜像 for i in `kubeadm config images list 2>/dev/null |sed 's/k8s.gcr.io\\///g'`; do docker pull gcr.azk8s.cn/google-containers/${i} docker tag gcr.azk8s.cn/google-containers/${i} k8s.gcr.io/${i} docker rmi gcr.azk8s.cn/google-containers/${i} done 配置时钟同步 yum -y install ntpdate ntpdate ntp1.aliyun.com echo \"*/5 * * * * bash ntpdate ntp1.aliyun.com\" >> /etc/crontab 修改时区 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime #### 配置负载均衡 **keepalived+haproxy方式** 所有节点安装harproxy yum -y install haproxy 修改配置文件 vim /etc/haproxy/haproxy.cfg 添加以下配置 frontend kubernetes-apiserver mode tcp bind *:8443 option tcplog default_backend kubernetes-apiserver backend kubernetes-apiserver mode tcp balance roundrobin server node1 172.16.145.160:6443 check server node2 172.16.145.161:6443 check server node3 172.16.145.162:6443 check 启动 systemctl enable haproxy --now 安装keepalived yum install -y keepalived node1节点配置 cat > /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1t gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum clean all && yum makecache 安装依赖 yum install -y epel-release conntrack ipvsadm ipset jq sysstat curl iptables libseccomp yum-utils device-mapper-persistent-data lvm2 安装kubelet等 yum install -y kubelet kubeadm --disableexcludes=kubernetes 安装命令补全 yum install -y bash-completion source /usr/share/bash-completion/bash_completion source > ~/.bashrc 下载k8s相关镜像 #k8s.gcr.io被墙，需要走微软代理地址 for i in `kubeadm config images list 2>/dev/null |sed 's/k8s.gcr.io\\///g'`; do docker pull gcr.azk8s.cn/google-containers/${i} docker tag gcr.azk8s.cn/google-containers/${i} k8s.gcr.io/${i} docker rmi gcr.azk8s.cn/google-containers/${i} done 配置时钟同步 yum -y install ntpdate ntpdate ntp1.aliyun.com echo \"*/5 * * * * bash ntpdate ntp1.aliyun.com\" >> /etc/crontab 修改时区 ```shell ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 修改kubelet配置 sed -i \"s;KUBELET_EXTRA_ARGS=;KUBELET_EXTRA_ARGS=\\\"--fail-swap-on=false\\\";g\" /etc/sysconfig/kubelet 启动kubelet systemctl enable --now kubelet 获取加入集群的token等 kubeadm token create --print-join-command 设置Hostname hostnamectl --static set-hostname work1 查看节点信息 kubectl get node 重复添加解决 https://blog.csdn.net/wzygis/article/details/84098247 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/install/k8s-rke.html":{"url":"2.容器/k8s/install/k8s-rke.html","title":"k8s-rke","keywords":"","body":" Table of Contents generated with DocToc k8s rke安装k8s集群 k8s rke安装k8s集群 rke文档地址 1、环境准备 虚拟机 * 3 配置阿里云yum源 2、下载rke二进制文件 安装节点下载即可 curl -L https://github.com/rancher/rke/releases/download/v0.3.2/rke_linux-amd64 -o /usr/bin/rke chmod +x /usr/bin/rke 3、关闭防火墙、selinux等 关闭所有主机的selinux、firewalld setenforce 0 sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config systemctl stop firewalld && systemctl disable firewalld 4、安装docker 所有节点 yum install -y epel-release yum install -y yum-utils net-tools conntrack-tools wget yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum install -y docker-ce-18.06.1.ce 启动 systemctl start docker systemctl enable docker 配置加速 sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json 5、初始化docker用户 所有节点 useradd -m docker && echo \"1qaz@WSX\" | passwd --stdin docker 6、安装节点配置互信 ssh-keygen -t rsa -b 2048 -N '' -f ~/.ssh/id_rsa ssh-copy-id docker@节点地址 7、创建集群配置文件 cat > rancher-cluster.yml 8.下载镜像 可提前下载好所需镜像 docker pull rancher/rke-tools:v0.1.50 docker pull rancher/hyperkube:v1.15.5-rancher1 docker pull rancher/coreos-etcd:v3.3.10-rancher1 9、构建集群 rke up --config ./rancher-cluster.yml 10、查看结果 11、配置 主节点 mkdir ~/.kube cat kube_config_rancher-cluster.yml > ~/.kube/config 12、安装kubectl等 cat > /etc/yum.repos.d/kubernetes.repo 主机点安装Kubectl即可 yum -y install kubectl 安装自动补全 yum install -y bash-completion source /usr/share/bash-completion/bash_completion source > ~/.bashrc 13、查看节点信息 kubectl get node 14、发布应用测试 cat >> nginx.yaml 创建 kubectl apply -f nginx.yaml 查看状态 kubectl get pod -o wide 查看svc，测试 kubectl get svc 删除 kubectl delete deploy --all kubectl delete svc/nginx-service Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/install/kubeadm-offline.html":{"url":"2.容器/k8s/install/kubeadm-offline.html","title":"kubeadm-offline","keywords":"","body":" Table of Contents generated with DocToc 离线安装 配置k8s源 cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum clean all && yum makecache 安装yum插件 yum install yum-plugin-downloadonly -y 导出依赖 yum install --downloadonly --downloaddir=/root/k8s kubelet-1.15.5 kubeadm-1.15.5 kubectl-1.15.5 安装kububeadm yum install -y kubelet-1.15.5 kubeadm-1.15.5 kubectl-1.15.5 --disableexcludes=kubernetes 下载k8s相关镜像 #k8s.gcr.io被墙，需要走微软代理地址 for i in `kubeadm config images list 2>/dev/null |sed 's/k8s.gcr.io\\///g'`; do docker pull gcr.azk8s.cn/google-containers/${i} docker tag gcr.azk8s.cn/google-containers/${i} k8s.gcr.io/${i} docker rmi gcr.azk8s.cn/google-containers/${i} done 离线下载镜像 docker save $(docker images | grep -v REPOSITORY | awk 'BEGIN{OFS=\":\";ORS=\" \"}{print $1,$2}') -o /root/k8s/k8s-master.tar 打包资源包 tar zcvf k8s-1.15.5.tar.gz k8s/ 离线安装 离线安装docker 上传k8s资源包，解压 tar zxvf k8s-*.tar.gz 导入镜像 docker load -i k8s/k8s-master.tar 安装kubeadm rpm -ivh k8s/*.rpm --nodeps --force 修改时区 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 关闭防火墙 systemctl stop firewalld && systemctl disable firewalld 关闭swap swapoff -a sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab sysctl -p 设置内核参数 modprobe br_netfilter cat 修改Linux 资源配置文件,调高ulimit最大打开数和systemctl管理的服务文件最大打开数 echo \"* soft nofile 655360\" >> /etc/security/limits.conf echo \"* hard nofile 655360\" >> /etc/security/limits.conf echo \"* soft nproc 655360\" >> /etc/security/limits.conf echo \"* hard nproc 655360\" >> /etc/security/limits.conf echo \"* soft memlock unlimited\" >> /etc/security/limits.conf echo \"* hard memlock unlimited\" >> /etc/security/limits.conf echo \"DefaultLimitNOFILE=1024000\" >> /etc/systemd/system.conf echo \"DefaultLimitNPROC=1024000\" >> /etc/systemd/system.conf 修改kubelet配置 sed -i \"s;KUBELET_EXTRA_ARGS=;KUBELET_EXTRA_ARGS=\\\"--fail-swap-on=false\\\";g\" /etc/sysconfig/kubelet 配置hostname 172.16.145.140主要替换为实际IP hostnamectl --static set-hostname master echo \"172.16.145.140 master\" >> /etc/hosts 配置kubelet自启动 systemctl enable kubelet 初始化集群 k8sversion=`kubeadm version -o yaml|grep gitVersion|sed 's#gitVersion:##g'|sed 's/ //g'` echo \"k8s version: $k8sversion\" kubeadm init --kubernetes-version=$k8sversion --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap 配置授权 mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 下载flannel.yml 下载以下镜像并导入 quay.io/coreos/flannel:v0.11.0-amd64 删除主节点污点 kubectl taint nodes --all node-role.kubernetes.io/master- 安装网络 kubectl apply -f kube-flannel.yml Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/issue/issue.html":{"url":"2.容器/k8s/issue/issue.html","title":"issue","keywords":"","body":" cgroups导致的内存泄露 网络 calico 1.异常: calico/node is not ready: BIRD is not ready: BGP not established with x.x.x.x 此时状态为: 运行中但未READY [root@k8s-master ~]# kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-578894d4cd-gxfw2 1/1 Running 0 82m kube-system calico-node-jzjj5 0/1 Running 0 15s kube-system calico-node-vwfrr 0/1 Running 0 15s 解决方式 kubectl edit ds -n kube-system calico-node spec.template.spec.containers下新增如下环境变量（注意缩进不能用tab） - name: IP_AUTODETECTION_METHOD value: interface=ens.* Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/monitor/":{"url":"2.容器/k8s/monitor/","title":"monitor","keywords":"","body":" weavescpoe Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/monitor/weavescope.html":{"url":"2.容器/k8s/monitor/weavescope.html","title":"weavescope","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:03 "},"2.容器/k8s/network/":{"url":"2.容器/k8s/network/","title":"network","keywords":"","body":" cni calico dns Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/network/calico.html":{"url":"2.容器/k8s/network/calico.html","title":"calico","keywords":"","body":"Calico介绍 官方文档地址 Calico是什么? Calico是一个面向容器、虚拟机、基于原生主机的工作负载的开源网络和网络安全解决方案。 Calico支持广泛的平台，包括Kubernetes、OpenShift、Docker EE、OpenStack和裸金属服务器。 Calico的eBPF数据平面与Linux的标准网络管道，提供了惊人的性能和真正的云原生可伸缩性。 为什么使用Calico? 可选的数据面 可选性更多：Calico为您提供了一个数据面选择，包括一个纯Linux eBPF数据面、一个标准Linux网络数据面和一个Windows HNS数据面。 无论哪种选择适合您，您都将获得相同的、易于使用的基础网络、网络策略和IP地址管理功能，这些功能使Calico成为云原生应用程序最值得信赖的网络和网络策略解决方案。 网络安全的最佳实践 Calico丰富的网络策略模型可以很容易地锁定通信，加上内置的支持Wireguard加密，确保您的pod间的流量安全可靠 Calico的策略引擎可以在主机网络层和在服务网格层(如Istio & Envoy)执行相同的策略模型，保护基础设施不受损害的工作负载影响，保护工作负载不受损害的基础设施的影响。 优异的性能 根据您的偏好，Calico使用Linux eBPF或Linux内核高度优化的标准网络管道来提供高性能的网络。 Calico的网络选项足够灵活，可以在大多数环境中不使用覆盖运行，避免了封装encap/decap的开销。 Calico的控制平面和策略引擎在多年的生产使用中经过优化，使得CPU使用和占用率较低。 可伸缩性 Calico基于云原生设计模式，并结合了世界上最大的互联网运营商所信任的经过验证的基于标准的网络协议。 其结果是一个具有特殊可伸缩性的解决方案，多年来一直在大规模生产中运行。Calico的开发测试周期包括定期测试数千个节点集群，保证了优异的性能和可伸缩性。 兼容性 Calico使Kubernetes工作负载和非Kubernetes工作负载能够无缝、安全地通信。Kubernetes pods是您网络上的一等公民，能够与网络上的任何其他工作负载进行通信。 此外，Calico还可以保护现有的基于主机的工作负载（无论是在公共云中还是在VMs或裸机服务器上）。所有工作负载都受相同的网络策略模型的约束，因此唯一允许流动的流量是您期望流动的流量。 经过生产环境验证 Calico深受包括SaaS提供商、金融服务公司和制造商在内的大型企业的信任和生产。 最大的公共云提供商已经选择Calico为其托管的Kubernetes服务(Amazon EKS、Azure AKS、谷歌GKE和IBM IKS)提供网络安全性，这些服务运行在数万个集群中 完全支持Kubernetes网络策略 在开发API期间，Calico的网络策略引擎参考Kubernetes原生网络策略的实现。 Calico的区别在于它实现了API定义的全部特性，为用户提供了定义API时所设想的所有功能和灵活性。 对于那些需要更强大功能的用户，Calico支持一组扩展的网络策略功能，这些功能与Kubernetes API无缝协作，为用户在定义网络策略方面提供了更大的灵活性。 Calico实践 Calico网络选型 Kubernetes网络基础知识 Kubernetes网络模型定义了一个\"扁平\"网络，其中: 每个Pod拥有自己的IP地址 任何节点上的pod都可以在没有NAT的情况下与所有其他节点上的pod通信 这创建了一个干净、向后兼容的模型，从端口分配、命名、服务发现、负载平衡、应用程序配置和迁移的角度来看， pod可以像vm或物理主机一样对待。可以使用网络策略来定义网络分段，以限制这些基本网络功能内的通信。 在这个模型中，对于支持不同的网络方法和环境有很大的灵活性。网络实现的具体细节取决于使用的CNI、网络和云提供商插件的组合 CNI插件 CNI(Container Network Interface容器网络接口)是一个标准的API，允许不同的网络实现接入到Kubernetes中。 Kubernetes在创建或销毁pod时调用API。CNI插件有两种类型: CNI网络插件：负责向Kubernetes pod网络添加或删除pod。这包括创建/删除每个pod的网络接口，并将其连接/断开与其余网络实现的连接。 CNI IPAM插件：负责在pod被创建或删除时为pod分配和释放IP地址。根据插件的不同，这可能包括为每个节点分配一个或多个IP地址范围（CIDR），或者从底层公共云的网络获取IP地址以分配给POD。 Overlay网络 在底层网络不知道连接到overlay覆盖网络的设备的情况下，overlay覆盖网络允许网络设备通过底层网络进行通信。 从连接到overlay覆盖网络的设备的角度来看，它看起来就像一个普通的网络。 有许多不同类型的overlay覆盖网络，它们使用不同的协议来实现这一点，但通常它们都有一个共同的特征，取一个网络包，称为内部包，并将它封装在一个外部网络包中。通过这种方式，底层可以看到外部包，而不需要了解如何处理内部包。 overlay如何知道将包发送到哪里取决于overlay的类型和它们使用的协议。 同样，数据包的包装方式在不同的覆盖类型之间也有所不同。以VXLAN为例，内部报文被封装，外部报文以UDP协议发送。 overlay网络的优点是对底层网络基础设施的依赖性最小，但有以下缺点: 如果运行网络密集型工作负载，与非覆盖网络相比，性能影响较小， overlay上的工作负载不容易从网络的其余部分寻址。因此，NAT网关或负载均衡器需要桥接overlay和底层网络。 Calico网络选项是非常灵活的，所以通常你可以选择是否你喜欢Calico提供覆盖网络，或非覆盖网络。 你可以在Calico确定最佳网络选项指南中了解更多。 跨子网Overlay网络 除了标准的VXLAN或IP-In-IP覆盖，Calico还支持VXLAN和IP-In-IP的\"跨子网\"模式。在这种模式下，在每个子网中，底层网络充当L2网络。 在单个子网内发送的数据包不会被封装，因此可以获得非覆盖网络的性能。通过子网发送的数据包被封装，就像普通的覆盖网络一样，减少了对底层网络的依赖性（无需与底层网络集成或对底层网络进行任何更改） 与标准覆盖网络一样，底层网络不知道pod IP地址，pod IP地址不能在集群外部路由。 集群外部的Pod IP可路由性 不同Kubernetes网络实现的一个重要区别是pod IP地址是否可以通过更广泛的网络在集群之外路由 不可路由 如果Pod IP地址集群外不可路由,当一个Pod试图建立一个网络连接集群外的一个IP地址, Kubernetes使用了一种叫做SNAT(来源网络地址转换)来改变源IP地址的IP地址, IP地址的节点承载Pod。连接上的任何返回包都会自动映射回pod IP地址。 因此，pod对SNAT无感知，连接的目的地将节点视为连接的源，集群之外的网络永远不会看到pod IP地址。 对于相反方向的连接，集群外部需要连接到pod情况下，这只能通过Kubernetes services或Kubernetes ingress来实现。 集群之外不能直接连接到pod IP地址，因为集群之外的的网络不知道如何将数据包路由到pod IP地址 可路由 如果pod IP地址可在集群外部路由，那么pod可以连接到外部世界而无需SNAT， 并且外部世界可以直接连接到pod而无需通过Kubernetes服务或Kubernetes入口。 集群外部可路由pod IP地址的优点是： 避免出站连接SNAT，简化操作日志的调试和理解 pod需要直接访问，而无需通过Kubernetes服务或Kubernetes入口，那么可路由pod ip在操作上可能比使用主机网络pod更简单 在集群外部可路由的pod IP地址的主要缺点是，pod IP在更广泛的网络中必须是唯一的。 例如，如果运行多个集群，则需要为每个集群中的pods使用不同的IP地址范围(CIDR)。 在大规模运行时，或者在现有企业对IP地址空间有其他重大需求时，这会导致IP地址范围耗尽。 BGP BGP（Border Gateway Protocol）是一种基于标准的网络协议，用于在网络上共享路由。它是互联网的基本组成部分之一，具有非凡的扩展特性。 Calico内置了对BGP的支持。在on-prem部署中，这允许Calico与物理网络（通常是顶部或机架路由器）对等以交换路由，从而形成一个非覆盖网络，其中pod IP地址可在更广泛的网络上路由，就像连接到网络的任何其他工作负载一样。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/network/dns.html":{"url":"2.容器/k8s/network/dns.html","title":"dns","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/opterator/如何写一个operator.html":{"url":"2.容器/k8s/opterator/如何写一个operator.html","title":"如何写一个operator","keywords":"","body":"如何写一个operator 文章源地址 writing-a-controller-for-pod-labels k8s中的operator是什么？ operator旨在简化基于k8s部署有状态服务（例如：ceph集群） 可以利用Operator SDK 构建一个operator， operator使扩展k8s及实现自定义调度变得更加简单。 尽管Operator SDK 适合构建功能齐全的operator， 但也可以使用它来编写单个控制器。 这篇文章将指导您在Go中编写一个Kubernetes控制器，该控制器将向具有特定注释的pod添加pod-name标签 为什么我们需要一个控制器呢？ 最近我们项目中有这么个需求：通过一个service将流量路由至同一ReplicaSet中指定pod内（service对应一个或多个pod） 而原生k8s并不能实现该功能，因为原生service只能通过label与Pod匹配，并且同一ReplicaSet内，Pod具有相同标签。 上述需求有两种解决方案: 创建service时不指定标签选择器，而是利用Endpoints或EndpointSlices关联pod 此时我们需要写一个自定义控制器，用于插入指定pod的端点地址至Endpoints或EndpointSlices对象 为每个Pod添加具有唯一value的标签，接下来我们就可以利用标签选择器进行service与Pod的关联。 由于k8s中的控制器实质是个控制循环程序，控制器可以对k8s的资源（Resource，比如namespace、service等）进行监听追踪。 此时如果我们创建一个控制器，仅监听Pod资源，针对指定Pod进行label处理，就可实现上述需求。 当然k8s原生资源StatefulSets也是可以实现这一功能的，但假设我们不想/不能使用StatefulSets类型去实现呢？ 一般情况下，我们很少直接创建Pod类型，而是通过Deployment, ReplicaSet间接创建Pod。 我们可以指定标签添加到PodSpec中的每个Pod，但不能使用动态值，因此无法复制StatefulSet的pod-name标签。 我们尝试使用mutating admission webhook 实现。 当任何人创建Pod时，webhook会自动注入一个包含Pod名称的标签对Pod进行修改。 遗憾的是这种方式并不能实现我们的需求： 并不是所有的Pod在创建前都有名字。 举个例子：当ReplicaSet控制器创建一个Pod时，他向kube-apiserver发送一个请求，获取一个namePrefix而非name kubeapi-server在将新的Pod持久化到etcd之前生成一个唯一的名称， 这个过程发生于在调用我们的许可webhook之后。所以在大多数情况下，我们无法知道一个带有mutating webhook的Pod的名字 一旦Pod持久化至K8s集群中时，它几乎不会发生变更，但我们仍然可以通过以下方式，添加label kubectl label my-pod my-label-key=my-label-value 我们需要观察Kubernetes API中任何Pod的变化，并添加我们想要的标签。 我们将编写一个控制器来为我们做这件事，而不是手动做这件事 利用Operator SDK构建一个控制器 控制器是一个协调循环，它从Kubernetes API中读取期望的资源状态，并采取行动使集群的实际状态达到期望状态 安装Operator SDK 下载二进制 sudo curl -LO https://github.com/operator-framework/operator-sdk/releases/download/v1.12.0/operator-sdk_linux_amd64 sudo mv operator-sdk_linux_amd64 /usr/local/bin/operator-sdk 构建工程 mkdir label-operator && cd label-operator 初始化工程 export GOPROXY=https://goproxy.cn operator-sdk init --domain=weiliang.io --repo=github.com/weiliang-ms/label-operator 创建控制器 接下来我们创建一个控制器，这个控制器将会处理Pod资源，而非自定义资源，所以不需要生成资源代码。 operator-sdk create api --group=core --version=v1 --kind=Pod --controller=true --resource=false We now have a new file: controllers/pod_controller.go. This file contains a PodReconciler type with two methods that we need to implement. The first is Reconcile, and it looks like this for now: 现在我们拥有了一个新文件: controllers/pod_controller.go。 该文件包含了PodReconciler类型，该类型包含两个方法： Reconcile方法： func (r *PodReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { _ = log.FromContext(ctx) // your logic here return ctrl.Result{}, nil } The Reconcile method is called whenever a Pod is created, updated, or deleted. 当创建、更新、或删除Pod时会调用Reconcile方法，Pod The name and namespace of the Pod are in the ctrl.Request the method receives as a parameter. Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-20 00:42:17 "},"2.容器/k8s/quota/quota.html":{"url":"2.容器/k8s/quota/quota.html","title":"quota","keywords":"","body":" 配额 相关概念 配额限制维度 节点级计算资源限制 命名空间级计算资源限制 命名空间容器默认配额设置 容器级计算资源限制 参考文档 配额 相关概念 request与limits Kubernetes采用request和limit两种限制类型来对资源进行分配 request(资源需求)：即运行Pod的节点必须满足运行Pod的最基本需求才能运行Pod limits(资源限额)：描述Pod运行期间，内存最大可申请大小 一个容器申请0.5个CPU，就相当于申请1个CPU的一半，加个后缀m表示千分之一的概念。 比如说100m的CPU，表示0.1个cpu 配额与Pod优先级关系 Request=Limit: Pod类型为Guaranted（保证型），只有内存使用量超限（OOM）时才会被kill Request: Pod类型为Burstable(突发流量型)，节点计算资源不足时，可能会被kill回收 未设置Request Limit: Pod类型为Best Effort（尽最大努力型），节点计算资源不足时，会被首先kill 配额限制维度 节点级计算资源限制 Kubelet Node Allocatable用来为Kube组件和System进程预留资源， 从而保证当节点出现满负荷时也能保证k8s系统服务和System宿主机守护进程有足够的资源 Node Capacity: Node的所有硬件资源 kube-reserved: kube组件预留的资源 system-reserved: System进程预留的资源 eviction-threshold（阈值）: kubelet eviction(回收)的阈值设定 allocatable: 真正scheduler调度Pod时的参考值（保证Node上所有Pods的request resource不超过Allocatable） 查看当前节点的Capacity和Allocatable [root@node1 ~]# kubectl describe node node1 ... Capacity: cpu: 4 ephemeral-storage: 17394Mi hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 4004944Ki pods: 300 Allocatable: cpu: 3600m ephemeral-storage: 17394Mi hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 3371721521 pods: 300 ... 查看docker驱动 cgroup驱动如果为systemd，则开启不了Kubelet Node Allocatable [root@node1 ~]# docker info | grep \"Cgroup Driver\" Cgroup Driver: systemd 调整docker驱动为cgroupfs 调整/etc/docker/daemon.json内容，添加/修改以下值（需升级内核） \"exec-opts\": [\"native.cgroupdriver=cgroupfs\"] 重启docker systemctl daemon-reload systemctl restart docker 调整kubelet参数配置 修改/var/lib/kubelet/kubeadm-flags.env，调整/增加以下参数： # 修改`kubelet cgroup`驱动`systemd`为`cgroupfs` --cgroup-driver=cgroupfs # 开启为kube组件和系统守护进程预留资源的功能 --enforce-node-allocatable=pods,kube-reserved,system-reserved # 设置k8s组件的cgroup --kube-reserved-cgroup=/system.slice/kubelet.service # 设置系统守护进程的cgroup --system-reserved-cgroup=/system.slice # 配置为k8s组件预留资源的大小，CPU、MEM --kube-reserved=cpu=1,memory=1Gi # 配置为系统进程（诸如 sshd、udev 等系统守护进程）预留资源的大小，CPU、MEM --system-reserved=cpu=0.5,memory=1Gi # 驱逐pod的配置：硬阈值（保证95%的内存利用率) --eviction-hard=memory.available 调整kubelet.service 调整文件/etc/systemd/system/kubelet.service 修改前 [Unit] Description=kubelet: The Kubernetes Node Agent Documentation=http://kubernetes.io/docs/ [Service] ExecStart=/usr/local/bin/kubelet Restart=always StartLimitInterval=0 RestartSec=10 [Install] WantedBy=multi-user.target 修改后 [Unit] Description=kubelet: The Kubernetes Node Agent Documentation=http://kubernetes.io/docs/ [Service] ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/cpuset/system.slice/kubelet.service ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/hugetlb/system.slice/kubelet.service ExecStart=/usr/local/bin/kubelet Restart=always StartLimitInterval=0 RestartSec=10 [Install] WantedBy=multi-user.target 重启kubelet再次查看节点的Capacity和Allocatable [root@node1 ~]# systemctl daemon-reload [root@node1 ~]# systemctl restart kubelet [root@node1 ~]# kubectl describe node node1 ... Capacity: cpu: 4 ephemeral-storage: 17394Mi hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 4004944Ki pods: 300 Allocatable: cpu: 2500m ephemeral-storage: 14267554175 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 1748525873 pods: 300 ... 官方的样例说明 这是一个用于说明节点可分配（Node Allocatable）计算方式的示例： 节点拥有32Gi memeory，16 CPU和100Gi Storage资源: --kube-reserved被设置为cpu=1,memory=2Gi,ephemeral-storage=1Gi --system-reserved被设置为cpu=500m,memory=1Gi,ephemeral-storage=1Gi --eviction-hard被设置为memory.available 在这个场景下，Allocatable将会是14.5 CPUs、28.5Gi内存以及88Gi本地存储。 调度器保证这个节点上的所有Pod的内存requests总量不超过28.5Gi，存储不超过88Gi。 当Pod的内存使用总量超过28.5Gi或者磁盘使用总量超过88Gi时， kubelet将会驱逐它们。 如果节点上的所有进程都尽可能多地使用CPU，则Pod加起来不能使用超过14.5 CPUs的资源。 当没有执行kube-reserved和/或system-reserved策略且系统守护进程 使用量超过其预留时， 如果节点内存用量高于31.5Gi或存储大于90Gi，kubelet将会驱逐Pod 命名空间级计算资源限制 设置限定对象数据的资源配额 指定命名空间test01生效 cat 设置限定计算资源配额限制 指定命名空间test01生效 cat 命名空间容器默认配额设置 缺省值 创建测试命名空间 kubectl create ns test01 创建命名空间容器默认配额设置 cat 容器如果未声明request与limits -> 会根据命名空间下LimitRange策略对容器配额赋值 容器如果声明limits未声明request -> 则容器的内存request和limits值一致 容器如果声明request,未声明limits -> 容器request值被设置为声明的值，limits被设置成了LimitRange值 容器级计算资源限制 针对业务容器设置配额 apiVersion: v1 kind: Pod metadata: name: frontend spec: containers: - name: db image: mysql env: - name: MYSQL_ROOT_PASSWORD value: \"password\" resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" - name: wp image: wordpress resources: requests: memory: \"64M\" cpu: \"0.25\" limits: memory: \"128M\" cpu: \"0.5\" 参考文档 k8s 节点可分配资源限制 Node Allocatable k8s官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/security/security.html":{"url":"2.容器/k8s/security/security.html","title":"security","keywords":"","body":"云原生安全 k8s安全 本概述定义了一个模型，用于在Cloud Native安全性上下文中考虑Kubernetes安全性。 云原生安全的4个C 你可以分层去考虑安全性，云原生安全的4个C分别是云（Cloud）、集群（Cluster）、容器（Container）和代码（Code） 4C中的云 在许多方面，云（或者位于同一位置的服务器，或者是公司数据中心）是Kubernetes集群中的可信计算基础。 如果云层容易受到攻击（或者被配置成了易受攻击的方式），就不能保证在此基础之上构建的组件是安全的。 每个云提供商都会提出安全建议，以在其环境中安全地运行工作负载。 云提供商安全性 下面是一些比较流行的云提供商的安全性文档链接： Alibaba Cloud Amazon Web Services 基础设施安全 通过网络访问API服务（控制平面） 所有对Kubernetes控制平面的访问不允许在Internet上公开， 同时应由网络访问控制列表控制，该列表包含管理集群所需的IP地址集 通过网络访问Node（节点） 节点应配置为仅能从控制平面上通过指定端口来接受（通过网络访问控制列表）连接， 以及接受NodePort和LoadBalancer类型的Kubernetes服务连接。 如果可能的话，这些节点不应完全暴露在公共互联网上。 Kubernetes访问云提供商的API 每个云提供商都需要向Kubernetes控制平面和节点授予不同的权限集。 为集群提供云提供商访问权限时，最好遵循对需要管理的资源的最小特权原则。 访问etcd 对etcd（Kubernetes的数据存储）的访问应仅限于控制平面。根据配置情况，你应该尝试通过TLS来使用etcd etcd加密 在所有可能的情况下，最好对所有驱动器进行静态数据加密， 但是由于etcd拥有整个集群的状态（包括机密信息），因此其磁盘更应该进行静态数据加密。 集群 保护Kubernetes有两个方面需要注意： 保护可配置的集群组件 保护在集群中运行的应用程序 控制对Kubernetes API的访问 因为Kubernetes是完全通过API驱动的，所以，控制和限制谁可以通过API访问集群， 以及允许这些访问者执行什么样的API动作，就成为了安全控制的第一道防线。 为所有API交互使用传输层安全 （TLS） Kubernetes期望集群中所有的API通信在默认情况下都使用TLS加密，大多数安装方法也允许创建所需的证书并且分发到集群组件中。 请注意，某些组件和安装方法可能使用HTTP来访问本地端口， 管理员应该熟悉每个组件的设置，以识别潜在的不安全的流量 API认证 安装集群时，选择一个API服务器的身份验证机制，去使用与之匹配的公共访问模式。 例如，小型的单用户集群可能希望使用简单的证书或静态承载令牌方法。 更大的集群则可能希望整合现有的、OIDC、LDAP等允许用户分组的服务器。 所有API客户端都必须经过身份验证，即使它是基础设施的一部分，比如节点、代理、调度程序和卷插件。 这些客户端通常使用服务帐户或X509客户端证书，并在集群启动时自动创建或是作为集群安装的一部分进行设置 API授权 一旦通过身份认证，每个API的调用都将通过鉴权检查。 Kubernetes集成基于角色的访问控制（RBAC）组件，将传入的用户或组与一组绑定到角色的权限匹配。 这些权限将动作（get，create，delete）和资源（pod，service, node）在命名空间或者集群范围内结合起来， 根据客户可能希望执行的操作，提供了一组提供合理的违约责任分离的外包角色。 建议你将节点和RBAC一起作为授权者，再与NodeRestriction准入插件结合使用。 与身份验证一样，简单而广泛的角色可能适合于较小的集群，但是随着更多的用户与集群交互，可能需要将团队划分成有更多角色限制的单独的命名空间。 就鉴权而言，理解怎么样更新一个对象可能导致在其它地方的发生什么样的行为是非常重要的。 例如，用户可能不能直接创建Pod，但允许他们通过创建一个Deployment来创建这些Pod， 这将让他们间接创建这些Pod。同样地，从API删除一个节点将导致调度到这些节点上的Pod被中止， 并在其他节点上重新创建。原生的角色设计代表了灵活性和常见用例之间的平衡，但有限制的角色应该仔细审查， 以防止意外升级。如果外包角色不满足你的需求，则可以为用例指定特定的角色 控制对Kubelet的访问 Kubelet公开HTTPS端点，这些端点授予节点和容器强大的控制权。 默认情况下，Kubelet允许对此API进行未经身份验证的访问。 生产级别的集群应启用Kubelet身份验证和授权。 Kubelet 身份认证 要禁用匿名访问并向未经身份认证的请求发送401 Unauthorized响应，请执行以下操作： 带--anonymous-auth=false标志启动kubelet 要对kubelet的HTTPS端点启用X509客户端证书认证: 带--client-ca-file标志启动kubelet，提供一个CA证书包以供验证客户端证书 带--kubelet-client-certificate和--kubelet-client-key标志启动apiserver 要启用API持有者令牌（包括服务帐户令牌）以对kubelet的HTTPS端点进行身份验证，请执行以下操作： 确保在API服务器中启用了authentication.k8s.io/v1beta1 API组 带--authentication-token-webhook和--kubeconfig标志启动kubelet kubelet调用已配置的API服务器上的TokenReview API，以根据持有者令牌确定用户信息 集群中的组件（自定义应用）安全性 RBAC 授权(访问 Kubernetes API) 认证方式 应用程序 Secret 管理 etcd静态数据加密 Pod安全策略 服务质量（和集群资源管理） 网络策略 Kubernetes Ingress 的 TLS 支持 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/sidecar/sidecar.html":{"url":"2.容器/k8s/sidecar/sidecar.html","title":"sidecar","keywords":"","body":"SideCar模式 SideCar中文译为边车，是附着在摩托车旁的小型车辆，用于载客。 在编程世界中，其主要功能是将主应用与外围辅助服务进行解耦，提供更灵活的应用部署方式。 其理念符合设计模式中的单一职责原则，让主应用和辅助服务分离，更专注自身功能。 使用场景-共享存储 基于K8S Pod特性，同一个Pod可以共享根容器中挂载的Volume。基于该特性，我们可以想到以下SideCar应用方式： 日志收集上传 我们可以应用日志挂载到共享的Volume上，业务容器写日志，SideCar容器读日志，并上传日志分析平台，以生产者消费者方式进行解耦。 应用Jar包挂载 因为Java应用需要依赖拥有Java运行环境，因此大多使用open-jdk等镜像作为基础镜像。 而这类镜像大多上百M。通过共享存储，我们可以利用busybox这类体积只有几M的镜像作为基础镜像，然后将jar包拷贝到共享Volume下。 并将这个承载jar的镜像作为InitContainer，主业务容器使用该共享Volume下的jar包启动业务。 后续应用版本更新，只需要更新jar包镜像。这个jar包镜像便是一个SideCar。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/storage/":{"url":"2.容器/k8s/storage/","title":"storage","keywords":"","body":" OpenEBS rook Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/storage/OpenEBS.html":{"url":"2.容器/k8s/storage/OpenEBS.html","title":"OpenEBS","keywords":"","body":" Table of Contents generated with DocToc OpenEBS 简介 OpenEBS是什么？ OpenEBS能做什么？ 对比传统分布式存储 OpenEBS存储引擎建议 OpenEBS特性 CAS介绍 OpenESB架构介绍 控制面 数据面 CAS引擎 存储引擎概述 存储引擎类型 存储引擎声明 CAS引擎使用场景 节点磁盘管理器（NDM） 落地实践 Local PV Hostpath实践 Local PV Device实践 总结 OpenEBS 简介 OpenEBS是什么？ OpenEBS是一种开源云原生存储解决方案，托管于CNCF基金会，目前该项目处于沙箱阶段， OpenEBS是一组存储引擎，允许您为有状态工作负载(StatefulSet)和Kubernetes平台类型选择正确的存储解决方案。 在高层次上，OpenEBS支持两大类卷——本地卷和复制卷 OpenEBS是Kubernetes本地超融合存储解决方案，它管理节点可用的本地存储，并为有状态工作负载提供本地或高可用的分布式持久卷。 作为一个完全的Kubernetes原生解决方案的另一个优势是，管理员和开发人员可以使用kubectl、Helm、 Prometheus、Grafana、Weave Scope等Kubernetes可用的所有优秀工具来交互和管理OpenEBS OpenEBS能做什么？ OpenEBS管理k8s节点上存储，并为k8s有状态负载（StatefulSet）提供本地存储卷或分布式存储卷。 本地卷（Local Storage） OpenEBS可以使用宿主机裸块设备或分区，或者使用Hostpaths上的子目录，或者使用LVM、ZFS来创建持久化卷 本地卷直接挂载到Stateful Pod中，而不需要OpenEBS在数据路径中增加任何开销 OpenEBS为本地卷提供了额外的工具，用于监控、备份/恢复、灾难恢复、由ZFS或LVM支持的快照等 对于分布式卷(即复制卷) OpenEBS使用其中一个引擎(Mayastor、cStor或Jiva)为每个分布式持久卷创建微服务 有状态Pod将数据写入OpenEBS引擎，OpenEBS引擎将数据同步复制到集群中的多个节点。 OpenEBS引擎本身作为pod部署，并由Kubernetes进行协调。 当运行Stateful Pod的节点失败时，Pod将被重新调度到集群中的另一个节点，OpenEBS将使用其他节点上的可用数据副本提供对数据的访问 有状态的Pods使用iSCSI (cStor和Jiva)或NVMeoF (Mayastor)连接OpenEBS分布式持久卷 OpenEBS cStor和Jiva专注于存储的易用性和持久性。它们分别使用自定义版本的ZFS和Longhorn技术将数据写入存储。 OpenEBS Mayastor是最新开发的以耐久性和性能为设计目标的引擎，高效地管理计算(大页面、核心)和存储(NVMe Drives)，以提供快速分布式块存储 注意: OpenEBS分布式块卷被称为复制卷，以避免与传统的分布式块存储混淆，传统的分布式块存储倾向于将数据分布到集群中的许多节点上。 复制卷是为云原生有状态工作负载设计的，这些工作负载需要大量的卷，这些卷的容量通常可以从单个节点提供，而不是使用跨集群中的多个节点分片的单个大卷 对比传统分布式存储 OpenEBS与其他传统存储解决方案不同的几个关键方面: 使用微服务体系结构构建，就像它所服务的应用程序一样。 OpenEBS本身作为一组容器部署在Kubernetes工作节点上。使用Kubernetes本身来编排和管理OpenEBS组件 完全建立在用户空间，使其高度可移植性，以运行在任何操作系统/平台。 完全意图驱动，继承了Kubernetes易用性的相同原则 OpenEBS支持一系列存储引擎，因此开发人员可以部署适合于其应用程序设计目标的存储技术。 像Cassandra这样的分布式应用程序可以使用LocalPV引擎进行最低延迟的写操作。 像MySQL和PostgreSQL这样的单片应用程序可以使用使用NVMe和SPDK构建的Mayastor或基于ZFS的cStor来实现弹性。 像Kafka这样的流媒体应用程序可以在边缘环境中使用NVMe引擎Mayastor以获得最佳性能。 驱使用户使用OpenEBS的主要原因是: 在所有的Kubernetes发行版上都是可移植的 提高了开发人员和平台SRE的生产力 与其他解决方案相比，易于使用 优秀的社区支持 免费开源 本地卷类型 本地卷只能从集群中的单个节点访问。必须在提供卷的节点上调度使用Local Volume的Pods。 本地卷通常是分布式工作负载的首选，比如Cassandra、MongoDB、Elastic等，这些工作负载本质上是分布式的，并且内置了高可用性（分片） 根据附加到Kubernetes工作节点上的存储类型，您可以从不同的动态本地PV进行选择——Hostpath、Device、LVM、ZFS或Rawfile 可复制卷类型 复制卷顾名思义，是指将数据同步复制到多个节点的卷。卷可以支持节点故障。还可以跨可用性区域设置复制，以帮助应用程序跨可用性区域移动。 复制卷还能够提供像快照、克隆、卷扩展等企业存储特性。复制卷是有状态工作负载(如Percona/MySQL、Jira、GitLab等)的首选。 根据附加到Kubernetes工作节点的存储类型和应用程序性能需求，您可以从Jiva、cStor或Mayastor中进行选择 OpenEBS存储引擎建议 应用需求 存储类型 OpenEBS卷类型 低时延、高可用性、同步复制、快照、克隆、精简配置 SSD/云存储卷 OpenEBS Mayastor 高可用性、同步复制、快照、克隆、精简配置 机械/SSD/云存储卷 OpenEBS cStor 高可用性、同步复制、精简配置 主机路径或外部挂载存储 OpenEBS Jiva 低时延、本地PV 主机路径或外部挂载存储 Dynamic Local PV - Hostpath, Dynamic Local PV - Rawfile 低时延、本地PV 本地机械/SSD/云存储卷等块设备 Dynamic Local PV - Device 低延迟，本地PV，快照，克隆 本地机械/SSD/云存储卷等块设备 OpenEBS Dynamic Local PV - ZFS , OpenEBS Dynamic Local PV - LVM 总结： 多机环境，如果有额外的块设备（非系统盘块设备）作为数据盘，选用OpenEBS Mayastor、OpenEBS cStor 多机环境，如果没有额外的块设备（非系统盘块设备）作为数据盘，仅单块系统盘块设备，选用OpenEBS Jiva 单机环境，建议本地路径Dynamic Local PV - Hostpath, Dynamic Local PV - Rawfile，由于单机多用于测试环境，数据可靠性要求较低。 由此看来，OpenEBS常用场景为以上三个场景 OpenEBS特性 容器附加存储 OpenEBS是一个容器附加存储(Container Attached Storage, CAS)的例子。 通过OpenEBS提供的卷总是被容器化。每个卷都有一个专用的存储控制器，用于提高有状态应用程序的持久性存储操作的敏捷性和粒度。 同步复制 同步复制是OpenEBS的一个可选的流行特性。 当与Jiva、cStor和Mayastor存储引擎一起使用时，OpenEBS可以同步复制数据卷以实现高可用性。 跨Kubernetes区域进行复制，从而为跨AZ设置提供高可用性。 这个特性对于使用GKE、EKS和AKS等云提供商服务上的本地磁盘构建高可用状态应用程序特别有用 快照和克隆 写时拷贝快照是OpenEBS另一个可选的流行特性。 使用cStor引擎时，快照是瞬时创建的，并且不受快照个数的限制。 增量快照功能增强了跨Kubernetes集群和跨不同云提供商或数据中心的数据迁移和可移植性。 对快照和克隆的操作完全以Kubernetes原生方法执行，使用标准kubectl命令。 常见的用例包括用于备份的高效复制和用于故障排除或针对数据的只读副本进行开发的克隆 备份和恢复 OpenEBS卷的备份和恢复可以通过开源的OpenEBS Velero插件与Kubernetes备份和恢复解决方案(如Velero(前身为Heptio Ark))协同工作。 经常使用OpenEBS增量快照功能，将数据备份到AWS S3、GCP object storage、MinIO等对象存储目标。 这种存储级别的快照和备份只使用增量数据进行备份，节省了大量的带宽和存储空间。 真正的Kubernetes云原生存储 OpenEBS是Kubernetes上有状态应用程序的云原生存储，云原生意味着遵循松散耦合的体系结构。 因此，云原生、松散耦合体系结构的一般好处是适用的。 例如，开发人员和DevOps架构师可以使用标准的Kubernetes技能和实用程序来配置、使用和管理持久存储需求 减少存储TCO高达50% 在大多数云上，块存储的收费是基于购买的多少，而不是使用的多少; 为了实现更高的性能，并在充分利用容量时消除中断的风险，容量经常被过度配置。 OpenEBS的精简配置能力可以共享本地存储或云存储，然后根据需要增加有状态应用程序的数据量。 可以动态添加存储，而不会中断暴露给工作负载或应用程序的卷。 某些用户报告说，由于使用了OpenEBS的精简配置，节省了超过60%的资源。 高可用性 由于OpenEBS遵循CAS架构，在节点故障时，Kubernetes将重新调度OpenEBS控制器，而底层数据则通过使用一个或多个副本来保护。 更重要的是——因为每个工作负载都可以利用自己的OpenEBS——不存在因存储丢失而导致系统大范围宕机的风险。 例如，卷的元数据不是集中的，它可能会像许多共享存储系统那样受到灾难性的通用中断的影响。 相反，元数据保持在卷的本地。丢失任何节点都会导致只存在于该节点上的卷副本的丢失。 由于卷数据至少在其他两个节点上进行了同步复制，因此当一个节点出现故障时，这些数据将在相同的性能级别上继续可用 CAS介绍 在CAS或容器附加存储(Container Attached Storage)体系结构中，存储在容器中运行，并且与存储绑定到的应用程序密切相关。 存储作为微服务运行，没有内核模块依赖关系。 像Kubernetes这样的编排系统编排存储卷，就像任何其他微服务或容器一样。CAS具有DAS和NAS的优点 非CAS系统上的pv 在非CAS模型中，Kubernetes持久卷仍然与内核模块紧密耦合，使得Kubernetes节点上的存储软件本质上是单片的 基于CAS系统上的pv 相反，CAS使您能够利用云原生应用程序的灵活性和可伸缩性。 定义Kubernetes PV (Persistent Volume)的存储软件是基于微服务架构的。 存储软件的控制平面(存储控制器)和数据平面(存储副本)作为Kubernetes Pods运行，因此，使您能够将云原生的所有优势应用到CAS。 CAS优势 敏捷 CAS中的每个存储卷都有一个容器化的存储控制器和相应的容器化副本。 因此，围绕这些组件的资源的维护和调优是真正敏捷的。 Kubernetes滚动升级功能可以实现存储控制器和存储副本的无缝升级。可以使用容器cGroups调优CPU和内存等资源配额。 存储策略粒度化 将存储软件容器化并将存储控制器专用于每个卷可以带来最大的存储策略粒度。 在CAS体系结构中，可以按卷配置所有存储策略。 此外，您可以监视每个卷的存储参数，并动态更新存储策略，以实现每个工作负载的预期结果。 随着卷存储策略中这种额外粒度级别的增加，存储吞吐量、IOPS和延迟的控制也会增加。 云原生 CAS将存储软件装入容器，并使用Kubernetes自定义资源定义(CRDs)来声明低级存储资源，如磁盘和存储池。 这个模型使存储能够无缝地集成到其他云原生工具中。 可以使用Prometheus、Grafana、Fluentd、Weavescope、Jaeger等云原生工具来供应、监控和管理存储资源 PV是CAS中的一个微服务 如上图所示，在CAS架构中，存储控制器和副本的软件完全是基于微服务的，因此不涉及内核组件。 通常，存储控制器POD被调度在与持久卷相同的节点上，以提高效率，副本POD可以被调度在集群节点上的任何位置。 每个副本使用本地磁盘、SAN磁盘和云磁盘的任意组合完全独立于其他副本进行配置。 这为大规模管理工作负载的存储分配提供了巨大的灵活性。 超融合非分布式 CAS架构没有遵循典型分布式存储架构。通过从存储控制器到存储副本的同步复制，存储变得高度可用。 卷副本的元数据不是在节点之间共享的，而是在每个本地节点上独立管理。 如果一个节点故障，存储控制器(在本例中是一个无状态容器)将在一个节点上轮转，该节点上运行着第二个或第三个副本，数据仍然可用。 与超融合系统类似，CAS中的卷的存储和性能是可扩展的。由于每个卷都有自己的存储控制器，因此存储可以在一个节点的存储容量允许的范围内进行扩展。 在给定的Kubernetes集群中，随着容器应用程序数量的增加，会增加更多的节点，从而提高存储容量和性能的整体可用性，从而使存储对新的应用程序容器可用。 这一过程与Nutanix等成功的超融合系统非常相似。 OpenESB架构介绍 OpenESB遵循容器附加存储（CAS）模型，每个卷都有一个专用的控制器POD和一组副本POD。 CAS体系结构的优点在CNCF博客 上进行了讨论。 OpenEBS操作和使用都很简单，因为它看起来和感觉上就像其他云原生和Kubernetes友好的项目。 OpenEBS有许多组件，可以分为以下类别: 控制面组件 - Provisioner, API Server, volume exports,volume sidecars 数据面组件 - Jiva、cStor 节点磁盘管理器 - Discover, monitor, 管理连接k8s的媒介 与云原生工具的集成 - 已经与Prometheus,Grafana, Fluentd、Jaeger集成 控制面 OpenEBS集群的控制平面通常被称为Maya OpenEBS控制平面负责提供卷、相关的卷操作，如快照、克隆、创建存储策略、执行存储策略、导出Prometheus/grafana使用的卷指标，等等。 OpenEBS提供了一个动态提供程序，这是Kubernetes的标准外部存储插件。 OpenEBS PV提供者的主要任务是向应用程序PODS启动卷供应，并实现PV的Kubernetes规范。 m-apiserver开放存储的REST API，并承担大量卷策略处理和管理工作。 控制平面和数据平面之间的连通性使用Kubernetes sidecar模式。控制平面需要与数据平面通信的场景如下所示。 对于卷的统计，如IOPS，吞吐量，延迟等--通过卷暴漏的sidecar实现 使用卷控制器pod执行卷策略，使用卷副本pod进行磁盘/池管理-通过卷管理sidecar实现 OpenEBS PV Provisioner 此组件作为POD运行，并做出配置决策,它的使用方式是: 开发人员用所需的卷参数构造一个声明，选择适当的存储类，并在YAML规范上调用kubelet。 OpenEBS PV动态提供程序与maya-apiserver交互，在适当的节点上为卷控制器pod和卷副本pod创建部署规范。 可以使用PVC规范中的注释来控制卷Pod(控制器/副本)的调度。 目前，OpenEBS Provisioner只支持一种绑定类型，即iSCSI。 Maya-ApiServer m-apiserver作为POD运行。顾名思义，m-apiserver公开OpenEBS REST api m-apiserver还负责创建创建卷pod所需的部署规范文件。 在生成这些规范文件之后，它将调用kube-apiserver来相应地调度这些pods。 OpenEBS PV提供者在卷发放结束时，将创建一个PV对象并将其挂载到应用程序pod上。 PV由控制器pod承载，控制器pod由不同节点中的一组副本pod支持。 控制器pod和复制pod是数据平面的一部分，在存储引擎部分有更详细的描述。 m-apiserver的另一个重要任务是卷策略管理。OpenEBS为表示策略提供了非常细粒度的规范。 m-apiserver解释这些YAML规范，将它们转换为可执行的组件，并通过容量管理sidecar来执行它们 Maya Volume Exporter Maya卷导出器是每个存储控制器pod的sidecar 这些sidecar将控制平面连接到数据平面以获取统计信息。统计信息的粒度在卷级别。一些统计数据示例如下： 卷读取延迟 卷写入延迟 卷每秒读取速度 卷每秒写入速度 读取块大小 写入块大小 容量统计 这些统计信息通常由Prometheus客户端来拉取，该客户端在OpenBS安装期间安装和配置 卷管理sidecar Sidecars还用于将控制器配置参数和卷策略传递给卷控制器pod(卷控制器pod是一个数据平面)， 并将副本配置参数和副本数据保护参数传递给卷副本pod。 数据面 OpenEBS数据平面负责实际的卷IO路径。存储引擎在数据平面实现实际的IO路径。 目前，OpenEBS提供了两个可以轻松插入的存储引擎。它们被称为Jiva和cStor。 这两个存储引擎都完全运行在Linux用户空间中，并基于微服务架构。 Jiva Jiva存储引擎基于Rancher's LongHorn与gotgt开发实现， 使用go语言开发，并运行于用户命名空间下。 LongHorn控制器将输入的IO同步复制到LongHorn副本。该副本将Linux稀疏文件视为构建存储特性(如精简配置、快照、重建等)的基础。 cStor cStor数据引擎使用C语言编写，具有高性能的iSCSI target和Copy-On-Write块系统，提供数据完整性、数据弹性和时间点的快照和克隆。 cStor有一个池特性，它以条带、镜像或RAIDZ模式聚合一个节点上的磁盘，以提供更大的容量和性能单位。 cStor还可以跨区域将数据同步复制到多个节点，从而避免节点丢失或节点重启导致数据不可用。 LocalPV 对于那些不需要存储级复制的应用程序，LocalPV可能是很好的选择，因为它能提供更高的性能。 OpenEBS LocalPV与Kubernetes LocalPV类似，不同之处在于它是由OpenEBS控制平面动态提供的， 就像任何其他常规PV一样。OpenEBS LocalPV有两种类型:hostpath LocalPV和device LocalPV。 hostpath LocalPV指的是主机上的子目录，LocalPV指的是在节点上发现的磁盘(可以是直接连接的，也可以是网络连接的)。 OpenEBS引入了一个LocalPV提供者，用于根据PVC和存储类规范中的一些标准选择匹配的磁盘或主机路径。 节点磁盘管理器 节点磁盘管理器(NDM)填补了使用Kubernetes管理有状态应用程序的持久存储所需的工具链的空白。 容器时代的DevOps架构师必须以一种自动化的方式满足应用程序和应用程序开发人员的基础设施需求， 这种方式可以跨环境提供弹性和一致性。这些要求意味着存储堆栈本身必须非常灵活， 以便Kubernetes和云原生生态系统中的其他软件可以轻松地使用这个堆栈。 NDM在Kubernetes的存储堆栈中起着基础性的作用，它统一了不同的磁盘， 并通过将它们标识为Kubernetes对象，提供了将它们汇聚的能力。 此外，NDM发现、提供、监视和管理底层磁盘的方式，可以让Kubernetes PV提供者(如OpenEBS和其他存储系统)和Prometheus管理磁盘子系统 CAS引擎 存储引擎概述 存储引擎是持久化卷IO路径的数据平面组件。 在CAS架构中，用户可以根据不同的配置策略，为不同的应用工作负载选择不同的数据平面。 存储引擎可以通过特性集或性能优化给定的工作负载。 操作员或管理员通常选择具有特定软件版本的存储引擎，并构建优化的卷模板， 这些卷模板根据底层磁盘的类型、弹性、副本数量和参与Kubernetes集群的节点集进行微调。 用户可以在发放卷时选择最优的卷模板，从而在给定的Kubernetes集群上为所有存储卷运行最优的软件和存储组合提供最大的灵活性。 存储引擎类型 OpenEBS提供了三种存储引擎 Jiva Jiva是OpenEBS 0.1版中发布的第一个存储引擎，使用起来最简单。它基于GoLang开发，内部使用LongHorn和gotgt堆栈。 Jiva完全在用户空间中运行，并提供同步复制等标准块存储功能。 Jiva通常适用于容量较小的工作负载，不适用于大量快照和克隆特性是主要需求的情况 cStor cStor是OpenEBS 0.7版本中最新发布的存储引擎。cStor非常健壮，提供数据一致性，很好地支持快照和克隆等企业存储特性。 它还提供了一个健壮的存储池特性，用于在容量和性能方面进行全面的存储管理。 cStor与NDM (Node Disk Manager)一起，为Kubernetes上的有状态应用程序提供完整的持久化存储特性 OpenEBS Local PV OpenEBS Local PV是一个新的存储引擎，它可以从本地磁盘或工作节点上的主机路径创建持久卷或PV。 CAS引擎可以从OpenEBS的1.0.0版本中获得。使用OpenEBS Local PV，性能将等同于创建卷的本地磁盘或文件系统(主机路径)。 许多云原生应用程序可能不需要复制、快照或克隆等高级存储特性，因为它们本身就提供了这些特性。这类应用程序需要以持久卷的形式访问管理的磁盘 SP 存储池，表示Jiva自定义存储资源 CV cStor卷，表示cStor卷自定义资源 CVR cStor卷副本 SPC 存储池声明，表示cStor池聚合的自定义资源 CSP cStor存储池，表示cStor Pool每个节点上的自定义资源 一个SPC对应多个CSP，相应的一个CV对应多个CVR 存储引擎声明 通过指定注释openebs来选择存储引擎。 StorageClass规范中的io/cas-type。StorageClass定义了提供程序的细节。为每个CAS引擎指定单独的供应程序。 cStor存储类规范文件内容 --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: cStor-storageclass annotations: openebs.io/cas-type: cstor cas.openebs.io/config: | - name: StoragePoolClaim value: \"cStorPool-SSD\" provisioner: openebs.io/provisioner-iscsi --- Jiva存储类规范文件内容 --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: jiva-storageclass annotations: openebs.io/cas-type: jiva cas.openebs.io/config: | - name: StoragePool value: default provisioner: openebs.io/provisioner-iscsi --- 当cas类型为Jiva时，StoragePool的default值具有特殊含义。 当pool为默认值时，Jiva引擎将从容器(副本pod)本身的存储空间中为副本pod开辟数据存储空间。 当所需的卷大小很小(比如5G到10G)时，StoragePool default工作得很好，因为它可以容纳在容器本身内。 Local PV存储类规范文件内容-主机路径 --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: localpv-hostpath-sc annotations: openebs.io/cas-type: local cas.openebs.io/config: | - name: BasePath value: \"/var/openebs/local\" - name: StorageType value: \"hostpath\" provisioner: openebs.io/local --- Local PV存储类规范文件内容-主机设备 --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: localpv-device-sc annotations: openebs.io/cas-type: local cas.openebs.io/config: | - name: StorageType value: \"device\" - name: FSType value: ext4 provisioner: openebs.io/local --- cStor、Jiva、LocalPV特性比较： 特性 Jiva cStor Local PV 轻量级运行于用户空间 Yes Yes Yes 同步复制 Yes Yes No 适合低容量工作负载 Yes Yes Yes 支持快照，克隆 Basic Advanced No 数据一致性 Yes Yes NA 使用Velero恢复备份 Yes Yes Yes 适合高容量工作负载 No Yes Yes 自动精简配置 Yes No 磁盘池或聚合支持 Yes No 动态扩容 Yes Yes 数据弹性(RAID支持) Yes No 接近原生磁盘性能 No No Yes 大多数场景推荐cStor，因其提供了强大的功能，包括快照/克隆、存储池功能（如精简资源调配、按需扩容等）。 Jiva适用于低容量需求的工作负载场景，例如5到50G。 尽管使用Jiva没有空间限制，但建议将其用于低容量工作负载。 Jiva非常易于使用，并提供企业级容器本地存储，而不需要专用硬盘。 有快照和克隆功能的需求的场景，优先考虑使用cStor而不是Jiva。 CAS引擎使用场景 如上表所示，每个存储引擎都有自己的优势。 选择引擎完全取决于应用程序的工作负载以及它当前和未来的容量和/或性能增长。 下面的指导原则在定义存储类时为选择特定的引擎提供一些帮助。 选择cStor的理想条件 当需要同步复制数据并在节点上有多个磁盘时 当您从每个节点上的本地磁盘或网络磁盘池管理多个应用程序的存储时。 通过精简配置、存储池和卷的按需扩容、存储池的性能按需扩容等特性，实现对存储层的管理。 cStor用于在本地运行的Kubernetes集群上构建Kubernetes本地存储服务，类似于AWS EBS或谷歌PD。 当需要存储级快照和克隆能力时 当您需要企业级存储保护特性，如数据一致性、弹性(RAID保护)。 如果您的应用程序不需要存储级复制，那么使用OpenEBS主机路径LocalPV或OpenEBS设备LocalPV可能是更好的选择。 选择Jiva的理想条件: 当您想要数据的同步复制，并且拥有单个本地磁盘或单个管理磁盘(如云磁盘(EBS、GPD))，并且不需要快照或克隆特性时 Jiva是最容易管理的，因为磁盘管理或池管理不在这个引擎的范围内。Jiva池是本地磁盘、网络磁盘、虚拟磁盘或云磁盘的挂载路径。 以下场景Jiva更优于cStor: 当程序不需要存储级的快照、克隆特性 当节点上没有空闲磁盘时。Jiva可以在主机目录上使用，并且仍然可以实现复制。 当不需要动态扩展本地磁盘上的存储时。将更多磁盘添加到Jiva池是不可能的，因此Jiva池的大小是固定的，如果它在物理磁盘上。 但是，如果底层磁盘是虚拟磁盘、网络磁盘或云磁盘，则可以动态地更改Jiva池的大小 容量需求较小。大容量应用通常需要动态增加容量，cStor更适合这种需求 选择OpenEBS主机路径LocalPV的理想条件: 当应用程序本身具备管理复制能力（例如：es）时，不需要在存储层进行复制。在大多数这样的情况下，应用程序是作为statefulset部署的 高于Jiva与cStor的读写性能需求 当特定应用程序没有专用的本地磁盘或特定应用程序不需要专用的存储时，建议使用Hostpath。 如果您想跨多个应用程序共享一个本地磁盘，主机路径LocalPV是正确的方法 选择OpenEBS主机设备LocalPV的理想条件: 当应用程序管理复制本身，不需要在存储层进行复制时。在大多数这种情况下，应用程序被部署为有状态集 高于Jiva与cStor的读写性能需求 高于OpenEBS主机路径LocalPV的读写性能需求 当需要接近磁盘性能时。该卷专用于写入单个SSD或NVMe接口以获得最高性能 总结 如果应用程序处于生产中，并且不需要存储级复制，那么首选LocalPV 如果您的应用程序处于生产状态，并且需要存储级复制，那么首选cStor 如果应用程序较小，需要存储级复制，但不需要快照或克隆，则首选Jiva 节点磁盘管理器（NDM） 节点磁盘管理器(NDM)是OpenEBS体系结构中的一个重要组件。 NDM将块设备视为需要监视和管理的资源，就像CPU、内存和网络等其他资源一样。 它是一个在每个节点上运行的守护进程，基于过滤器检测附加的块设备，并将它们作为块设备自定义资源加载到Kubernetes中。这些定制资源旨在通过提供类似于: 轻松访问Kubernetes集群中可用的块设备清单 预测磁盘的故障，以帮助采取预防措施 允许动态地将磁盘挂载/卸载到存储Pod中，而无需重新启动在磁盘挂载/卸载的节点上运行的相应NDM Pod 尽管做了上述所有工作，NDM还是有助于提供持久卷的总体简化。 NDM是在OpenEBS安装期间作为守护进程部署的。NDM daemonset发现每个节点上的磁盘，并创建一个名为Block Device或BD的自定义资源。 访问权限说明 NDM守护进程运行在容器中，必须访问底层存储设备并以特权模式运行。 NDM需要特权模式，因为它需要访问/dev、/proc和/sys目录来监视附加设备，还需要使用各种探测器获取附加设备的详细信息。 NDM负责发现块设备并过滤掉不应该被OpenEBS使用的设备;例如，检测有OS文件系统的磁盘。 NDM pod默认情况下在容器中挂载主机的/proc目录，然后加载/proc/1/mounts，以找到操作系统使用的磁盘 NDM守护程序功能 发现Kubernetes节点上的块设备 在启动时发现块设备-创建和/或更新状态。 维护集群范围内磁盘的唯一id: 对WWN / PartitionUUID / FileSystemUUID / DeviceMapperUUID进行Hash计算 检测节点中添加/移除块设备，并更新块设备状态 添加块设备作为Kubernetes自定义资源，具有以下属性： spec: 如果可用，将更新以下内容 设备路径 设备链接 供应商和型号信息 WWN和序列号 容量 扇区和区块大小 labels: 主机名称(kubernetes.io/hostname) 块设备类型(ndm.io/blockdevice-type) Managed (ndm.io/managed) status: 状态可以有以下值 Active : 节点上存在块设备 Inactive : 给定节点上不存在块设备 Unknown : NDM在块设备最后被检测到的节点上停止/无法确定状态 过滤器 为要创建块设备CR的块设备类型配置过滤器。过滤器可以通过供应商类型、设备路径模式或挂载点进行配置 过滤器可以是包含过滤器，也可以是排除过滤器。它们被配置为configmap。 管理员用户可以在OpenEBS安装时通过更改OpenEBS操作员yaml文件或helm值中的NDM configmap来配置这些过滤器。 yaml文件。如果这些过滤器需要在安装后更新，那么可以遵循以下方法之一: 使用operator方式安装OpenEBS。在Yaml文件中，更新configmap中的过滤器并应用operator.yaml 如果OpenEBS是使用helm安装的，更新values.yaml中的configmap并使用helm进行升级 或者，使用kubectl编辑NDM configmap，更新过滤器 落地实践 OpenEBS的cStor与Jiva引擎由于组件过多，配置相较其他存储方案繁琐，生产环境不建议使用，这里我们仅论述Local PV引擎。 Local PV引擎不具备存储级复制能力，适用于k8s有状态服务的后端存储（如: es、redis等） Local PV Hostpath实践 对比Kubernetes Hostpath卷相比，OpenEBS本地PV Hostpath卷具有以下优势: OpenEBS本地PV Hostpath允许您的应用程序通过StorageClass、PVC和PV访问Hostpath。 这为您提供了更改PV提供者的灵活性，而无需重新设计应用程序YAML 使用Velero备份和恢复进行数据保护 通过对应用程序YAML和pod完全屏蔽主机路径来防范主机路径安全漏洞 环境依赖: k8s 1.12以上 OpenEBS 1.0以上 实践环境: docker 19.03.8 k8s 1.18.6 CentOS7 [root@localhost ~]# kubectl get node NAME STATUS ROLES AGE VERSION node1 Ready master,worker 8m8s v1.18.6 node2 Ready master,worker 7m15s v1.18.6 node3 Ready master,worker 7m15s v1.18.6 创建数据目录 在将要创建Local PV Hostpaths的节点上设置目录。这个目录将被称为BasePath。默认位置是/var/openebs/local 节点node1、node2、node3创建/data/openebs/local目录 （/data可以预先挂载数据盘，如未挂载额外数据盘，则使用操作系统'/'挂载点存储空间） mkdir -p /data/openebs/local 下载应用描述文件 yaml文件 发布openebs应用 根据上述配置文件，保证k8s集群可访问到如下镜像（建议导入本地私有镜像库，如: harbor） openebs/node-disk-manager:1.5.0 openebs/node-disk-operator:1.5.0 openebs/provisioner-localpv:2.10.0 更新openebs-operator.yaml中镜像tag为实际tag image: openebs/node-disk-manager:1.5.0 image: openebs/node-disk-operator:1.5.0 image: openebs/provisioner-localpv:2.10.0 发布 kubectl apply -f openebs-operator.yaml 查看发布状态 [root@localhost openebs]# kubectl get pod -n openebs -w NAME READY STATUS RESTARTS AGE openebs-localpv-provisioner-6d6d9cfc99-4sltp 1/1 Running 0 10s openebs-ndm-85rng 1/1 Running 0 10s openebs-ndm-operator-7df6668998-ptnlq 0/1 Running 0 10s openebs-ndm-qgqm9 1/1 Running 0 10s openebs-ndm-zz7ps 1/1 Running 0 10s 创建存储类 更改配置文件中的内容 value: \"/var/openebs/local/\" 发布创建存储类 cat > openebs-hostpath-sc.yaml 创建pvc验证可用性 cat > local-hostpath-pvc.yaml 查看pvc状态 [root@localhost openebs]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE local-hostpath-pvc Pending openebs-hostpath 2m15s 输出显示STATUS为Pending。这意味着PVC还没有被应用程序使用。 创建pod cat > local-hostpath-pod.yaml > /mnt/store/greet.txt; sleep $(($RANDOM % 5 + 300)); done' volumeMounts: - mountPath: /mnt/store name: local-storage EOF 发布创建 kubectl apply -f local-hostpath-pod.yaml 验证数据是否写入卷 [root@localhost openebs]# kubectl exec hello-local-hostpath-pod -- cat /mnt/store/greet.txt Thu Jun 24 15:10:45 CST 2021 [node1] Hello from OpenEBS Local PV. 验证容器是否使用Local PV Hostpath卷 [root@localhost openebs]# kubectl describe pod hello-local-hostpath-pod Name: hello-local-hostpath-pod Namespace: default Priority: 0 ... Volumes: local-storage: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: local-hostpath-pvc ReadOnly: false default-token-98scc: Type: Secret (a volume populated by a Secret) SecretName: default-token-98scc Optional: false ... 查看pvc状态 [root@localhost openebs]# kubectl get pvc local-hostpath-pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE local-hostpath-pvc Bound pvc-6eac3773-49ef-47af-a475-acb57ed15cf6 5G RWO openebs-hostpath 10m 查看该pv卷数据存储目录为 [root@localhost openebs]# kubectl get -o yaml pv pvc-6eac3773-49ef-47af-a475-acb57ed15cf6|grep 'path:' f:path: {} path: /data/openebs/local/pvc-6eac3773-49ef-47af-a475-acb57ed15cf6 并且pv配置了亲和性，制定了调度节点为node2 spec: accessModes: - ReadWriteOnce capacity: storage: 5G claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: local-hostpath-pvc namespace: default resourceVersion: \"9034\" uid: 6eac3773-49ef-47af-a475-acb57ed15cf6 local: fsType: \"\" path: /data/openebs/local/pvc-6eac3773-49ef-47af-a475-acb57ed15cf6 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node2 persistentVolumeReclaimPolicy: Delete storageClassName: openebs-hostpath volumeMode: Filesystem 验证三个节点存储目录下 结果证明数据仅存在于node2下 清理pod kubectl delete pod hello-local-hostpath-pod 基准测试 下载基准测试Job声明文件 调整以下内容 image: openebs/perf-test:latest # 调整为内网镜像库tag claimName: dbench # 调整为local-hostpath-pvc 发布运行 kubectl create -f fio-deploy.yaml 查看运行状态 [root@node1 openebs]# kubectl get pod NAME READY STATUS RESTARTS AGE dbench-729cw-nqfpt 1/1 Running 0 24s 查看基准测试结果 [root@node1 openebs]# kubectl logs -f dbench-729cw-nqfpt ... All tests complete. ================== = Dbench Summary = ================== Random Read/Write IOPS: 2144/6654. BW: 131MiB/s / 403MiB/s Average Latency (usec) Read/Write: 4254.08/3661.59 Sequential Read/Write: 1294MiB/s / 157MiB/s Mixed Random Read/Write IOPS: 1350/443 清理 kubectl delete pvc local-hostpath-pvc kubectl delete sc openebs-hostpath Local PV Device实践 对比Kubernetes本地持久卷，OpenEBS本地PV设备卷有以下优点: OpenEBS本地PV设备卷provider是动态的，Kubernetes设备卷provider是静态的 OpenEBS NDM更好地管理用于创建本地pv的块设备。 NDM提供了发现块设备属性、设置设备筛选器、度量集合以及检测块设备是否已经跨节点移动等功能 环境依赖: k8s 1.12以上 OpenEBS 1.0以上 实践环境: docker 19.03.8 k8s 1.18.6 CentOS7 [root@localhost ~]# kubectl get node NAME STATUS ROLES AGE VERSION node1 Ready master,worker 8m8s v1.18.6 node2 Ready master,worker 7m15s v1.18.6 node3 Ready master,worker 7m15s v1.18.6 三个节点上的/dev/sdb作为块设备存储 [root@node1 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 400G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 399G 0 part └─centos-root 253:0 0 399G 0 lvm / sdb 8:16 0 20G 0 disk sr0 11:0 1 4.4G 0 rom [root@node2 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 400G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 399G 0 part └─centos-root 253:0 0 399G 0 lvm / sdb 8:16 0 20G 0 disk sr0 11:0 1 4.4G 0 rom [root@node3 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 400G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 399G 0 part └─centos-root 253:0 0 399G 0 lvm / sdb 8:16 0 20G 0 disk sr0 11:0 1 4.4G 0 rom 创建数据目录 在将要创建Local PV Hostpaths的节点上设置目录。这个目录将被称为BasePath。默认位置是/var/openebs/local 节点node1、node2、node3创建/data/openebs/local目录 （/data可以预先挂载数据盘，如未挂载额外数据盘，则使用操作系统'/'挂载点存储空间） mkdir -p /data/openebs/local 下载应用描述文件 yaml文件 发布openebs应用 根据上述配置文件，保证k8s集群可访问到如下镜像（建议导入本地私有镜像库，如: harbor） openebs/node-disk-manager:1.5.0 openebs/node-disk-operator:1.5.0 openebs/provisioner-localpv:2.10.0 更新openebs-operator.yaml中镜像tag为实际tag image: openebs/node-disk-manager:1.5.0 image: openebs/node-disk-operator:1.5.0 image: openebs/provisioner-localpv:2.10.0 发布 kubectl apply -f openebs-operator.yaml 查看发布状态 [root@localhost openebs]# kubectl get pod -n openebs -w NAME READY STATUS RESTARTS AGE openebs-localpv-provisioner-6d6d9cfc99-4sltp 1/1 Running 0 10s openebs-ndm-85rng 1/1 Running 0 10s openebs-ndm-operator-7df6668998-ptnlq 0/1 Running 0 10s openebs-ndm-qgqm9 1/1 Running 0 10s openebs-ndm-zz7ps 1/1 Running 0 10s 创建存储类 cat > local-device-sc.yaml 创建pod及pvc cat > local-device-pod.yaml > /mnt/store/greet.txt; sleep $(($RANDOM % 5 + 300)); done' volumeMounts: - mountPath: /mnt/store name: local-storage EOF 发布 kubectl apply -f local-device-pod.yaml 查看pod状态 [root@node1 openebs]# kubectl get pod hello-local-device-pod -w NAME READY STATUS RESTARTS AGE hello-local-device-pod 1/1 Running 0 9s 确认pod关联pvc是否为local-device-pvc [root@node1 openebs]# kubectl describe pod hello-local-device-pod Name: hello-local-device-pod Namespace: default Node: node2/192.168.1.112 ... Volumes: local-storage: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: local-device-pvc ReadOnly: false ... 观察到调度的节点为node2，确认node2节点/dev/sdb是否被使用 [root@node2 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 400G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 399G 0 part └─centos-root 253:0 0 399G 0 lvm / sdb 8:16 0 20G 0 disk sr0 11:0 1 4.4G 0 rom [root@node2 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 400G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 399G 0 part └─centos-root 253:0 0 399G 0 lvm / sdb 8:16 0 20G 0 disk /var/lib/kubelet/pods/266b7b14-5eb7-40ec-bccb-3ac189acf939/volumes/kubernetes.io~local-volume/pvc-9bd89019-13dc-4 sr0 11:0 1 4.4G 0 rom 确实被使用，OpenEBS强大之处则在于此，极致的简洁。 如上文我们讨论的那样，NDM负责发现块设备并过滤掉不应该被OpenEBS使用的设备，例如，检测有OS文件系统的磁盘。 基准测试 创建基准测试pvc cat > dbench-pvc.yaml 下载基准测试Job声明文件 调整以下内容 image: openebs/perf-test:latest # 调整为内网镜像库tag 发布运行 kubectl create -f dbench-pvc.yaml kubectl create -f fio-deploy.yaml 查看运行状态 [root@node1 openebs]# kubectl get pod NAME READY STATUS RESTARTS AGE dbench-vqk68-f9877 1/1 Running 0 24s 查看基准测试结果 [root@node1 openebs]# kubectl logs -f dbench-vqk68-f9877 ... All tests complete. ================== = Dbench Summary = ================== Random Read/Write IOPS: 3482/6450. BW: 336MiB/s / 1017MiB/s Average Latency (usec) Read/Write: 2305.77/1508.63 Sequential Read/Write: 6683MiB/s / 2312MiB/s Mixed Random Read/Write IOPS: 3496/1171 从结果来看，相较Local PV HostPath模式性能翻倍 总结 在整个测试验证过程，OpenEBS给我的感觉是：极简的操作，尤其Local PV引擎的部署使用。 但OpenEBS现阶段也存在一些不足： cStor与Jiva数据面组件较多，配置较为繁琐（第一感觉概念性的组件过多，） cStor与Jiva部分组件创建依赖内部定义的镜像tag，在离线环境下无法通过调整为私有库tag导致组件无法成功运行 存储类型单一，多个引擎仅支持块存储类型，不支持原生多节点读写（需结合NFS实现），对比ceph等稍显逊色 建议以下场景使用OpenEBS作为后端存储： 单机测试环境 多机实验/演示环境 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/storage/rook.html":{"url":"2.容器/k8s/storage/rook.html","title":"rook","keywords":"","body":" Table of Contents generated with DocToc Rook Rook架构设计 Rook实践 Rook管理ceph-基础版 环境说明 依赖说明 开启准入控制器 创建rook operator 创建ceph集群 创建ceph工具箱 创建块存储 创建共享存储 开启控制面板 Rook Rook架构设计 Rook使Ceph存储系统能够使用Kubernetes原生资源对象在Kubernetes上运行。 下图说明了Ceph Rook如何与Kubernetes集成： 通过在Kubernetes集群中运行Ceph，Kubernetes应用可以挂载Rook管理的块设备和文件系统， 或者可以使用S3/swiftapi进行对象存储。 Rook operator自动配置存储组件并监视集群，以确保存储保持可用和正常。 Rook operator是一个简单的容器，它拥有引导和监视存储集群所需的所有东西。 operator将启动和监控Ceph monitor pods, Ceph OSD守护进程提供RADOS存储，以及启动和管理其他Ceph守护进程。 operator通过初始化运行服务所需的pods和其他组件来管理池、对象存储(S3/Swift)和文件系统的crd。 operator将监视存储守护程序，以确保群集正常运行。Ceph-mons将在必要时启动或故障转移， 并随着集群的增长或收缩进行其他调整。operator还将监视api服务请求的所需状态更改，并应用更改。 Rook operator还初始化消耗存储所需的代理。Rook会自动配置Ceph CSI驱动程序，将存储装载到pod中。 rook/ceph镜像包括管理集群所需的所有工具。许多Ceph概念，如放置组和crush map是隐藏的。 Rook在物理资源、池、卷、文件系统和存储桶方面为管理员创建了一个非常简化的用户体验。同时，当需要Ceph工具时，可以应用高级配置 Rook基于golang实现。Ceph是基于C++中实现的，其中数据路径是高度优化的。二者是最好的组合。 Rook实践 Rook管理ceph-基础版 环境说明 k8s集群信息 [root@node1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION node1 Ready master,worker 2d4h v1.18.6 node2 Ready master,worker 2d4h v1.18.6 node3 Ready master,worker 2d4h v1.18.6 内核版本 5.4.108-1.el7.elrepo.x86_64 磁盘设备 node1-node3各挂载一块100G磁盘，设备路径为/dev/sdb 依赖说明 要配置Ceph存储群集，至少需要以下一个本地存储选项： 原始设备（没有分区或格式化文件系统） 原始分区（无格式化文件系统） k8s存储类提供的块模式PV 安装LVM包 在以下场景中，Ceph OSD依赖于LVM： OSD是在原始设备或分区上创建的 如果启用了加密（encryptedDevice: true） 指定了元数据设备 在以下情况下，OSD不需要LVM： 使用storageClassDeviceSets在PVC上创建OSD 如果您的场景需要LVM，那么LVM需要在运行OSD的主机上可用。有些Linux发行版不附带lvm2包。 要运行Ceph OSDs，k8s群集中的所有存储节点上都需要此包。如果没有这个包， 即使Rook能够成功地创建Ceph OSD，当一个节点重新启动时，在重新启动的节点上运行的OSD pods将无法启动。 请使用Linux发行版的包管理器安装LVM。例如： yum install -y lvm2 操作系统内核 RBD类型 Ceph需要一个用RBD模块构建的Linux内核。 许多Linux发行版都有这个模块，但不是所有发行版都有。例如，GKE容器优化OS（COS）没有RBD。 您可以通过运行modprobe rbd来测试Kubernetes节点 cephfs类型 如果要从Ceph共享文件系统（CephFS）创建卷，建议的最低内核版本为4.17。 如果内核版本低于4.17，则不会强制执行所请求的PVC大小。存储配额将仅在较新的内核上强制执行。 开启准入控制器 准入控制器在对象持久化之前拦截到Kubernetes API服务器的请求，但在对请求进行身份验证和授权之后。 建议启用Rook准入控制器，以提供额外级别的验证，以确保Rook是使用自定义资源(CR)设置正确配置的。 下载ceph配置声明文件上传至k8s节点/root下,解压 rook-1.6.6.tar.gz tar zxvf v1.6.6.tar.gz mv rook-1.6.6 rook 可利用助手脚本自动配置部署Rook准入控制器,这个脚本将帮助我们完成以下任务: 创建自签名证书 为证书创建证书签名请求（CSR），并从Kubernetes集群获得批准 将这些证书存储为Kubernetes Secret 创建Service Account、ClusterRole和ClusterRoleBindings，以便以最低权限运行webhook服务 创建ValidatingWebhookConfig并使用来自集群的适当值填充CA bundle cd rook/ kubectl create -f cluster/examples/kubernetes/ceph/crds.yaml -f cluster/examples/kubernetes/ceph/common.yaml 离线环境下，手动下载cert-manager.yaml 上传至/root/rook/下，替换文件路径 sed -i \"s#https://github.com/jetstack/cert-manager/releases/download/\\$CERT_VERSION/#rook/#g\" \\ tests/scripts/deploy_admission_controller.sh 调整/root/rook/cert-manager.yaml文件内引用镜像tag(替换为可访问的镜像tag) [root@node1 rook]# cat cert-manager.yaml |grep 'image:' image: harbor.chs.neusoft.com/ceph-csi/jetstack/cert-manager-cainjector:v1.2.0 image: harbor.chs.neusoft.com/ceph-csi/jetstack/cert-manager-controller:v1.2.0 image: harbor.chs.neusoft.com/ceph-csi/jetstack/cert-manager-webhook:v1.2.0 发布 tests/scripts/deploy_admission_controller.sh 创建rook operator 调整/root/rook/cluster/examples/kubernetes/ceph/operator.yaml文件内引用镜像tag(替换为可访问的镜像tag) [root@node1 rook]# grep \"image:\" cluster/examples/kubernetes/ceph/operator.yaml image: rook/ceph:v1.6.6 调整以下内容，并放开注释 # ROOK_CSI_CEPH_IMAGE: \"quay.io/cephcsi/cephcsi:v3.3.1\" # ROOK_CSI_REGISTRAR_IMAGE: \"k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.2.0\" # ROOK_CSI_RESIZER_IMAGE: \"k8s.gcr.io/sig-storage/csi-resizer:v1.2.0\" # ROOK_CSI_PROVISIONER_IMAGE: \"k8s.gcr.io/sig-storage/csi-provisioner:v2.2.2\" # ROOK_CSI_SNAPSHOTTER_IMAGE: \"k8s.gcr.io/sig-storage/csi-snapshotter:v4.1.1\" # ROOK_CSI_ATTACHER_IMAGE: \"k8s.gcr.io/sig-storage/csi-attacher:v3.2.1\" 发布 kubectl apply -f cluster/examples/kubernetes/ceph/operator.yaml 查看状态 [root@node1 rook]# kubectl get pod -n rook-ceph NAME READY STATUS RESTARTS AGE rook-ceph-admission-controller-549f58dd9-rs8qs 1/1 Running 0 86s rook-ceph-admission-controller-549f58dd9-tmfnr 1/1 Running 0 86s rook-ceph-operator-869777bc74-dtv2h 1/1 Running 0 2m9s 此时，operator将自动启动准入控制器部署，而Webhook将开始拦截对Rook资源的请求。 创建ceph集群 /root/rook/cluster/examples/kubernetes/ceph/cluster.yaml 调整镜像tag ceph/ceph:v15.2.13 配置ceph数据盘 修改以下配置，其他默认 storage: # cluster level storage configuration and selection useAllNodes: true nodes: - name: \"node1\" devices: # specific devices to use for storage can be specified for each node - name: \"sdb\" - name: \"node2\" devices: # specific devices to use for storage can be specified for each node - name: \"sdb\" - name: \"node3\" devices: # specific devices to use for storage can be specified for each node - name: \"sdb\" 发布 kubectl apply -f /root/rook/cluster/examples/kubernetes/ceph/cluster.yaml 查看状态 查看pod [root@node1 ceph]# kubectl get pod -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-jsw67 3/3 Running 0 55s csi-cephfsplugin-provisioner-6667846fc9-g4jj2 6/6 Running 0 52s csi-cephfsplugin-provisioner-6667846fc9-wmzc7 6/6 Running 0 53s csi-cephfsplugin-tb4mh 3/3 Running 0 55s csi-cephfsplugin-xvk4s 3/3 Running 0 55s csi-rbdplugin-gl4tx 3/3 Running 0 57s csi-rbdplugin-provisioner-7bf5687dcd-4rsst 6/6 Running 0 57s csi-rbdplugin-provisioner-7bf5687dcd-7hf94 6/6 Running 0 57s csi-rbdplugin-x57s2 3/3 Running 0 57s csi-rbdplugin-zs64m 3/3 Running 0 57s rook-ceph-admission-controller-549f58dd9-rs8qs 1/1 Running 0 18h rook-ceph-admission-controller-549f58dd9-tmfnr 1/1 Running 0 18h rook-ceph-mon-a-canary-6d7bf84b9c-pjmd4 1/1 Running 0 50s rook-ceph-mon-b-canary-5b647bf496-pdvhj 1/1 Running 0 50s rook-ceph-mon-c-canary-54d67bb847-8pb7x 1/1 Running 0 49s rook-ceph-operator-869777bc74-dtv2h 1/1 Running 0 18h 查看集群状态 kubectl -n rook-ceph get CephCluster -o yaml 创建ceph工具箱 相当于ceph客户端，用于与ceph集群交互 创建发布 离线环境注意替换镜像tag(rook/ceph:v1.6.6) cat 测试可用性 [root@node1 ceph]# kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash 创建块存储 创建块存储池 cat 创建块存储存储类 cat 查看存储类 [root@node1 ceph]# kubectl get sc rook-ceph-block NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE rook-ceph-block rook-ceph.rbd.csi.ceph.com Delete Immediate false 3s 验证可用性 cat > /mnt/store/greet.txt; sleep $(($RANDOM % 5 + 300)); done' volumeMounts: - mountPath: /mnt/store name: ceph-rbd-storage EOF 查看pvc状态 [root@node1 kubernetes]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mysql-pv-claim Bound pvc-c1d19cfe-5028-43de-8cbd-405af96eb66c 20Gi RWO rook-ceph-block 93s 查看pod状态 [root@node1 ceph]# kubectl get pod NAME READY STATUS RESTARTS AGE ceph-block-pod 1/1 Running 0 13s 查看写入内容是否正确 [root@node1 ceph]# kubectl exec ceph-block-pod -- cat /mnt/store/greet.txt Tue Jun 29 16:20:07 CST 2021 [node1] Hello from Ceph RBD PV. 创建共享存储 一个共享的文件系统支持多pod读/写，这对于可以使用共享文件系统进行集群的应用程序可能很有用。 创建文件系统CRD cat 查看ceph mds服务状态 [root@node1 ceph]# kubectl -n rook-ceph get pod -l app=rook-ceph-mds NAME READY STATUS RESTARTS AGE rook-ceph-mds-myfs-a-5c9d84c7f8-z4csx 1/1 Running 0 48s rook-ceph-mds-myfs-b-5485989ff8-zl7g2 1/1 Running 0 47s 查看池 可以看到池myfs-data0（数据池）与myfs-metadata（元数据池）已自动创建 [root@node1 ceph]# kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph df --- RAW STORAGE --- CLASS SIZE AVAIL USED RAW USED %RAW USED hdd 300 GiB 297 GiB 137 MiB 3.1 GiB 1.04 TOTAL 300 GiB 297 GiB 137 MiB 3.1 GiB 1.04 --- POOLS --- POOL ID PGS STORED OBJECTS USED %USED MAX AVAIL device_health_metrics 1 1 0 B 0 0 B 0 94 GiB rbd-pool 2 32 31 MiB 154 119 MiB 0.04 94 GiB myfs-metadata 3 32 2.2 KiB 22 1.5 MiB 0 94 GiB myfs-data0 4 32 0 B 0 0 B 0 94 GiB 创建存储类（sc） cat 查看已有存储类 [root@node1 ceph]# kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE openebs-hostpath (default) openebs.io/local Delete WaitForFirstConsumer false 3d4h rook-ceph-block rook-ceph.rbd.csi.ceph.com Delete Immediate false 3h39m rook-cephfs rook-ceph.cephfs.csi.ceph.com Delete Immediate false 4s 测试可用性 离线环境注意替换镜像tag cat > kube-registry.yaml 发布 kubectl apply -f kube-registry.yaml 查看运行状态 [root@node1 ceph]# kubectl get pod -A|grep regi kube-system kube-registry-64f97cd5d6-f6hfz 1/1 Running 0 7m20s kube-system kube-registry-64f97cd5d6-l5h5m 1/1 Running 0 7m20s kube-system kube-registry-64f97cd5d6-nrvkc 1/1 Running 0 7m20s 开启控制面板 安装上述步骤搭建ceph集群，自带dashboard 查看服务信息 [root@node1 ceph]# kubectl -n rook-ceph get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE csi-cephfsplugin-metrics ClusterIP 10.233.41.137 8080/TCP,8081/TCP 80m csi-rbdplugin-metrics ClusterIP 10.233.53.251 8080/TCP,8081/TCP 80m rook-ceph-admission-controller ClusterIP 10.233.18.242 443/TCP 19h rook-ceph-mgr ClusterIP 10.233.32.164 9283/TCP 77m rook-ceph-mgr-dashboard ClusterIP 10.233.38.102 8443/TCP 77m rook-ceph-mon-a ClusterIP 10.233.35.191 6789/TCP,3300/TCP 79m rook-ceph-mon-b ClusterIP 10.233.39.246 6789/TCP,3300/TCP 78m rook-ceph-mon-c ClusterIP 10.233.30.204 6789/TCP,3300/TCP 78m 创建NodePort类型服务 cat > /root/rook/cluster/examples/kubernetes/ceph/dashboard-external-https.yaml 发布 kubectl apply -f /root/rook/cluster/examples/kubernetes/ceph/dashboard-external-https.yaml 获取登录信息 登录地址（http://节点IP:32445） [root@node1 ceph]# kubectl get svc rook-ceph-mgr-dashboard-external-https -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE rook-ceph-mgr-dashboard-external-https NodePort 10.233.39.95 8443:32445/TCP 21s 登录口令 kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=\"{['data']['password']}\" | base64 --decode && echo 查看控制面板 主页包含集群状态、存储容量、存储池等 存储池信息 存储osd信息 块存储信息 文件系统信息 经过上述实践，我们发现在现有k8s集群搭建一套ceph存储只要满足以下两点即可： 内核高于4.17 未被格式化分区的存储设备 整个部署流程对比非容器化部署更容易上手，利用k8s天生优势，保证了存储服务的高可用性。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/storage/Velero.html":{"url":"2.容器/k8s/storage/Velero.html","title":"Velero","keywords":"","body":"Velero 基于v1.6 介绍 Velero（以前叫Heptio Ark）提供了备份和恢复Kubernetes集群资源和持久卷的工具。 Velero主要功能如下： 备份k8s集群，并在集群丢失时恢复 迁移k8s集群资源至另一个集群 将生产集群复制到开发和测试集群 Velero组件: 服务端：运行于k8s集群内部 客户端：cli工具 工作原理 每个Velero操作（按需备份、定时备份、还原）都是一个自定义资源，由Kubernetes自定义资源定义（CRD）定义并存储在etcd中。 Velero还包括处理自定义资源以执行备份、恢复和所有相关操作的控制器。 可以备份或还原群集中的所有对象，也可以按类型、命名空间和/或标签筛选对象。 Velero非常适合于灾难恢复用例，以及在集群上执行系统操作（如升级）之前对应用程序状态进行快照 按需备份 备份操作包括： 将复制的Kubernetes对象上传到云对象存储中 调用云提供程序API以生成持久卷的磁盘快照（如果指定） 您可以选择指定备份期间要执行的备份hook。 例如，在拍摄快照之前，您可能需要通知数据库将其内存缓冲区刷新到磁盘。 请注意，群集备份并不是严格的原子备份。 如果备份时正在创建或编辑Kubernetes对象，则备份中可能不包括这些对象。 捕捉不一致信息的几率很低，但这是可能的 定时备份 定时备份操作允许您定期备份数据。 第一次备份是在第一次创建计划时执行的，随后的备份将按计划的指定间隔进行。这些间隔由Cron表达式指定。 定时备份以名称-保存，其中的格式为YYYYMMDDhhmmss 数据恢复/还原 还原操作允许您从以前创建的备份还原所有对象和持久卷。也可以仅还原对象和持久卷的筛选子集。 Velero支持多个命名空间重新映射—例如: 命名空间abc中的对象可以在命名空间def下重新创建 命名空间123中的对象可以在456下重新创建 还原的默认名称是-，其中的格式为YYYYMMDDhhmmss。也可以指定自定义名称。 还原的对象还包括一个带有键velero.io/restore-name和值的标签。 默认情况下，以读写模式创建备份存储位置。 但是，在还原过程中，可以将备份存储位置配置为只读模式，这将禁用存储位置的备份创建和删除。 这有助于确保在还原场景中不会无意中创建或删除备份。 您可以选择指定要在还原期间或资源还原之后执行的还原hook。 例如，您可能需要在启动数据库应用程序容器之前执行自定义数据库还原操作。 备份操作流程 当你执行velero backup create test-backup时： Velero客户端调用kubernetes api服务来创建备份对象 BackupController控制器发现新的备份对象并对其执行验证 BackupController控制器开始备份过程。它通过查询API服务的资源来收集要备份的数据。 BackupController调用对象存储服务（例如，AWS S3）来上载备份文件 默认情况下，velero backup create为任何持久卷创建磁盘快照。 您可以通过指定其他标志来调整快照。运行velero backup create--帮助查看可用标志。 可以使用选项--snapshot volumes=false禁用快照。 备份的API版本 Velero为每个组/资源使用Kubernetes API服务器的首选版本备份资源。 还原资源时，目标群集中必须存在相同的API组/版本才能成功还原。 例如，如果要备份的集群在things API组中有一个gizmos资源， 其中包含group/versions things/v1alpha1、things/v1beta1和things/v1， 并且服务器的首选组/版本是things/v1，那么将从things/v1api端点备份所有gizmos。 还原此群集的备份时，目标群集必须具有things/v1端点才能还原gizmo。 注意，things/v1不需要是目标集群中的首选版本；它只需要存在。 配置备份过期时间 创建备份时，可以通过添加标志--TTL来指定TTL（生存时间）。 如果Velero发现现有备份资源已过期，它将删除： 备份资源对象 来自云对象存储的备份文件 所有PersistentVolume快照 所有相关恢复数据 TTL标志允许用户使用以小时、分钟和秒为单位的值指定备份保留期， 格式为TTL 24h0m0s。如果未指定，将应用默认的TTL值30天。 同步对象存储与集群的备份信息 Velero会不断检查是否始终存在正确的备份资源。 如果存储桶中有格式正确的备份文件，但Kubernetes API中没有相应的备份资源， 则Velero会将对象存储中的信息同步到Kubernetes。 这使得恢复功能可以在群集迁移场景中工作，其中新群集中不存在原始备份对象。 同样，如果备份对象存在于Kubernetes中，但不在对象存储中， 那么它将从Kubernetes中删除，因为备份tarball不再存在。 安装Velero 基础安装 安装环境要求 k8s主节点 kubectl可用 Velero使用对象存储来存储备份和相关的工件。它还可以选择与受支持的块存储系统集成以快照持久卷。 在开始安装过程之前，应该从兼容提供程序列表中标识要使用的对象存储提供程序和可选块存储提供程序。 安装cli工具 下载velero-v1.6.1-linux-amd64.tar.gz 解压安装 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/workload/endpoint.html":{"url":"2.容器/k8s/workload/endpoint.html","title":"endpoint","keywords":"","body":"endpoint 将外部服务映射为集群内布，便于配置应用路由等 创建endpoint对象 addresses: 数组类型，可以为多个，也可以为一个 namespace: 命名空间 cat 创建service对象 port: 与endpoint一致 metadata.name: 与endpoint一致 metadata.namespace: 与endpoint一致 cat Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/workload/job.html":{"url":"2.容器/k8s/workload/job.html","title":"job","keywords":"","body":"Job Job创建一个或多个pod，并将继续重试pod的执行，直到指定数量的pod成功终止。 当pods成功完成时，Job将跟踪成功的完成。 当达到指定的成功完成次数时，Job（即作业）即完成。 删除Job将清理它创建的pod，暂停Job将删除其活动pod，直到作业再次恢复。 一个简单的例子是创建一个作业对象，以便可靠地运行一个Pod以完成任务。如果第一个Pod失败或被删除（例如由于节点硬件故障或节点重新启动），作业对象将启动一个新的Pod。 还可以使用作业并行运行多个pod。 job类型 适合以Job形式来运行的任务主要有三种 1.非并行Job: 通常只启动一个Pod，除非该Pod失败 当Pod成功终止时，立即视Job为完成状态 2.具有确定完成计数(completions)的并行Job: .spec.completions字段设置为非0的正数值 Job用来代表整个任务，当成功的Pod个数达到.spec.completions时，Job被视为完成 3.带工作队列(parallelism)的并行Job: 不设置spec.completions，默认值为.spec.parallelism 多个Pod之间必须相互协调，或者借助外部服务确定每个Pod要处理哪个工作条目。 例如，任一Pod都可以从工作队列中取走最多N个工作条目 每个Pod都可以独立确定是否其它Pod都已完成，进而确定Job是否完成 当Job中任何Pod成功终止，不再创建新Pod 一旦至少1个Pod成功完成，并且所有Pod都已终止，即可宣告Job成功完成 一旦任何Pod成功退出，任何其它Pod都不应再对此任务执行任何操作或生成任何输出。 所有Pod都应启动退出过程 第三种Job适用于并行计算？ 对于非并行的Job，你可以不设置spec.completions和spec.parallelism。 这两个属性都不设置时，均取默认值1，即第一种类型Job 对于确定完成计数类型的Job，你应该设置.spec.completions为所需要的完成个数。 你可以设置.spec.parallelism，也可以不设置，其默认值为1 控制并行性 并行性请求（.spec.parallelism）可以设置为任何非负整数。如果未设置，则默认为1。 如果设置为0，则Job相当于启动之后便被暂停，直到此值被增加 实际并行性（在任意时刻运行状态的Pods个数）可能比并行性请求略大或略小，原因如下: 对于确定完成计数Job，实际上并行执行的Pods个数不会超出剩余的完成数。 如果.spec.parallelism值较高，会被忽略。 对于工作队列Job，有任何Job成功结束之后，不会有新的Pod启动。不过，剩下的Pods允许执行完毕 如果Job控制器没有时间作出反应 如果Job控制器因为任何原因（例如，缺少ResourceQuota或者没有权限）无法创建Pods。 Pods个数可能比请求的数目小 Job控制器可能会因为之前同一Job中Pod失效次数过多而抑制新Pod的创建 当Pod被优雅地关闭时，它需要一段时间才能停止 IndexedJob特性 特性状态 FEATURE STATE: Kubernetes v1.21 [alpha] 开启方式 API server与controller manager服务通过添加--feature-gates=\"IndexedJob=true\"开启IndexedJob特性 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/k8s/workload/pod.html":{"url":"2.容器/k8s/workload/pod.html","title":"pod","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:03 "},"2.容器/kubesphere/":{"url":"2.容器/kubesphere/","title":"kubesphere","keywords":"","body":"扩容数据节点 ES每个索引默认5个主分片, 而每个主分片都相应的有一个副本。 查看es索引 [root@ceph01 ~]# kubectl -n kubesphere-logging-system exec -it elasticsearch-logging-data-0 -- curl -XGET 'localhost:9200/_cat/indices?v&pretty' health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open ks-logstash-log-2021.03.15 Ch7ACUwWQlinSz8WB8GvWA 5 1 2538499 0 1.8gb 962.4mb green open ks-logstash-events-2021.03.15 EkPSa0acR4umfw7hbf_kFA 5 1 16049 0 19.9mb 9.9mb green open ks-logstash-log-2021.03.16 GRrg9bRsRQKG22ef5WrDUw 5 1 1466740 0 1gb 556.7mb green open ks-logstash-events-2021.03.16 wpDRAwpPQ1e1B7E6mfawSw 5 1 8612 0 15.7mb 8mb 查看索引分片 [root@ceph01 ~]# kubectl -n kubesphere-logging-system exec -it elasticsearch-logging-data-0 -- curl -XGET 'localhost:9200/ks-logstash-log-2021.03.16/_search_shards' {\"nodes\":{\"J-y4ZBHES1uQX9u63Wn4Qw\":{\"name\":\"elasticsearch-logging-data-1\",\"ephemeral_id\":\"1KL5L_HqTRe4R2jb5TnqSw\",\"transport_address\":\"10.233.111.39:9300\",\"attributes\":{}},\"kyrYoJtoQQmoHk479cQ5yw\":{\"name\":\"elasticsearch-logging-data-0\",\"ephemeral_id\":\"b1wbbdqNRiq9017XPhdgZg\",\"transport_address\":\"10.233.111.22:9300\",\"attributes\":{}}},\"indices\":{\"ks-logstash-log-2021.03.16\":{}},\"shards\":[[{\"state\":\"STARTED\",\"primary\":true,\"node\":\"J-y4ZBHES1uQX9u63Wn4Qw\",\"relocating_node\":null,\"shard\":0,\"index\":\"ks-logstash-log-2021.03.16\",\"allocation_id\":{\"id\":\"oBpLdhR7Tb6_l9CxK3n94A\"}},{\"state\":\"STARTED\",\"primary\":false,\"node\":\"kyrYoJtoQQmoHk479cQ5yw\",\"relocating_node\":null,\"shard\":0,\"index\":\"ks-logstash-log-2021.03.16\",\"allocation_id\":{\"id\":\"Wnn1wRUtTGiaQQ2jydjcGA\"}}],[{\"state\":\"STARTED\",\"primary\":true,\"node\":\"kyrYoJtoQQmoHk479cQ5yw\",\"relocating_node\":null,\"shard\":1,\"index\":\"ks-logstash-log-2021.03.16\",\"allocation_id\":{\"id\":\"EctVniCpRAOiOhS7-h8ebQ\"}},{\"state\":\"STARTED\",\"primary\":false,\"node\":\"J-y4ZBHES1uQX9u63Wn4Qw\",\"relocating_node\":null,\"shard\":1,\"index\":\"ks-logstash-log-2021.03.16\",\"allocation_id\":{\"id\":\"reEv3HA3RN-DI9NIllD6ew\"}}],[{\"state\":\"STARTED\",\"primary\":true,\"node\":\"kyrYoJtoQQmoHk479cQ5yw\",\"relocating_node\":null,\"shard\":2,\"index\":\"ks-logstash-log-2021.03.16\",\"allocation_id\":{\"id\":\"b3s-F7yqTSKLWIGZBDIfJg\"}},{\"state\":\"STARTED\",\"primary\":false,\"node\":\"J-y4ZBHES1uQX9u63Wn4Qw\",\"relocating_node\":null,\"shard\":2,\"index\":\"ks-logstash-log-2021.03.16\",\"allocation_id\":{\"id\":\"Fe0052TWRAyYhB1ZS9WQoA\"}}],[{\"state\":\"STARTED\",\"primary\":false,\"node\":\"kyrYoJtoQQmoHk479cQ5yw\",\"relocating_node\":null,\"shard\":3,\"index\":\"ks-logstash-log-2021.03.16\",\"allocation_id\":{\"id\":\"CptaDh8wSK-HBWUt-lzIkw\"}},{\"state\":\"STARTED\",\"primary\":true,\"node\":\"J-y4ZBHES1uQX9u63Wn4Qw\",\"relocating_node\":null,\"shard\":3,\"index\":\"ks-logstash-log-2021.03.16\",\"allocation_id\":{\"id\":\"m335EmbPRF-4xrn_kHMQyA\"}}],[{\"state\":\"STARTED\",\"primary\":false,\"node\":\"J-y4ZBHES1uQX9u63Wn4Qw\",\"relocating_node\":null,\"shard\":4,\"index\":\"ks-logstash-log-2021.03.16\",\"allocation_id\":{\"id\":\"2YDSnbxQRz-WIhQpSxfIgg\"}},{\"state\":\"STARTED\",\"primary\":true,\"node\":\"kyrYoJtoQQmoHk479cQ5yw\",\"relocating_node\":null,\"shard\":4,\"index\":\"ks-logstash-log-2021.03.16\",\"allocation_id\":{\"id\":\"U08x7XvpTLS1aWXraD2VSg\"}}]]} 返回数据导入json在线视图查看 扩容elasticsearch-logging-data 扩容elasticsearch-logging-data至5副本（默认2），以保证每个节点1主分片1副本分片 kubectl scale sts elasticsearch-logging-data -n kubesphere-logging-system --replicas=5 修改数据节点存储卷 修改数据节点默认存储类 调整索引过期策略 kubesphere在开启日志系统功能下，默认在kubesphere-logging-system有个定时任务，每天凌晨1点执行 [root@ceph01 ~]# kubectl get CronJob -n kubesphere-logging-system NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE elasticsearch-logging-curator-elasticsearch-curator 0 1 * * * False 0 19h 32h 执行内容如下： [root@ceph01 ~]# kubectl -n kubesphere-logging-system get configmaps elasticsearch-logging-curator-elasticsearch-curator-config -o yaml 即为action_file.yml --- actions: 1: action: delete_indices description: \"Clean up ES by deleting old indices\" options: timeout_override: continue_if_exception: False disable_action: False ignore_empty_list: True filters: - filtertype: age source: name direction: older timestring: '%Y.%m.%d' unit: days unit_count: 7 field: stats_result: epoch: exclude: False elastisearch curator 即es索引管理器 ，主要用于管理索引生命周期 上述配置解析： 删除七天前的索引 调整为30天过期 kubectl -n kubesphere-logging-system edit configmaps elasticsearch-logging-curator-elasticsearch-curator-config 调整unit_count: 7为unit_count: 30 elasticsearch-logging-curator-elasticsearch-curator-config 更多高阶配置参考curator官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/容器原理/cgroup.html":{"url":"2.容器/容器原理/cgroup.html","title":"cgroup","keywords":"","body":"cgroups 简介 Cgroups用来提供对一组进程以及将来子进程的资源限制，包含三个组件： 控制组: 一个cgroups包含一组进程，并可以在这个cgroups上增加Linux subsystem的各种参数配置， 将一组进程和一组subsystem关联起来 subsystem 子系统是一组资源控制模块 可以通过lssubsys -a命令查看当前内核支持哪些subsystem 利用cgroup限制程序资源 安装cgroup管理工具 yum install libcgroup libcgroup-tools -y 限制cpu 创建cpu的cgroup控制组，控制组名为demo01 cgcreate -g cpu:/demo01 查看cpu控制组 [root@localhost ~]# cd /sys/fs/cgroup/cpu/demo01 [root@localhost demo01]# ls cgroup.clone_children cgroup.procs cpuacct.usage cpu.cfs_period_us cpu.rt_period_us cpu.shares notify_on_release cgroup.event_control cpuacct.stat cpuacct.usage_percpu cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat tasks cpu.cfs_period_us: cpu分配的周期(微秒），默认为100000 cpu.cfs_quota_us: 表示该control group限制占用的时间（微秒），默认为-1，表示不限制。 配置demo1控制组cpu参数 cpu.cfs_quota_us和cpu.cfs_period_us是控制cpu的两个属性， 可以通过设置它们的比值来设置某个组群的cpu使用率。在此，我们将cpu的使用率限制到30%。 # 设置cpu分配的周期(微秒），即为默认值 cgset -r cpu.cfs_period_us=100000 demo01 # 如果设为30000，表示占用30000/10000=30%的CPU cgset -r cpu.cfs_quota_us=30000 demo01 创建测试程序，测试无限制情况下cpu占用率 创建死循环脚本 cat > ~/demo.sh /dev/null done EOF 执行死循环脚本 [root@localhost ~]# nohup sh ~/demo.sh &> /dev/null & [1] 15319 查看进程占用系统资源 top - 23:38:34 up 20 min, 1 user, load average: 0.76, 0.20, 0.11 Tasks: 128 total, 2 running, 126 sleeping, 0 stopped, 0 zombie %Cpu(s): 19.7 us, 5.3 sy, 0.0 ni, 74.5 id, 0.0 wa, 0.0 hi, 0.6 si, 0.0 st KiB Mem : 3861280 total, 3284036 free, 203144 used, 374100 buff/cache KiB Swap: 2097148 total, 2097148 free, 0 used. 3425504 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 15319 root 20 0 113288 1184 1012 R 99.7 0.0 0:43.61 sh 9 root 20 0 0 0 0 S 1.0 0.0 0:01.43 rcu_sched 1037 root 20 0 157828 6668 5120 S 0.3 0.2 0:01.83 sshd kill该进程 kill -9 15319 通过指定cgroup进行cpu配额限制 [root@localhost ~]# nohup cgexec -g cpu:/demo1 sh ~/demo.sh &> /dev/null & [1] 16893 查看cpu占用情况 top - 23:58:27 up 40 min, 2 users, load average: 0.72, 0.28, 0.18 Tasks: 132 total, 2 running, 130 sleeping, 0 stopped, 0 zombie %Cpu(s): 7.1 us, 0.7 sy, 0.0 ni, 92.2 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 3861280 total, 3274444 free, 208332 used, 378504 buff/cache KiB Swap: 2097148 total, 2097148 free, 0 used. 3420156 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 22745 root 20 0 113288 1184 1008 R 30.0 0.0 0:01.22 sh /root/demo.sh Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/容器原理/foundation.html":{"url":"2.容器/容器原理/foundation.html","title":"foundation","keywords":"","body":"容器基础 容器与虚拟化 传统虚拟化-虚拟机 实质：一套物理资源、多套Guest操作系统、系统级隔离 容器虚拟化 实质：一套物理资源、一套内核、进程级隔离 两者对比 容器的本质 一个视图被隔离、资源受限的进程，容器里PID=1的进程就是应用本身 初识hypervisor TODO 容器核心技术 基于5.4.108-1.el7.elrepo.x86_64内核 命名空间概念 什么是命名空间？ namespace是Linux内核用来隔离内核资源的实现方式 命名空间实质 进程视图隔离 常用命名空间（进程视图隔离内容） USER命名空间 UTS命名空间 IPC命名空间 通过这七个选项我们能在创建新的进程时设置新进程应该在哪些资源上与宿主机器进行隔离。 因此容器只能感知内部的进程，而对宿主机和其他容器一无所知。 命名空间管理 查看当前命名空间 [root@localhost user]# lsns NS TYPE NPROCS PID USER COMMAND 4026531836 pid 258 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531837 user 289 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531838 uts 276 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531839 ipc 258 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531840 mnt 248 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531856 mnt 1 28 root kdevtmpfs 4026531956 net 282 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026532512 net 1 747 rtkit /usr/libexec/rtkit-daemon 4026532583 mnt 1 747 rtkit /usr/libexec/rtkit-daemon 4026532584 mnt 2 773 root /usr/sbin/NetworkManager --no-daemon 4026532585 mnt 1 778 root /usr/libexec/bluetooth/bluetoothd 4026532586 mnt 1 788 chrony /usr/sbin/chronyd 4026532587 mnt 1 17813 root /usr/local/bin/etcd 4026532588 uts 1 17813 root /usr/local/bin/etcd 4026532589 ipc 1 17813 root /usr/local/bin/etcd 4026532590 pid 1 17813 root /usr/local/bin/etcd ... 命名空间限制 /proc/sys/user/max_user_namespaces: 15511 /proc/sys/user/max_uts_namespaces: 15511 /proc/sys/user/max_pid_namespaces: 15511 /proc/sys/user/max_net_namespaces: 15511 /proc/sys/user/max_mnt_namespaces: 15511 /proc/sys/user/max_ipc_namespaces: 15511 /proc/sys/user/max_inotify_watches: 8192 /proc/sys/user/max_inotify_instances: 524288 /proc/sys/user/max_cgroup_namespaces: 15511 参考文献 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/容器原理/ns-ipc.html":{"url":"2.容器/容器原理/ns-ipc.html","title":"ns-ipc","keywords":"","body":"ipc命名空间 概述 主要能力 提供进程间通信的隔离能力 ipc命名空间隔离性验证 获取当前进程ID ```shell script [root@localhost ~]# echo $$ 49265 在这里为了方面解释，我们定义进程`ID`为`49265`的进程名称为`PID-A` > 查看当前进程命名空间信息 ```shell script [root@localhost ~]# ls -l /proc/$$/ns total 0 lrwxrwxrwx 1 root root 0 Jul 14 07:27 cgroup -> cgroup:[4026531835] lrwxrwxrwx 1 root root 0 Jul 14 07:27 ipc -> ipc:[4026531839] lrwxrwxrwx 1 root root 0 Jul 14 07:27 mnt -> mnt:[4026531840] lrwxrwxrwx 1 root root 0 Jul 14 07:27 net -> net:[4026531992] lrwxrwxrwx 1 root root 0 Jul 14 07:27 pid -> pid:[4026531836] lrwxrwxrwx 1 root root 0 Jul 14 07:27 pid_for_children -> pid:[4026531836] lrwxrwxrwx 1 root root 0 Jul 14 07:27 user -> user:[4026531837] lrwxrwxrwx 1 root root 0 Jul 14 07:27 uts -> uts:[4026531838] 查看PID-A进程ipc信息 ```shell script [root@localhost ~]# ipcs ------ Message Queues -------- key msqid owner perms used-bytes messages ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0x00000000 2 gdm 777 16384 1 dest 0x00000000 5 gdm 777 7372800 2 dest ------ Semaphore Arrays -------- key semid owner perms nsems > 使用`unshare`隔离`ipc namespace` ```shell script unshare --ipc /bin/bash 查看进程ID，发现已变更 ```shell script [root@localhost ~]# echo $$ 62293 在这里为了方面解释，我们定义进程`ID`为`62293`的进程名称为`PID-B` 查看两个进程关系，显然`PID-A`与`PID-B`为父子关系的两个进程 ```shell script [root@localhost ~]# ps -ef|grep 62293 root 62293 49265 0 07:33 pts/0 00:00:00 /bin/bash root 62430 62293 0 07:33 pts/0 00:00:00 ps -ef root 62431 62293 0 07:33 pts/0 00:00:00 grep --color=auto 62293 查看PID-B进程的ipc信息 ```shell script [root@localhost ~]# ipcs ------ Message Queues -------- key msqid owner perms used-bytes messages ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status ------ Semaphore Arrays -------- key semid owner perms nsems 显然与`PID-A`进程不一致 > 测试: `PID-B`创建一个消息队列，是否`PID-A`中可以看到 ```shell script [root@localhost ~]# ipcmk --queue Message queue id: 0 [root@localhost ~]# ipcs ------ Message Queues -------- key msqid owner perms used-bytes messages 0x6c54b6c4 0 root 644 0 0 ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status ------ Semaphore Arrays -------- key semid owner perms nsems 新开一个ssh链接（会产生新的进程），查看是否可以看到PID-B中消息队列 ```shell script [root@localhost ~]# echo $$ 49857 [root@localhost ~]# ipcs -q ------ Message Queues -------- key msqid owner perms used-bytes messages 显然无法查看，隔离验证成功！ > 查看`PID-B`进程命名空间信息 ```shell script [root@localhost ~]# ls -l /proc/62293/ns total 0 lrwxrwxrwx 1 root root 0 Jul 14 07:47 cgroup -> cgroup:[4026531835] lrwxrwxrwx 1 root root 0 Jul 14 07:47 ipc -> ipc:[4026532765] lrwxrwxrwx 1 root root 0 Jul 14 07:47 mnt -> mnt:[4026531840] lrwxrwxrwx 1 root root 0 Jul 14 07:47 net -> net:[4026531992] lrwxrwxrwx 1 root root 0 Jul 14 07:47 pid -> pid:[4026531836] lrwxrwxrwx 1 root root 0 Jul 14 07:47 pid_for_children -> pid:[4026531836] lrwxrwxrwx 1 root root 0 Jul 14 07:47 user -> user:[4026531837] lrwxrwxrwx 1 root root 0 Jul 14 07:47 uts -> uts:[4026531838] 对比PID-A，发现二者区别仅为ipc不同 IPC实现方式 Linux进程通信方式 信号量 共享内存 消息队列 管道 信号 套接字通信 其中信号量，共享内存，消息队列基于内核的IPC命名空间实现 ```shell script [root@localhost ~]# ipcs ------ Message Queues -------- key msqid owner perms used-bytes messages 0x84300480 0 root 644 0 0 0xba58165a 1 root 644 0 0 0xb5be9e2a 2 root 644 0 0 ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0x00000000 2 gdm 777 16384 1 dest 0x00000000 5 gdm 777 7372800 2 dest ------ Semaphore Arrays -------- key semid owner perms nsems ``` 参考文档 Docker基础: Linux内核命名空间之（2） ipc namespace Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/容器原理/ns-mnt.html":{"url":"2.容器/容器原理/ns-mnt.html","title":"ns-mnt","keywords":"","body":"MNT命名空间 概念 mnt namespace有什么能力？ mnt(mount缩写) 命名空间提供了隔离mount point能力。 每个mnt namespace内的文件结构可以单独修改，互不影响。 隔离性验证 查看挂载信息 查看当前进程挂载点信息 ```shell script [root@localhost ns]# ll /proc/$$/mount* -r--r--r-- 1 root root 0 Jul 14 22:21 /proc/88929/mountinfo -r--r--r-- 1 root root 0 Jul 14 22:21 /proc/88929/mounts -r-------- 1 root root 0 Jul 14 22:21 /proc/88929/mountstats > 查看`mountinfo`内容 ```shell script 23 46 0:22 / /sys rw,nosuid,nodev,noexec,relatime shared:6 - sysfs sysfs rw 24 46 0:5 / /proc rw,nosuid,nodev,noexec,relatime shared:5 - proc proc rw 25 46 0:6 / /dev rw,nosuid shared:2 - devtmpfs devtmpfs rw,size=1985460k,nr_inodes=496365,mode=755 26 23 0:7 / /sys/kernel/security rw,nosuid,nodev,noexec,relatime shared:7 - securityfs securityfs rw 27 25 0:23 / /dev/shm rw,nosuid,nodev shared:3 - tmpfs tmpfs rw 28 25 0:24 / /dev/pts rw,nosuid,noexec,relatime shared:4 - devpts devpts rw,gid=5,mode=620,ptmxmode=000 29 46 0:25 / /run rw,nosuid,nodev shared:23 - tmpfs tmpfs rw,mode=755 30 23 0:26 / /sys/fs/cgroup ro,nosuid,nodev,noexec shared:8 - tmpfs tmpfs ro,mode=755 31 30 0:27 / /sys/fs/cgroup/systemd rw,nosuid,nodev,noexec,relatime shared:9 - cgroup cgroup rw,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd 32 23 0:28 / /sys/fs/pstore rw,nosuid,nodev,noexec,relatime shared:21 - pstore pstore rw 33 30 0:29 / /sys/fs/cgroup/cpu,cpuacct rw,nosuid,nodev,noexec,relatime shared:10 - cgroup cgroup rw,cpu,cpuacct 34 30 0:30 / /sys/fs/cgroup/memory rw,nosuid,nodev,noexec,relatime shared:11 - cgroup cgroup rw,memory 35 30 0:31 / /sys/fs/cgroup/devices rw,nosuid,nodev,noexec,relatime shared:12 - cgroup cgroup rw,devices 36 30 0:32 / /sys/fs/cgroup/freezer rw,nosuid,nodev,noexec,relatime shared:13 - cgroup cgroup rw,freezer 37 30 0:33 / /sys/fs/cgroup/perf_event rw,nosuid,nodev,noexec,relatime shared:14 - cgroup cgroup rw,perf_event 38 30 0:34 / /sys/fs/cgroup/pids rw,nosuid,nodev,noexec,relatime shared:15 - cgroup cgroup rw,pids 39 30 0:35 / /sys/fs/cgroup/hugetlb rw,nosuid,nodev,noexec,relatime shared:16 - cgroup cgroup rw,hugetlb 40 30 0:36 / /sys/fs/cgroup/net_cls,net_prio rw,nosuid,nodev,noexec,relatime shared:17 - cgroup cgroup rw,net_cls,net_prio 41 30 0:37 / /sys/fs/cgroup/blkio rw,nosuid,nodev,noexec,relatime shared:18 - cgroup cgroup rw,blkio 42 30 0:38 / /sys/fs/cgroup/rdma rw,nosuid,nodev,noexec,relatime shared:19 - cgroup cgroup rw,rdma 43 30 0:39 / /sys/fs/cgroup/cpuset rw,nosuid,nodev,noexec,relatime shared:20 - cgroup cgroup rw,cpuset 44 23 0:40 / /sys/kernel/config rw,relatime shared:22 - configfs configfs rw 46 1 8:3 / / rw,relatime shared:1 - xfs /dev/sda3 rw,attr2,inode64,logbufs=8,logbsize=32k,noquota 22 24 0:21 / /proc/sys/fs/binfmt_misc rw,relatime shared:24 - autofs systemd-1 rw,fd=23,pgrp=1,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=711 47 25 0:42 / /dev/hugepages rw,relatime shared:25 - hugetlbfs hugetlbfs rw,pagesize=2M 48 25 0:20 / /dev/mqueue rw,relatime shared:26 - mqueue mqueue rw 49 23 0:8 / /sys/kernel/debug rw,relatime shared:27 - debugfs debugfs rw 51 46 8:1 / /boot rw,relatime shared:28 - xfs /dev/sda1 rw,attr2,inode64,logbufs=8,logbsize=32k,noquota 52 46 0:44 / /var/lib/nfs/rpc_pipefs rw,relatime shared:29 - rpc_pipefs sunrpc rw 203 46 0:46 / /var/lib/docker/231072.231072/overlay2/6979cabf2db354126e61163ddc90b5738c600797fefbf0c41c9b56600f011d1f/merged rw,relatime shared:173 - overlay overlay rw,lowerdir=/var/lib/docker/231072.231072/overlay2/l/67CHEM5VH4RUKAJ7YJQWRBNVJE:/var/lib/docker/231072.231072/overlay2/l/GOFSE7LF6JYPQUVKRKRQFETASF,upperdir=/var/lib/docker/231072.231072/overlay2/6979cabf2db354126e61163ddc90b5738c600797fefbf0c41c9b56600f011d1f/diff,workdir=/var/lib/docker/231072.231072/overlay2/6979cabf2db354126e61163ddc90b5738c600797fefbf0c41c9b56600f011d1f/work 310 29 0:4 net:[4026532689] /run/docker/netns/bae6cec20525 rw shared:181 - nsfs nsfs rw 333 29 0:45 / /run/user/1000 rw,nosuid,nodev,relatime shared:217 - tmpfs tmpfs rw,size=400496k,mode=700,uid=1000,gid=1000 343 333 0:59 / /run/user/1000/gvfs rw,nosuid,nodev,relatime shared:227 - fuse.gvfsd-fuse gvfsd-fuse rw,user_id=1000,group_id=1000 353 23 0:60 / /sys/fs/fuse/connections rw,relatime shared:237 - fusectl fusectl rw 522 29 0:63 / /run/user/0 rw,nosuid,nodev,relatime shared:440 - tmpfs tmpfs rw,size=400496k,mode=700 查看mount内容 ```shell script [root@localhost 88929]# cat /proc/$$/mounts sysfs /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0 proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0 devtmpfs /dev devtmpfs rw,nosuid,size=1985460k,nr_inodes=496365,mode=755 0 0 securityfs /sys/kernel/security securityfs rw,nosuid,nodev,noexec,relatime 0 0 tmpfs /dev/shm tmpfs rw,nosuid,nodev 0 0 devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0 tmpfs /run tmpfs rw,nosuid,nodev,mode=755 0 0 tmpfs /sys/fs/cgroup tmpfs ro,nosuid,nodev,noexec,mode=755 0 0 cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd 0 0 pstore /sys/fs/pstore pstore rw,nosuid,nodev,noexec,relatime 0 0 cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0 cgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0 cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0 cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0 cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0 cgroup /sys/fs/cgroup/pids cgroup rw,nosuid,nodev,noexec,relatime,pids 0 0 cgroup /sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0 cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0 cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0 cgroup /sys/fs/cgroup/rdma cgroup rw,nosuid,nodev,noexec,relatime,rdma 0 0 cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0 configfs /sys/kernel/config configfs rw,relatime 0 0 /dev/sda3 / xfs rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota 0 0 systemd-1 /proc/sys/fs/binfmt_misc autofs rw,relatime,fd=23,pgrp=1,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=711 0 0 hugetlbfs /dev/hugepages hugetlbfs rw,relatime,pagesize=2M 0 0 mqueue /dev/mqueue mqueue rw,relatime 0 0 debugfs /sys/kernel/debug debugfs rw,relatime 0 0 /dev/sda1 /boot xfs rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota 0 0 sunrpc /var/lib/nfs/rpc_pipefs rpc_pipefs rw,relatime 0 0 overlay /var/lib/docker/231072.231072/overlay2/6979cabf2db354126e61163ddc90b5738c600797fefbf0c41c9b56600f011d1f/merged overlay rw,relatime,lowerdir=/var/lib/docker/231072.231072/overlay2/l/67CHEM5VH4RUKAJ7YJQWRBNVJE:/var/lib/docker/231072.231072/overlay2/l/GOFSE7LF6JYPQUVKRKRQFETASF,upperdir=/var/lib/docker/231072.231072/overlay2/6979cabf2db354126e61163ddc90b5738c600797fefbf0c41c9b56600f011d1f/diff,workdir=/var/lib/docker/231072.231072/overlay2/6979cabf2db354126e61163ddc90b5738c600797fefbf0c41c9b56600f011d1f/work 0 0 nsfs /run/docker/netns/bae6cec20525 nsfs rw 0 0 tmpfs /run/user/1000 tmpfs rw,nosuid,nodev,relatime,size=400496k,mode=700,uid=1000,gid=1000 0 0 gvfsd-fuse /run/user/1000/gvfs fuse.gvfsd-fuse rw,nosuid,nodev,relatime,user_id=1000,group_id=1000 0 0 fusectl /sys/fs/fuse/connections fusectl rw,relatime 0 0 tmpfs /run/user/0 tmpfs rw,nosuid,nodev,relatime,size=400496k,mode=700 0 0 > 查看`mountstats` 挂载状态 ```shell script [root@localhost ~]# cat /proc/$$/mountstats device sysfs mounted on /sys with fstype sysfs device proc mounted on /proc with fstype proc device devtmpfs mounted on /dev with fstype devtmpfs device securityfs mounted on /sys/kernel/security with fstype securityfs device tmpfs mounted on /dev/shm with fstype tmpfs device devpts mounted on /dev/pts with fstype devpts device tmpfs mounted on /run with fstype tmpfs device tmpfs mounted on /sys/fs/cgroup with fstype tmpfs device cgroup mounted on /sys/fs/cgroup/systemd with fstype cgroup device pstore mounted on /sys/fs/pstore with fstype pstore device cgroup mounted on /sys/fs/cgroup/cpu,cpuacct with fstype cgroup device cgroup mounted on /sys/fs/cgroup/memory with fstype cgroup device cgroup mounted on /sys/fs/cgroup/devices with fstype cgroup device cgroup mounted on /sys/fs/cgroup/freezer with fstype cgroup device cgroup mounted on /sys/fs/cgroup/perf_event with fstype cgroup device cgroup mounted on /sys/fs/cgroup/pids with fstype cgroup device cgroup mounted on /sys/fs/cgroup/hugetlb with fstype cgroup device cgroup mounted on /sys/fs/cgroup/net_cls,net_prio with fstype cgroup device cgroup mounted on /sys/fs/cgroup/blkio with fstype cgroup device cgroup mounted on /sys/fs/cgroup/rdma with fstype cgroup device cgroup mounted on /sys/fs/cgroup/cpuset with fstype cgroup device configfs mounted on /sys/kernel/config with fstype configfs device /dev/sda3 mounted on / with fstype xfs device systemd-1 mounted on /proc/sys/fs/binfmt_misc with fstype autofs device hugetlbfs mounted on /dev/hugepages with fstype hugetlbfs device mqueue mounted on /dev/mqueue with fstype mqueue device debugfs mounted on /sys/kernel/debug with fstype debugfs device /dev/sda1 mounted on /boot with fstype xfs device sunrpc mounted on /var/lib/nfs/rpc_pipefs with fstype rpc_pipefs device overlay mounted on /var/lib/docker/231072.231072/overlay2/6979cabf2db354126e61163ddc90b5738c600797fefbf0c41c9b56600f011d1f/merged with fstype overlay device nsfs mounted on /run/docker/netns/bae6cec20525 with fstype nsfs device tmpfs mounted on /run/user/1000 with fstype tmpfs device gvfsd-fuse mounted on /run/user/1000/gvfs with fstype fuse.gvfsd-fuse device fusectl mounted on /sys/fs/fuse/connections with fstype fusectl device tmpfs mounted on /run/user/0 with fstype tmpfs 隔离性验证 关于mount namespace记住以下三点: a.每个mount namespace都有一份自己的挂载点列表 b.当使用clone函数或unshare函数并传入CLONE_NEWNS标志创建新的mount namespace时， 新mount namespace中的挂载点其实是从调用者所在的mount namespace中拷贝的。 c.在新的mount namespace创建之后，这两个mount namespace及其挂载点基本无任何关系， 两个mount namespace是相互隔离的。 安装mkisofs演示mnt namespace ```shell script yum install mkisofs -y > 创建演示用`iso`文件 ```shell script hostnamectl set-hostname vm mkdir -p ~/iso/{A,B} echo \"A\" > ~/iso/A/a.txt echo \"B\" > ~/iso/B/b.txt cd ~/iso mkisofs -o ./A.iso ./A mkisofs -o ./B.iso ./B exec bash 创建用于挂载的目录 ```shell script mkdir -p /mnt/{isoA,isoB} > 在当前`mnt namepace`下挂载`~/iso/A.iso`至`/mnt/isoA` 查看当前`mnt namepace`编号 ```shell script [root@vm iso]# readlink /proc/$$/ns/mnt mnt:[4026531840] 挂载 ```shell script [root@vm iso]# mount ~/iso/A.iso /mnt/isoA mount: /dev/loop0 is write-protected, mounting read-only [root@vm iso]# cat /mnt/isoA/a.txt A 查看挂载状态 ```shell script [root@vm iso]# mount |grep A.iso /root/iso/A.iso on /mnt/isoA type iso9660 (ro,relatime,nojoliet,check=s,map=n,blocksize=2048) 创建并进入新的mount和uts namespace ```shell script unshare --mount --uts /bin/bash hostnamectl set-hostname container-A exec bash > 查看新的`mount namespace`挂载信息 查看`mnt`命名空间编号 ```shell script [root@vm iso]# readlink /proc/$$/ns/mnt mnt:[4026532765] 查看挂载信息 ```shell script [root@vm iso]# mount|grep A.iso /root/iso/A.iso on /mnt/isoA type iso9660 (ro,relatime,nojoliet,check=s,map=n,blocksize=2048) `b.`内容验证成功 > 新的`mount namespace`内挂载`~/iso/B.iso`至`/mnt/isoB` 挂载 ```shell script [root@vm iso]# mount ~/iso/B.iso /mnt/isoB mount: /dev/loop1 is write-protected, mounting read-only 查看挂载状态 ```shell script [root@vm iso]# mount |grep B.iso /root/iso/B.iso on /mnt/isoB type iso9660 (ro,relatime,nojoliet,check=s,map=n,blocksize=2048) > 新的`mount namespace`内卸载`/mnt/isoA` ```shell script [root@vm iso]# umount /mnt/isoA [root@vm iso]# ls /mnt/isoA 返回第一个mnt命名空间 通过新建session实现，并执行以下命令确认mnt命名空间(4026531840) ```shell script readlink /proc/$$/ns/mnt 查看挂载信息 ```shell script [root@container-a ~]# cat /mnt/isoA/a.txt A [root@container-a ~]# mount |grep iso /root/iso/A.iso on /mnt/isoA type iso9660 (ro,relatime,nojoliet,check=s,map=n,blocksize=2048) c.验证成功 参考文献 liunx mnt命名空间 Docker 学习笔记11 容器技术原理 Mount Namespace Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/容器原理/ns-net.html":{"url":"2.容器/容器原理/ns-net.html","title":"ns-net","keywords":"","body":" 网络命名空间 概念 veth-pair介绍 不同网络命名空间之间联通方案 直连 通过Bridge相连 通过OVS相连 docker网络浅析 容器网络互通原理 网络隔离原理 参考文献 网络命名空间 概念 网络命名空间有什么能力？ 隔离网络设备、协议栈、端口等 Linux中，网络命名空间可以被认为是隔离的拥有单独网络栈（网卡、路由转发表、iptables）的环境。网络命名空间经常用来隔离网络设备和服务，只有拥有同样网络命名空间的设备，才能看到彼此。 从逻辑上说，网络命名空间是网络栈的副本，有自己的网络设备、路由选择表、邻接表、Netfilter表、网络套接字、网络procfs条目、网络sysfs条目和其他网络资源。 从系统的角度来看，当通过clone()系统调用创建新进程时，传递标志CLONE_NEWNET将在新进程中创建一个全新的网络命名空间。 从用户的角度来看，我们只需使用工具ip（package is iproute2）来创建一个新的持久网络命名空间 veth-pair介绍 veth-pair是什么? 顾名思义，veth-pair就是一对的虚拟设备接口，和tap/tun设备不同的是，它都是成对出现的。 一端连着协议栈，一端彼此相连着。如下所示: +-------------------------------------------------------------------+ | | | +------------------------------------------------+ | | | Newwork Protocol Stack | | | +------------------------------------------------+ | | ↑ ↑ ↑ | |.................|...............|...............|.................| | ↓ ↓ ↓ | | +----------+ +-----------+ +-----------+ | | | ens33 | | veth0 | | veth1 | | | +----------+ +-----------+ +-----------+ | |192.168.235.128 ↑ ↑ ↑ | | | +---------------+ | | | 10.10.10.2 10.10.10.3 | +-----------------|-------------------------------------------------+ ↓ Physical Network veth设备的特点 veth和其它的网络设备都一样，一端连接的是内核协议栈 veth设备是成对出现的，另一端两个设备彼此相连 一个设备收到协议栈的数据发送请求后，会将数据发送到另一个设备上去 正因为有这个特性，它常常充当着一个桥梁，连接着各种虚拟网络设备，典型的例子如下： 两个net namespace之间的连接 Bridge、OVS之间的连接 Docker容器之间的连接 不同网络命名空间之间联通方案 OVS是第三方开源的虚拟交换机，功能比Linux Bridge要更强大 veth-pair在虚拟网络中充当着桥梁的角色，连接多种网络设备构成复杂的网络。 veth-pair三个网络联通方案：直接相连、通过Bridge相连和通过OVS相连 直连 直接相连是最简单的方式，如下图，一对veth-pair直接将两个namespace连接在一起 创建测试用net命名空间 ```shell script ip netns a ns0 ip netns a ns1 > 添加`veth0`和`veth1`设备，并配置`veth0 IP`地址，分别加入不同`net`命名空间 ```shell script # 创建veth-pair对 ip link add veth0 type veth peer name veth1 # 分别加入不同命名空间 ip l s veth0 netns ns0 ip l s veth1 netns ns1 # 配置ip地址，并启用 ip netns exec ns0 ip a a 10.10.10.2/24 dev veth0 ip netns exec ns0 ip l s veth0 up ip netns exec ns1 ip a a 10.10.10.3/24 dev veth1 ip netns exec ns1 ip l s veth1 up 互ping ```shell script [root@localhost ~]# ip netns exec ns0 ping -c 4 10.10.10.3 PING 10.10.10.3 (10.10.10.3) 56(84) bytes of data. 64 bytes from 10.10.10.3: icmp_seq=1 ttl=64 time=0.045 ms 64 bytes from 10.10.10.3: icmp_seq=2 ttl=64 time=0.090 ms 64 bytes from 10.10.10.3: icmp_seq=3 ttl=64 time=0.045 ms 64 bytes from 10.10.10.3: icmp_seq=4 ttl=64 time=0.106 ms --- 10.10.10.3 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3030ms rtt min/avg/max/mdev = 0.045/0.071/0.106/0.028 ms [root@localhost ~]# ip netns exec ns1 ping -c 4 10.10.10.2 PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=0.142 ms 64 bytes from 10.10.10.2: icmp_seq=2 ttl=64 time=0.118 ms 64 bytes from 10.10.10.2: icmp_seq=3 ttl=64 time=0.104 ms 64 bytes from 10.10.10.2: icmp_seq=4 ttl=64 time=0.054 ms --- 10.10.10.2 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3009ms rtt min/avg/max/mdev = 0.054/0.104/0.142/0.033 ms > 清除net命名空间 ```shell script ip netns del ns0 ip netns del ns1 通过Bridge相连 当必须连接两个以上的net命名空间(或KVM或LXC实例)时，应使用交换机。 Linux提供了著名的Linux网桥解决方案。 Linux Bridge相当于一台交换机，可以中转多个namespace的流量， 如下图，两对veth-pair分别将两个namespace连到Bridge上。 创建net命名空间 ```shell script add the namespaces ip netns add ns1 ip netns add ns2 ip netns add ns3 > 创建并启用网桥 ```shell script # create the switch yum install bridge-utils -y BRIDGE=br-test brctl addbr $BRIDGE brctl stp $BRIDGE off ip link set dev $BRIDGE up 创建三对veth-pair ```shell script ip l a veth0 type veth peer name br-veth0 ip l a veth1 type veth peer name br-veth1 ip l a veth2 type veth peer name br-veth2 > 分别将三对`veth-pair`加入三个`net`命名空间和`br-test` ```shell script ip l s veth0 netns ns1 ip l s br-veth0 master br-test ip l s br-veth0 up ip l s veth1 netns ns2 ip l s br-veth1 master br-test ip l s br-veth1 up ip l s veth2 netns ns3 ip l s br-veth2 master br-test ip l s br-veth2 up 配置三个ns中的veth-pair的IP并启用 ```shell script ip netns exec ns1 ip a a 10.10.10.2/24 dev veth0 ip netns exec ns1 ip l s veth0 up ip netns exec ns2 ip a a 10.10.10.3/24 dev veth1 ip netns exec ns2 ip l s veth1 up ip netns exec ns3 ip a a 10.10.10.4/24 dev veth2 ip netns exec ns3 ip l s veth2 up > 互`ping` ```shell script [root@localhost ~]# ip netns exec ns1 ping -c 1 10.10.10.3 PING 10.10.10.3 (10.10.10.3) 56(84) bytes of data. 64 bytes from 10.10.10.3: icmp_seq=1 ttl=64 time=0.103 ms --- 10.10.10.3 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.103/0.103/0.103/0.000 ms [root@localhost ~]# ip netns exec ns1 ping -c 1 10.10.10.4 PING 10.10.10.4 (10.10.10.4) 56(84) bytes of data. 64 bytes from 10.10.10.4: icmp_seq=1 ttl=64 time=0.118 ms --- 10.10.10.4 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.118/0.118/0.118/0.000 ms [root@localhost ~]# ip netns exec ns2 ping -c 1 10.10.10.2 PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=0.044 ms --- 10.10.10.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.044/0.044/0.044/0.000 ms [root@localhost ~]# ip netns exec ns2 ping -c 1 10.10.10.4 PING 10.10.10.4 (10.10.10.4) 56(84) bytes of data. 64 bytes from 10.10.10.4: icmp_seq=1 ttl=64 time=0.064 ms --- 10.10.10.4 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.064/0.064/0.064/0.000 ms [root@localhost ~]# ip netns exec ns3 ping -c 1 10.10.10.2 PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=0.042 ms --- 10.10.10.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.042/0.042/0.042/0.000 ms [root@localhost ~]# ip netns exec ns3 ping -c 1 10.10.10.3 PING 10.10.10.3 (10.10.10.3) 56(84) bytes of data. 64 bytes from 10.10.10.3: icmp_seq=1 ttl=64 time=0.096 ms --- 10.10.10.3 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.096/0.096/0.096/0.000 ms 清理测试用例 ```shell script ip netns del ns1 ip netns del ns2 ip netns del ns3 ip l s br-test down brctl delbr br-test ### 通过OVS相连 `OVS`是第三方开源的`Bridge`，功能比`Linux Bridge`要更强大 `OVS`有两种方案实现多命名空间网络互通，一种方式为`veth-pair`方式，类似`Linux Bridge`实现 ![](images/linuxswitch-ovs-veth.png) 另一种解决方案是使用`openvswitch`，并利用`openvswitch`的内部端口。 这避免了在所有其他解决方案中必须使用的`veth`对的使用 ![](images/linuxswitch-ovs.png) 关于第二种方式实现 > 关闭`selinux` ```shell script setenforce 0 sed -i \"s#SELINUX=enforcing#SELINUX=disabled#g\" /etc/selinux/config 安装OVS编译依赖 ```shell script yum install -y python-six selinux-policy-devel gcc make \\ python-devel openssl-devel kernel-devel graphviz kernel-debug-devel autoconf \\ automake rpm-build redhat-rpm-config libtool wget net-tools > 编译安装`OVS` - [openvswitch-2.5.4.tar.gz](http://openvswitch.org/releases/openvswitch-2.5.4.tar.gz) ```shell script mkdir -p ~/rpmbuild/SOURCES tar -zxvf openvswitch-2.5.4.tar.gz cp openvswitch-2.5.4.tar.gz ~/rpmbuild/SOURCES/ ls /lib/modules/$(uname -r) -ln rpmbuild -bb --without check openvswitch-2.5.4/rhel/openvswitch.spec cd rpmbuild/RPMS/x86_64/ yum localinstall -y openvswitch-2.5.4-1.x86_64.rpm systemctl enable openvswitch.service systemctl start openvswitch.service 创建net命名空间 ```shell script ip netns add ns1 ip netns add ns2 ip netns add ns3 > 创建`osv`虚拟交换机 ```shell script ovs-vsctl add-br ovs-br0 创建ovs port ```shell script PORT 1 create an internal ovs port ovs-vsctl add-port ovs-br0 tap1 -- set Interface tap1 type=internal attach it to namespace ip link set tap1 netns ns1 set the ports to up ip netns exec ns1 ip link set dev tap1 up # PORT 2 create an internal ovs port ovs-vsctl add-port ovs-br0 tap2 -- set Interface tap2 type=internal attach it to namespace ip link set tap2 netns ns2 set the ports to up ip netns exec ns2 ip link set dev tap2 up PORT 3 create an internal ovs port ovs-vsctl add-port ovs-br0 tap3 -- set Interface tap3 type=internal attach it to namespace ip link set tap3 netns ns3 set the ports to up ip netns exec ns3 ip link set dev tap3 up > 配置ip并启用 ```shell script ip netns exec ns1 ip a a 10.10.10.2/24 dev tap1 ip netns exec ns1 ip l s tap1 up ip netns exec ns2 ip a a 10.10.10.3/24 dev tap2 ip netns exec ns2 ip l s tap2 up ip netns exec ns3 ip a a 10.10.10.4/24 dev tap3 ip netns exec ns3 ip l s tap3 up 分别将三对veth-pair加入三个net命名空间和br-test ```shell script ip l s veth0 netns ns1 ip l s br-veth0 master br-test ip l s br-veth0 up ip l s veth1 netns ns2 ip l s br-veth1 master br-test ip l s br-veth1 up ip l s veth2 netns ns3 ip l s br-veth2 master br-test ip l s br-veth2 up > 互`ping` ```shell script [root@localhost ~]# ip netns exec ns1 ping -c 1 10.10.10.3 PING 10.10.10.3 (10.10.10.3) 56(84) bytes of data. 64 bytes from 10.10.10.3: icmp_seq=1 ttl=64 time=0.103 ms --- 10.10.10.3 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.103/0.103/0.103/0.000 ms [root@localhost ~]# ip netns exec ns1 ping -c 1 10.10.10.4 PING 10.10.10.4 (10.10.10.4) 56(84) bytes of data. 64 bytes from 10.10.10.4: icmp_seq=1 ttl=64 time=0.118 ms --- 10.10.10.4 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.118/0.118/0.118/0.000 ms [root@localhost ~]# ip netns exec ns2 ping -c 1 10.10.10.2 PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=0.044 ms --- 10.10.10.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.044/0.044/0.044/0.000 ms [root@localhost ~]# ip netns exec ns2 ping -c 1 10.10.10.4 PING 10.10.10.4 (10.10.10.4) 56(84) bytes of data. 64 bytes from 10.10.10.4: icmp_seq=1 ttl=64 time=0.064 ms --- 10.10.10.4 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.064/0.064/0.064/0.000 ms [root@localhost ~]# ip netns exec ns3 ping -c 1 10.10.10.2 PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=0.042 ms --- 10.10.10.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.042/0.042/0.042/0.000 ms [root@localhost ~]# ip netns exec ns3 ping -c 1 10.10.10.3 PING 10.10.10.3 (10.10.10.3) 56(84) bytes of data. 64 bytes from 10.10.10.3: icmp_seq=1 ttl=64 time=0.096 ms --- 10.10.10.3 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.096/0.096/0.096/0.000 ms 清除测试用例 ```shell script ip netns del ns1 ip netns del ns2 ip netns del ns3 ip l s ovs-br0 down ovs-vsctl del-br ovs-br0 ## docker网络浅析 ### 容器网络互通原理 > 启动测试容器 ```shell script docker run -itd --name test1 busybox docker run -itd --name test2 busybox 查看容器ip地址 ```shell script [root@localhost ~]# docker exec test1 ip a 1: lo: mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 4: eth0@if5: mtu 1500 qdisc noqueue link/ether 02:42:ac:50:00:02 brd ff:ff:ff:ff:ff:ff inet 172.80.0.2/24 brd 172.80.0.255 scope global eth0 valid_lft forever preferred_lft forever [root@localhost ~]# docker exec test1 ip a 1: lo: mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 4: eth0@if5: mtu 1500 qdisc noqueue link/ether 02:42:ac:50:00:02 brd ff:ff:ff:ff:ff:ff inet 172.80.0.2/24 brd 172.80.0.255 scope global eth0 valid_lft forever preferred_lft forever > 互`Ping` ```shell script [root@localhost ~]# docker exec test2 ping -c 2 172.80.0.2 PING 172.80.0.2 (172.80.0.2): 56 data bytes 64 bytes from 172.80.0.2: seq=0 ttl=64 time=0.080 ms 64 bytes from 172.80.0.2: seq=1 ttl=64 time=0.086 ms --- 172.80.0.2 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.080/0.083/0.086 ms [root@localhost ~]# docker exec test1 ping -c 2 172.80.0.3 PING 172.80.0.3 (172.80.0.3): 56 data bytes 64 bytes from 172.80.0.3: seq=0 ttl=64 time=0.055 ms 64 bytes from 172.80.0.3: seq=1 ttl=64 time=0.153 ms --- 172.80.0.3 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.055/0.104/0.153 ms 此时两个容器互通 ping内网其他主机 ```shell script [root@localhost ~]# docker exec test1 ping -c 2 192.168.2.78 PING 192.168.2.78 (192.168.2.78): 56 data bytes 64 bytes from 192.168.2.78: seq=0 ttl=127 time=3.494 ms 64 bytes from 192.168.2.78: seq=1 ttl=127 time=3.007 ms --- 192.168.2.78 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 3.007/3.250/3.494 ms [root@localhost ~]# docker exec test2 ping -c 2 192.168.2.78 PING 192.168.2.78 (192.168.2.78): 56 data bytes 64 bytes from 192.168.2.78: seq=0 ttl=127 time=3.301 ms 64 bytes from 192.168.2.78: seq=1 ttl=127 time=3.442 ms --- 192.168.2.78 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 3.301/3.371/3.442 ms > 连通原理解析 其实原理也是类似于上一节所说。实际上就是新建了一对`veth`，将网络打通了。 - 1、容器里能访问外网原理：是因为有一对`veth`,一端连着容器，一端连着主机的`docker0`， 这样容器就能共用主机的网络了，当容器访问外网时，就会通过`NAT`进行地址转换，实际是通过`iptables`来实现的。 - 2、容器间互通原理：每个容器创建时会生成一对`veth`,一端连着容器，一端连着`docker0`网络，这样两个容器都连着`docker0`，他们就可以互相通信了。 `docker0`相当于上述的`Linux Bridge` 如图所示： ![](images/docker0-br.png) > 清理测试用例 ```shell script docker rm -f test1 docker rm -f test2 网络隔离原理 创建测试网络 ```shell script docker network create net-1 docker network create net-2 > 启动容器 ```shell script docker run -itd --name test1 --network=net-1 busybox docker run -itd --name test2 --network=net-2 busybox 查看IP ```shell script [root@localhost ~]# docker exec test1 ip a 1: lo: mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 10: eth0@if11: mtu 1500 qdisc noqueue link/ether 02:42:ac:50:01:02 brd ff:ff:ff:ff:ff:ff inet 172.80.1.2/24 brd 172.80.1.255 scope global eth0 valid_lft forever preferred_lft forever [root@localhost ~]# docker exec test2 ip a 1: lo: mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 12: eth0@if13: mtu 1500 qdisc noqueue link/ether 02:42:ac:50:02:02 brd ff:ff:ff:ff:ff:ff inet 172.80.2.2/24 brd 172.80.2.255 scope global eth0 valid_lft forever preferred_lft forever > 互`Ping` ```shell script [root@localhost ~]# docker exec test1 ping -c 2 -i 1 172.80.2.2 ^C [root@localhost ~]# docker exec test1 ping -c 2 -W 1 172.80.2.2 PING 172.80.2.2 (172.80.2.2): 56 data bytes --- 172.80.2.2 ping statistics --- 2 packets transmitted, 0 packets received, 100% packet loss [root@localhost ~]# docker exec test2 ping -c 2 -W 1 172.80.1.2 PING 172.80.1.2 (172.80.1.2): 56 data bytes --- 172.80.1.2 ping statistics --- 2 packets transmitted, 0 packets received, 100% packet loss 由于归属不同网桥，网络不通 Ping外部主机 ```shell script [root@localhost ~]# docker exec test2 ping -c 2 -W 1 192.168.2.78 PING 192.168.2.78 (192.168.2.78): 56 data bytes 64 bytes from 192.168.2.78: seq=0 ttl=127 time=3.282 ms 64 bytes from 192.168.2.78: seq=1 ttl=127 time=2.960 ms --- 192.168.2.78 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 2.960/3.121/3.282 ms [root@localhost ~]# docker exec test1 ping -c 2 -W 1 192.168.2.78 PING 192.168.2.78 (192.168.2.78): 56 data bytes 64 bytes from 192.168.2.78: seq=0 ttl=127 time=3.984 ms 64 bytes from 192.168.2.78: seq=1 ttl=127 time=3.429 ms --- 192.168.2.78 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 3.429/3.706/3.984 ms 由此可见，通过不同网桥实现了网络隔离 > 清理测试用例 ```shell script docker rm -f test1 docker rm -f test2 关于docker网络具体实现，这里不做过多讨论。 参考文献 Linux Switching – Interconnecting Namespaces Linux网络命名空间 linux中的网络命名空间的使用 LINUX 内核网络设备——VETH 设备和 NETWORK NAMESPACE 初步 Linux 虚拟网络设备 veth-pair 详解，看这一篇就够了 为什么docker容器之间能互通？为什么容器里能访问外网？ 模拟 Docker网桥连接外网 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/容器原理/ns-user.html":{"url":"2.容器/容器原理/ns-user.html","title":"ns-user","keywords":"","body":"USER命名空间 USER namespace有什么能力？ 提供用户隔离能力，隔离用户的用户ID与用户组ID 使用场景 在宿主机上以一个非root用户运行创建一个User namespace，然后在User namespace里面却映射成root用户 这样意味着，这个进程在User namespace里面有root权限，但是在User namespace外面却没有root的权限 重映射容器内用户Uid至宿主机 启动nginx docker run -itd --name nginx nginx 获取nginx容器pid [root@localhost 4609]# ps -ef|grep nginx root 46991 46971 0 07:58 pts/0 00:00:00 nginx: master process nginx -g daemon off; 101 47051 46991 0 07:58 pts/0 00:00:00 nginx: worker process 101 47052 46991 0 07:58 pts/0 00:00:00 nginx: worker process 101 47053 46991 0 07:58 pts/0 00:00:00 nginx: worker process 101 47054 46991 0 07:58 pts/0 00:00:00 nginx: worker process root 48582 4609 0 07:59 pts/0 00:00:00 grep --color=auto nginx 进入nginx容器进程空间 cd /proc/46991 查看uid_map属性 [root@localhost 46991]# cat uid_map 0 0 4294967295 第一列字段表示在容器显示的UID或GID 第二列字段表示容器外映射的真实的UID或GID 第三个字段表示映射的范围 如果为1，表示一一对应（内部与外部uid一一对应） 如果为4294967295，表示把namespace内部从0开始的uid映射到外部从0开始的uid， 其最大范围是无符号32位整形 上述nginx进程表示，容器内的nginx: master用户为root权限，即在容器外部也有root权限 docker启用用户命名空间 由上述步骤我们可知，默认docker未启用用户命名空间，容器内的uid与宿主机一致 如容器内使用root（uid=0）启动服务，有安全风险（宿主机视角也是root用户） 修改系统参数 ```shell script sed -i \"/user.max_user_namespaces/d\" /etc/sysctl.conf echo \"user.max_user_namespaces=15511\" >> /etc/sysctl.conf sysctl -p > 编辑配置文件 ```shell script vi /etc/docker/daemon.json 添加参数\"userns-remap\": \"default\", 参考配置： { \"log-opts\": { \"max-size\": \"5m\", \"max-file\":\"3\" }, \"userns-remap\": \"default\", \"exec-opts\": [\"native.cgroupdriver=systemd\"] } 重载服务 ```shell script systemctl daemon-reload systemctl restart docker > 启动一个容器 ```shell script docker rm -f nginx docker run -itd --name nginx nginx 查看容器内进程用户 shell script [root@localhost ~]# ps -p $(docker inspect --format='{{.State.Pid}}' $(docker ps |grep ccc|awk '{print $1}')) -o pid,user PID USER 2535 100000 参考文献 DOCKER基础技术：LINUX NAMESPACE（下） Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/容器原理/ns-uts.html":{"url":"2.容器/容器原理/ns-uts.html","title":"ns-uts","keywords":"","body":"UTS命名空间 概念 UTS namespace有什么能力？ UTS(UNIX Time-sharing System) 命名空间提供了主机名（hostname）和域名（/etc/hosts解析）的隔离。 能够使得子进程有独立的主机名和域名(hostname) 这一特性在Docker容器技术中被用到，使得docker容器在网络上被视作一个独立的节点，而不仅仅是宿主机上的一个进程 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/容器原理/ns.html":{"url":"2.容器/容器原理/ns.html","title":"ns","keywords":"","body":"命名空间 USER-用户命名空间 UTS-用户命名空间 IPC-进程通信命名空间 MNT-挂载点命名空间 NET-网络命名空间 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/镜像构建/buildah.html":{"url":"2.容器/镜像构建/buildah.html","title":"buildah","keywords":"","body":"buildah 安装 编译安装 安装依赖 ```shell script yum -y install \\ make \\ gcc \\ golang \\ bats \\ btrfs-progs-devel \\ device-mapper-devel \\ glib2-devel \\ gpgme-devel \\ libassuan-devel \\ libseccomp-devel \\ git \\ bzip2 \\ go-md2man \\ runc \\ skopeo-containers ```shell script mkdir ~/buildah cd ~/buildah export GOPATH=`pwd` git clone https://github.com/containers/buildah ./src/github.com/containers/buildah cd ./src/github.com/containers/buildah make sudo make install buildah --help 安装binary 下载解压 buildah-release-1.21-linux-amd64.tar.gz shell script tar zxvf buildah-release-1.21-linux-amd64.tar.gz cp buildah-release-1.21-linux-amd64/bin/buildah /usr/bin Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/镜像构建/podman.html":{"url":"2.容器/镜像构建/podman.html","title":"podman","keywords":"","body":"podman Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"2.容器/镜像构建/skopeo.html":{"url":"2.容器/镜像构建/skopeo.html","title":"skopeo","keywords":"","body":"skopeo 项目地址 安装 编译安装 安装 ```shell script yum install -y btrfs-progs-devel gpgme-devel device-mapper-devel libassuan-devel git clone https://github.com/containers/skopeo.git cd skopeo make bin/skopeo cp bin/skopeo /usr/local/bin chmod +x /usr/local/bin/skopeo ### 预编译包 - [skopeo-1.3.1-linux-amd64.tar.gz](https://github.com/weiliang-ms/skopeo/releases/download/v1.3.1/skopeo-1.3.1-linux-amd64.tar.gz) > 下载安装 ```shell script wget https://github.com/weiliang-ms/skopeo/releases/download/v1.3.1/skopeo-1.3.1-linux-amd64.tar.gz tar zxvf skopeo-1.3.1-linux-amd64.tar.gz cp skopeo-1.3.1-linux-amd64/bin/skopeo /usr/local/bin chmod +x /usr/local/bin/skopeo dir格式批量导出/导入 文件层级 ```shell script harbor-export ├── download.sh ├── image-list.txt └── upload.sh > `image-list.txt` ```shell script harbor.wl.com/kubernetes/csi-snapshotter:v3.0.2 harbor.wl.com/kubernetes/csi-attacher:v3.0.2 harbor.wl.com/kubernetes/csi-node-driver-registrar:v2.0.1 harbor.wl.com/kubernetes/csi-provisioner:v2.0.4 harbor.wl.com/kubernetes/csi-resizer:v1.0.1 harbor.wl.com/kubernetes/cephfs-provisioner:latest harbor.wl.com/kubernetes/cephcsi:v3.2.0 harbor.wl.com/kubernetes/cephcsi:v3.2.1 harbor.wl.com/kubernetes/node:v3.15.1 harbor.wl.com/kubernetes/cni:v3.15.1 harbor.wl.com/kubernetes/pod2daemon-flexvol:v3.15.1 harbor.wl.com/kubernetes/kube-controllers:v3.15.1 harbor.wl.com/kubernetes/coredns:1.6.9 导出脚本download.sh ```shell script !/bin/bash GREEN_COL=\"\\033[32;1m\" RED_COL=\"\\033[1;31m\" NORMAL_COL=\"\\033[0;39m\" SOURCE_REGISTRY=harbor.wl.com REGISTRY_USER=admin REGISTRY_PASS=Harbor-12345 TARGET_REGISTRY=\"\" PROJECT_NAME=$1 IMAGES_DIR=$PROJECT_NAME/images : ${IMAGES_DIR:=\"images\"} : ${IMAGES_LIST_FILE:=\"$PROJECT_NAME/image-list.txt\"} : ${TARGET_REGISTRY:=\"hub.k8s.li\"} : ${SOURCE_REGISTRY:=\"harbor.chs.neusoft.com\"} BLOBS_PATH=\"$PROJECT_NAME/docker/registry/v2/blobs/sha256\" REPO_PATH=\"$PROJECT_NAME/docker/registry/v2/repositories\" set -eo pipefail CURRENT_NUM=0 TOTAL_NUMS=$(cat \"$IMAGES_LIST_FILE\" | wc -l) skopeo_sync() { mkdir -p $2/$1 if skopeo sync --all --insecure-policy --src-tls-verify=false --dest-tls-verify=false \\ --override-arch amd64 --override-os linux --src docker --dest dir $1 $2 > /dev/null; then echo -e \"$GREEN_COL Progress: ${CURRENT_NUM}/${TOTAL_NUMS} sync $1 to $2 successful $NORMAL_COL\" else echo -e \"$RED_COL Progress: ${CURRENT_NUM}/${TOTAL_NUMS} sync $1 to $2 failed $NORMAL_COL\" exit 2 fi } if [ -d $IMAGES_DIR ];then rm -rf $IMAGES_DIR fi mkdir -p $IMAGES_DIR while read line do let CURRENT_NUM=${CURRENT_NUM}+1 skopeo_sync ${line} $IMAGES_DIR done convert_images() { rm -rf ${IMAGES_DIR}; mkdir -p ${IMAGES_DIR} while read image do let CURRENT_NUM=${CURRENT_NUM}+1 image=echo $image |sed \"s#harbor.chs.neusoft.com/##g\" image_name=${image%%:} image_tag=${image##:} image_repo=${image%%/*} echo \"image-name -> $image_name image-tag -> $image_tag image-repo -> $image_repo\" mkdir -p ${IMAGES_DIR}/${image_repo} skopeo_sync ${SOURCE_REGISTRY}/${image} ${IMAGES_DIR}/${image_repo} manifest=\"${IMAGES_DIR}/${image}/manifest.json\" manifest_sha256=$(sha256sum ${manifest} | awk '{print $1}') mkdir -p ${BLOBS_PATH}/${manifest_sha256:0:2}/${manifest_sha256} ln -f ${manifest} ${BLOBS_PATH}/${manifest_sha256:0:2}/${manifest_sha256}/data make image repositories dir mkdir -p ${REPO_PATH}/${image_name}/{_uploads,_layers,_manifests} mkdir -p ${REPO_PATH}/${image_name}/_manifests/revisions/sha256/${manifest_sha256} mkdir -p ${REPO_PATH}/${image_name}/_manifests/tags/${image_tag}/{current,index/sha256} mkdir -p ${REPO_PATH}/${image_name}/_manifests/tags/${image_tag}/index/sha256/${manifest_sha256} create image tag manifest link file echo -n \"sha256:${manifest_sha256}\" > ${REPO_PATH}/${image_name}/_manifests/tags/${image_tag}/current/link echo -n \"sha256:${manifest_sha256}\" > ${REPO_PATH}/${image_name}/_manifests/revisions/sha256/${manifest_sha256}/link echo -n \"sha256:${manifest_sha256}\" > ${REPO_PATH}/${image_name}/_manifests/tags/${image_tag}/index/sha256/${manifest_sha256}/link link image layers file to registry blobs dir for layer in $(sed '/v1Compatibility/d' ${manifest} | grep -Eo \"\\b[a-f0-9]{64}\\b\"); do mkdir -p ${BLOBS_PATH}/${layer:0:2}/${layer} mkdir -p ${REPO_PATH}/${image_name}/_layers/sha256/${layer} echo -n \"sha256:${layer}\" > ${REPO_PATH}/${image_name}/_layers/sha256/${layer}/link ln -f ${IMAGES_DIR}/${image}/${layer} ${BLOBS_PATH}/${layer:0:2}/${layer}/data done done rm -rf ${IMAGES_DIR} } convert_images 使用方式 ```shell sh download.sh library 导入脚本upload.sh ```shell script !/bin/bash REGISTRY_DOMAIN=\"harbor.wl.com\" 切换到 registry 存储主目录下 gen_skopeo_dir() { 定义 registry 存储的 blob 目录 和 repositories 目录，方便后面使用 BLOB_DIR=\"docker/registry/v2/blobs/sha256\" REPO_DIR=\"docker/registry/v2/repositories\" # 定义生成 skopeo 目录 SKOPEO_DIR=\"docker/skopeo\" # 通过 find 出 current 文件夹可以得到所有带 tag 的镜像，因为一个 tag 对应一个 current 目录 for image in $(find ${REPO_DIR} -type d -name \"current\"); do # 根据镜像的 tag 提取镜像的名字 name=$(echo ${image} | awk -F '/' '{print $5\"/\"$6\":\"$9}') link=$(cat ${image}/link | sed 's/sha256://') mfs=\"${BLOB_DIR}/${link:0:2}/${link}/data\" # 创建镜像的硬链接需要的目录 mkdir -p \"${SKOPEO_DIR}/${name}\" # 硬链接镜像的 manifests 文件到目录的 manifest 文件 ln ${mfs} ${SKOPEO_DIR}/${name}/manifest.json # 使用正则匹配出所有的 sha256 值，然后排序去重 layers=$(grep -Eo \"\\b[a-f0-9]{64}\\b\" ${mfs} | sort -n | uniq) for layer in ${layers}; do # 硬链接 registry 存储目录里的镜像 layer 和 images config 到镜像的 dir 目录 ln ${BLOB_DIR}/${layer:0:2}/${layer}/data ${SKOPEO_DIR}/${name}/${layer} done done } sync_image() { # 使用 skopeo sync 将 dir 格式的镜像同步到 harbor for project in $(ls ${SKOPEO_DIR}); do skopeo sync --insecure-policy --src-tls-verify=false --dest-tls-verify=false \\ --src dir --dest docker ${SKOPEO_DIR}/${project} ${REGISTRY_DOMAIN}/${project} done } gen_skopeo_dir sync_image > 登录 ```shell script skopeo login harbor.wl.com --tls-verify=false 执行导入 shell script sh upload.sh Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"3.集成部署/":{"url":"3.集成部署/","title":"3.集成部署","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:03 "},"3.集成部署/ftp/":{"url":"3.集成部署/ftp/","title":"ftp","keywords":"","body":"ftp 安装ftp yum install -y vsftpd 创建用户 useradd -s /sbin/nologin ftpuser passwd ftpuser 目录赋权 chown -R ftpuser:ftpuser /data 配置vsftp服务 sed -i \"s/anonymous_enable=YES/anonymous_enable=NO/g\" /etc/vsftpd/vsftpd.conf cat >> /etc/vsftpd/vsftpd.conf 添加用户 cat > /etc/vsftpd/user_list 启动 systemctl enable vsftpd --now Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"3.集成部署/git/":{"url":"3.集成部署/git/","title":"git","keywords":"","body":"gitlab安装 添加源 ```shell script cat> /etc/yum.repos.d/gitlab-ce.repohttps://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/gpgcheck=0 enabled=1 EOF > 安装 ```shell script yum install curl policycoreutils-python openssh-server gitlab-ce -y 调整配置 精简配置 调整/etc/gitlab/gitlab.rb external_url(若80被占用建议使用8888) 数据目录 ```shell script git_data_dirs({ \"default\" => { \"path\" => \"/data/gitlab/data\" } }) > 重载配置 ```shell script gitlab-ctl reconfigure 启动 ```shell script gitlab-ctl start > 初始化`root`用户 建立连接，需要大约半分钟左右 ```shell script gitlab-rails console 初始化 ```shell script u=User.where(id:1).first u.password='Gitlab@321' u.password_confirmation='Gitlab@321' u.save! quit ## git-cli安装 ### 编译安装 ```shell script curl -L https://mirrors.edge.kernel.org/pub/software/scm/git/git-2.9.5.tar.xz -o ./git-2.9.5.tar.xz -k yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel gcc perl-ExtUtils-MakeMaker -y tar xvf git-2.9.5.tar.xz cd git-2.9.5 ./configure --prefix=/usr/local/git make && make install cat >> ~/.bash_profile Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"3.集成部署/hadoop/":{"url":"3.集成部署/hadoop/","title":"hadoop","keywords":"","body":"hadoop 准备离线资源 mysql-connector-java-5.1.48.tar.gz linux节点配置 假设节点IP为： 主：192.168.1.12 从：192.168.1.13 从：192.168.1.14 设置hostname 节点一执行 cat >> /etc/sysconfig/network /proc/sys/kernel/hostname 节点二执行 cat >> /etc/sysconfig/network /proc/sys/kernel/hostname 节点三执行 cat >> /etc/sysconfig/network /proc/sys/kernel/hostname 配置host解析（三个节点均执行，注意IP替换为实际IP） cat >> /etc/hosts 关闭防火墙、selinux systemctl stop firewalld --now setenforce 0 sed -i \"s#SELINUX=enforcing#SELINUX=disabled#g\" /etc/selinux/config 节点主机互信 master节点执行 ssh-keygen -t rsa -n '' -f ~/.ssh/id_rsa # 根据提示输入对应节点root口令 ssh-copy-id hadoop1 ssh-copy-id hadoop2 ssh-copy-id hadoop3 安装oracle jdk（1.8）并配置软链接(oracle jdk安装至/opt/java下) mkdir -p /usr/java ln -s /opt/java /usr/java/jdk1.8 调整文件句柄数 echo \"* soft nofile 655350\" >> /etc/security/limits.conf echo \"* hard nofile 655350\" >> /etc/security/limits.conf echo \"* soft nproc 65535\" >> /etc/security/limits.conf echo \"* hard nproc 65535\" >> /etc/security/limits.conf ulimit -n 655350 主节点安装Mysql yum安装Marbidb yum install mariadb mariadb-server -y 启动 systemctl enable mariadb --now 初始化用户、数据库 mysql -u root 上传mysql-connector-java-5.1.48.tar.gz 至/tmp目录下，执行以下命令 mkdir -p /usr/share/java tar zxvf mysql-connector-java-5.1.48.tar.gz cp mysql-connector-java-5.1.48/mysql-connector-java-5.1.48.jar /usr/share/java/mysql-connector-java.jar 初始化CM Server数据库 /usr/share/cmf/schema/scm_prepare_database.sh mysql scm_db scm_server scm_server -h 127.0.0.1 创建cloudera-manager本地镜像源（主节点） 安装repo工具 yum install yum-utils createrepo yum-plugin-priorities -y 创建/cm目录，上传安装介质,结构如下 /cm ├── cloudera-manager-agent-5.7.0-1.cm570.p0.76.el7.x86_64.rpm ├── cloudera-manager-daemons-5.7.0-1.cm570.p0.76.el7.x86_64.rpm ├── cloudera-manager-server-5.7.0-1.cm570.p0.76.el7.x86_64.rpm ├── cloudera-manager-server-db-2-5.7.0-1.cm570.p0.76.el7.x86_64.rpm ├── enterprise-debuginfo-5.7.0-1.cm570.p0.76.el7.x86_64.rpm ├── jdk-6u31-linux-amd64.rpm ├── oracle-j2sdk1.7-1.7.0+update67-1.x86_64.rpm └── RPM-GPG-KEY-cloudera 创建repo源文件 cd /cm && createrepo ./ 配置本地cloudera-manager源 cat > /etc/yum.repos.d/cm.repo 主节点上传CDH文件 创建目录(server节点、agent节点均需执行) mkdir -p /opt/cloudera/parcel-repo 上传以下文件至主节点/opt/cloudera/parcel-repo下 CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.sha1 manifest.json 生成CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.torrent.sha cd /opt/cloudera/parcel-repo sha1sum CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.torrent | awk '{print $1}'> CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.torrent.sha 修改CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.sha1名 cd /opt/cloudera/parcel-repo mv CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.sha1 CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.sha 分发至agent节点 scp /opt/cloudera/parcel-repo/* hadoop2:/opt/cloudera/parcel-repo/ scp /opt/cloudera/parcel-repo/* hadoop3:/opt/cloudera/parcel-repo/ 安装Cloudera Manager Server端 yum安装cloudera-manager yum install cloudera-manager-daemons cloudera-manager-server -y 安装Cloudera Manager Agent端（所有agent节点） 拷贝资源文件 主节点拷贝以下内容至agent节点 scp -r /cm hadoop2:/ scp -r /cm hadoop3:/ scp /etc/yum.repos.d/cm.repo hadoop2:/etc/yum.repos.d/ scp /etc/yum.repos.d/cm.repo hadoop3:/etc/yum.repos.d/ 安装agent（agent节点运行） yum install cloudera-manager-agent -y 修改agent配置文件 修改文件/etc/cloudera-scm-agent/config.ini以下内容 server_host=localhost # listening_ip # listening_hostname= 启动agent systemctl start cloudera-scm-agent 启动CM Server端 启动 systemctl start cloudera-scm-server 访问WEB UI （主节点7180端口） 登录账号：admin/admin 接受协议 部署免费版本 确认部署应用，点击继续 添加部署节点，点击搜索 选取节点，继续 确认cdh版本，继续 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"3.集成部署/mysql/":{"url":"3.集成部署/mysql/","title":"mysql","keywords":"","body":" 集成部署 单机集成部署 添加slave节点 cli命令 查看连接 配置优化 连接数 暂存连接数 缓冲区变量 防止暴力破解 限制数据包大小 使用教程 慢查询 查看变量 查看锁性能 查看连接数 查看回滚数量 查询运行时间 查询缓存状态 查看连接信息 查询表使用状态 查看增删改数量 修改密码 binlog 集成部署 较全的教程 单机集成部署 适用于CentOS Red Hat 官网 5.5 下载 官网 5.6 下载 官网 5.7 下载 版本信息 5.7.27社区版 配置yum源 配置基础yum源即可无需epel源 配置阿里云源（保证网络可达） Centos-5.repo Centos-6.repo Centos-7.repo 安装卸载依赖 yum install net-tools -y yum remove mysql* -y centos7需要卸载mariadb yum remove -y mariadb-libs 上传安装包至/tmp下，进行安装 cd /tmp rpm -ivh mysql-community-common-*.el7.x86_64.rpm rpm -ivh mysql-community-libs-*.el7.x86_64.rpm rpm -ivh mysql-community-client-*.el7.x86_64.rpm rpm -ivh mysql-community-server-*.el7.x86_64.rpm 配置 echo \"default-storage-engine=INNODB\" >>/etc/my.cnf echo \"character-set-server=utf8\" >>/etc/my.cnf echo \"collation-server=utf8_general_ci\" >>/etc/my.cnf echo \"lower_case_table_names=1\" >>/etc/my.cnf 启动 centos7 systemctl daemon-reload systemctl enable mysqld --now centos6 service mysqld start chkconfig mysqld on 修改防火墙、SElinux策略 firewall-cmd --permanent --zone=public --add-port=3306/tcp firewall-cmd --reload setenforce 0 修改root密码 password=`grep 'temporary password' /var/log/mysqld.log|awk '{print $NF}'|awk 'END {print}'` mysql -uroot -p$password --connect-expired-password 添加slave节点 1、确认主节点版本 2、从节点安装相同版本mysql 3、更换默认存储目录（可选） systemctl stop mysqld.service mkdir -p /data/mysql chown -R mysql.mysql /data/mysql cp -a /var/lib/mysql/* /data/mysql/ sed -i \"s#/var/lib/mysql#/data/mysql#g\" /etc/my.cnf cat >> /etc/my.cnf 4、初始化密码 password=`grep 'temporary password' /var/log/mysqld.log|awk '{print $NF}'|awk 'END {print}'` mysql -uroot -p$password --connect-expired-password 5、调整主库参数 原有主库配置参数如下： server-id = 1 #id要唯一 log-bin = mysql-bin #开启binlog日志 auto-increment-increment = 1 #在Ubuntu系统中MySQL5.5以后已经默认是1 auto-increment-offset = 1 slave-skip-errors = all #跳过主从复制出现的错误 主库创建同步账号 grant all on *.* to 'sync'@'192.168.%.%' identified by 'sync'; 6、从库配置MySQL server-id = 3 #这个设置3 log-bin = mysql-bin #开启binlog日志 auto-increment-increment = 1 #这两个参数在Ubuntu系统中MySQL5.5以后都已经默认是1 auto-increment-offset = 1 slave-skip-errors = all #跳过主从复制出现的错误 update mysql.user set authentication_string=password('1qaz#EDC') where user='root'; mysqldump -h 192.168.174.30 -p3306 -uroot -p1qaz#EDC --all-databases > /root/all_db.sql cli命令 查看连接 查看当前连接数 show status like 'Threads%'; 查看最大连接数 show variables like '%max_connections%'; 查看显示连接状态 SHOW STATUS LIKE '%connect%'; 查看当前所有连接 show full processlist; 配置优化 参考地址 连接数 查看最大连接数，默认151 show VARIABLES like 'max_connections'; +-----------------+-------+ | Variable_name | Value | +-----------------+-------+ | max_connections | 151 | +-----------------+-------+ 1 row in set (0.00 sec) 查看当前连接数 SHOW STATUS LIKE 'max_used_connections'; # 理想值约等于85% max_used_connections/max_connections*100% 配置方式 #客户端命令行 set GLOBAL max_connections=2000; set GLOBAL max_user_connections=1500; #配置文件 [mysqld] max_connections = 1000 max_user_connections=1500 暂存连接数 MySQL能够暂存的连接数量。 当主要MySQL线程在一个很短时间内得到非常多的连接请求，他就会起作用。 如果MySQL的连接数据达到max_connections时，新的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈数量即back_log，如果等待连接的数量超过back_log，将不被接受连接资源。 show VARIABLES like 'back_log'; mysql> show VARIABLES like 'back_log'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | back_log | 80 | +---------------+-------+ 1 row in set (0.00 sec) back_log值不能超过TCP/IP连接的侦听队列的大小。若超过则无效，查看当前系统的TCP/IP连接的侦听队列的大小命令（默认128） cat /proc/sys/net/ipv4/tcp_max_syn_backlog 配置方式 echo \"net.ipv4.tcp_max_syn_backlog = 8192\" >> /etc/sysctl.conf sysctl -p #配置文件 [mysqld] back_log=128 缓冲区变量 1、key_buffer_size 默认的配置数时8388608（8M），主机有4G内存可以调优值为268435456（256M） 通过检查状态值 key_read_requests和key_reads，可以知道key_buffer_size设置是否合理。 比例key_reads/key_read_requests应该尽可能的低，至少是1：100，1：1000更好（上述状态值可以使用show status like ‘key_read%'获得） mysql> show variables like 'key_buffer_size'; +-----------------+---------+ | Variable_name | Value | +-----------------+---------+ | key_buffer_size | 8388608 | +-----------------+---------+ 1 row in set (0.00 sec) mysql> show status like 'key_read%'; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | Key_read_requests | 33 | | Key_reads | 8 | +-------------------+-------+ 2 rows in set (0.00 sec) set global key_buffer_size = 256*1024*1024; 2、query_cache_size 使用查询缓存，MySQL将查询结果存放在缓冲区中，今后对同样的select语句（区分大小写），将直接从缓冲区中读取结果。 一个SQL查询如果以select开头，那么MySQL服务器将尝试对其使用查询缓存。 注：两个SQL语句，只要相差哪怕是一个字符（例如 大小写不一样：多一个空格等），那么两个SQL将使用不同的cache 通过 show status like 'Qcache%'; 可以知道query_cache_size的设置是否合理 mysql> show status like 'Qcache%'; +-------------------------+---------+ | Variable_name | Value | +-------------------------+---------+ | Qcache_free_blocks | 1 | | Qcache_free_memory | 1031832 | | Qcache_hits | 0 | | Qcache_inserts | 0 | | Qcache_lowmem_prunes | 0 | | Qcache_not_cached | 9 | | Qcache_queries_in_cache | 0 | | Qcache_total_blocks | 1 | +-------------------------+---------+ 8 rows in set (0.00 sec) 3、sort_buffer_size 每个需要排序的线程分配该大小的一个缓冲区。增加这值加速ORDER BY 或 GROUP BY操作 sort_buffer_size是一个connection级的参数，在每个connection（session）第一次需要使用这个buffer的时候，一次性分配设置的内存。 sort_buffer_size并不是越大越好，由于是connection级的参数，过大的设置+高并发可能会耗尽系统的内存资源。例如：500个连接将会消耗500*sort_buffer_size(2M)=1G 默认0.25M set global sort_buffer_size = 1 *1024 * 1024; join_buffer_size 用于表示关联缓存的大小，和sort_buffer_size一样，该参数对应的分配内存也是每个连接独享。 set global join_buffer_size = 1 *1024 * 1024; 4、thread_cache_size 服务器线程缓存，这个值表示可以重新利用保存在缓存中的线程数量， 当断开连接时，那么客户端的线程将被放到缓存中以响应下一个客户而不是销毁（前提时缓存数未达上限），如果线程重新被请求，那么请求将从缓存中读取，如果缓存中是空的或者是新的请求，这个线程将被重新请求，那么这个线程将被重新创建，如果有很多新的线程，增加这个值可以改善系统性能，通过比较Connections和Threads_created状态的变量，可以看到这个变量的作用。 默认9 set global thread_cache_size = 100; 可以通过如下几个MySQL状态值来适当调整线程池的大小 Threads_cached : 当前线程池中缓存有多少空闲线程 Threads_connected : 当前的连接数 ( 也就是线程数 ) Threads_created : 已经创建的线程总数 Threads_running : 当前激活的线程数 ( Threads_connected 中的线程有些可能处于休眠状态 ) 可以通过 show global status like 'Threads_%'; 命令查看以上4个状态值 防止暴力破解 max_connect_errors 是一个MySQL中与安全有关的计数器值，他负责阻止过多尝试失败的客户端以防止暴力破解密码的情况， 当超过指定次数，MySQL服务器将禁止host的连接请求，直到mysql服务器重启或通过flush hosts命令清空此host的相关信息。 set global max_connect_errors = 20; 限制数据包大小 限制server接受的数据包大小，默认4M。 mysql> show VARIABLES like 'max_allowed_packet'; +--------------------+---------+ | Variable_name | Value | +--------------------+---------+ | max_allowed_packet | 4194304 | +--------------------+---------+ 1 row in set (0.00 sec) set global max_allowed_packet = 32*1024*1024; 使用教程 使用教程1 使用教程2 使用教程3 使用教程4 使用教程5 使用教程6 慢查询 查看查询慢sql配置 show variables like 'slow%'; 开启慢sql set global slow_query_log='ON' 查询慢 SQL 秒数值 show variables like 'long%'; 查看变量 #该语句输出较多 SHOW VARIABLES; SHOW VARIABLES like 'version'; 查看锁性能 锁性能状态： SHOW STATUS LIKE 'innodb_row_lock_%'; mysql> SHOW STATUS LIKE 'innodb_row_lock_%'; +-------------------------------+--------+ | Variable_name | Value | +-------------------------------+--------+ | Innodb_row_lock_current_waits | 0 | | Innodb_row_lock_time | 497180 | | Innodb_row_lock_time_avg | 4075 | | Innodb_row_lock_time_max | 51006 | | Innodb_row_lock_waits | 122 | +-------------------------------+--------+ 5 rows in set (0.00 sec) Innodb_row_lock_current_waits：当前等待锁的数量 Innodb_row_lock_time：系统启动到现在、锁定的总时间长度 Innodb_row_lock_time_avg：每次平均锁定的时间 Innodb_row_lock_time_max：最长一次锁定时间 Innodb_row_lock_waits：系统启动到现在、总共锁定次数 查看连接数 mysql> SHOW STATUS LIKE 'max_used_connections'; +----------------------+-------+ | Variable_name | Value | +----------------------+-------+ | Max_used_connections | 86 | +----------------------+-------+ 1 row in set (0.02 sec) mysql> 查看回滚数量 如果 rollback 过多，说明程序肯定哪里存在问题 SHOW STATUS LIKE '%Com_rollback%'; 查询运行时间 显示MySQL服务启动运行了多少时间，如果MySQL服务重启，该时间重新计算，单位秒 SHOW STATUS LIKE 'uptime'; 查询缓存状态 显示查询缓存的状态情况 SHOW STATUS LIKE 'qcache%'; 查看连接信息 例子出处 SHOW FULL PROCESSLIST; #输出如下 mysql> show processlist; +----+------+----------------------+---------+---------+------+-------+------------------+ | Id | User | Host | db | Command | Time | State | Info | +----+------+----------------------+---------+---------+------+-------+------------------+ | 1 | root | 192.168.20.160:53417 | firefly | Sleep | 50 | | NULL | | 2 | root | localhost | NULL | Query | 0 | init | show processlist | +----+------+----------------------+---------+---------+------+-------+------------------+ 2 rows in set (0.00 sec) mysql> show processlist; +----+------+----------------------+---------+---------+------+--------------+---------------------+ | Id | User | Host | db | Command | Time | State | Info | +----+------+----------------------+---------+---------+------+--------------+---------------------+ | 1 | root | 192.168.20.160:53417 | firefly | Query | 125 | Sending data | SELECT o.order_id, creator_id, '', city_name, order_address, city_id, order_type_description, | | 2 | root | localhost | NULL | Query | 0 | init | show processlist | +----+------+----------------------+---------+---------+------+--------------+-------------------+ 2 rows in set (0.00 sec) id：标识 user：当前用户，如果不是root，这个命令就只显示你权限范围内的sql语句 host：显示执行sql语句的ip地址和端口号，追踪出问题语句的用户 db：显示这个进程目前连接的是哪个数据库 command：显示当前连接的执行的命令，一般就是休眠（sleep），查询（query），连接（connect） time：状态持续的时间，单位是秒。 state，使用当前连接的sql语句的状态，很重要的列。 注意，state只是语句执行中的某一个状态，一个sql语句，已查询为例，可能需要经过copying to tmp table，Sorting result，Sending data等状态才可以完成 info：显示执行的sql语句，因为长度有限，所以长的sql语句就显示不全，但是，是一个判断问题语句的重要依据。 state列 这个命令中最关键的就是state列，mysql列出的状态主要有以下几种，所有状态参考下面官方手册： Checking table 正在检查数据表（这是自动的）。 Closing tables 正在将表中修改的数据刷新到磁盘中，同时正在关闭已经用完的表。这是一个很快的操作，如果不是这样的话，就应该确认磁盘空间是否已经满了或者磁盘是否正处于重负中。 Connect Out 复制从服务器正在连接主服务器。 Copying to tmp table on disk 由于临时结果集大于tmp_table_size，正在将临时表从内存存储转为磁盘存储以此节省内存。 Creating tmp table 正在创建临时表以存放部分查询结果。 deleting from main table 服务器正在执行多表删除中的第一部分，刚删除第一个表。 deleting from reference tables 服务器正在执行多表删除中的第二部分，正在删除其他表的记录。 Flushing tables 正在执行FLUSH TABLES，等待其他线程关闭数据表。 Killed 发送了一个kill请求给某线程，那么这个线程将会检查kill标志位，同时会放弃下一个kill请求。MySQL会在每次的主循环中检查kill标志位，不过有些情况下该线程可能会过一小段才能死掉。如果该线程程被其他线程锁住了，那么kill请求会在锁释放时马上生效。 Locked 被其他查询锁住了。 Sending data 正在处理Select查询的记录，同时正在把结果发送给客户端。 Sorting for group 正在为GROUP BY做排序。 Sorting for order 正在为ORDER BY做排序。 Opening tables 这个过程应该会很快，除非受到其他因素的干扰。例如，在执Alter TABLE或LOCK TABLE语句行完以前，数据表无法被其他线程打开。正尝试打开一个表。 Removing duplicates 正在执行一个Select DISTINCT方式的查询，但是MySQL无法在前一个阶段优化掉那些重复的记录。因此，MySQL需要再次去掉重复的记录，然后再把结果发送给客户端。 Reopen table 获得了对一个表的锁，但是必须在表结构修改之后才能获得这个锁。已经释放锁，关闭数据表，正尝试重新打开数据表。 Repair by sorting 修复指令正在排序以创建索引。 Repair with keycache 修复指令正在利用索引缓存一个一个地创建新索引。它会比Repair by sorting慢些。 Searching rows for update 正在讲符合条件的记录找出来以备更新。它必须在Update要修改相关的记录之前就完成了。 Sleeping 正在等待客户端发送新请求. System lock 正在等待取得一个外部的系统锁。如果当前没有运行多个mysqld服务器同时请求同一个表，那么可以通过增加--skip-external-locking参数来禁止外部系统锁。 Upgrading lock Insert DELAYED正在尝试取得一个锁表以插入新记录。 Updating 正在搜索匹配的记录，并且修改它们。 User Lock 正在等待GET_LOCK()。 Waiting for tables 该线程得到通知，数据表结构已经被修改了，需要重新打开数据表以取得新的结构。然后，为了能的重新打开数据表，必须等到所有其他线程关闭这个表。以下几种情况下会产生这个通知：FLUSH TABLES tbl_name, Alter TABLE, RENAME TABLE, REPAIR TABLE, ANALYZE TABLE,或OPTIMIZE TABLE。 waiting for handler insert Insert DELAYED已经处理完了所有待处理的插入操作，正在等待新的请求。 大部分状态对应很快的操作，只要有一个线程保持同一个状态好几秒钟，那么可能是有问题发生了，需要检查一下。 还有其他的状态没在上面中列出来，不过它们大部分只是在查看服务器是否有存在错误是才用得着 查询表使用状态 查询哪些表在被使用，是否有锁表： SHOW OPEN TABLES WHERE In_use > 0; 查看增删改数量 查询当前MySQL中查询、更新、删除执行多少条了，可以通过这个来判断系统是侧重于读还是侧重于写，如果是写要考虑使用读写分离。 SHOW STATUS LIKE '%Com_select%'; SHOW STATUS LIKE '%Com_insert%'; SHOW STATUS LIKE '%Com_update%'; SHOW STATUS LIKE '%Com_delete%'; 修改密码 use mysql update user set authentication_string=password('1qaz#EDC') where user='root'; flush privileges; binlog 查看binlog保存天数 默认值为0，即永久保存 show variables like 'expire_logs_days'; 配置binlog失效时间 set global expire_logs_days=7; 清理binlog flush logs; 清除指定时间的binlog purge binary logs before '2017-05-01 13:09:51'; Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"3.集成部署/nginx/":{"url":"3.集成部署/nginx/","title":"nginx","keywords":"","body":" Table of Contents generated with DocToc 安装部署 使用源码编译安装 使用预编译包安装 配置调优 安全加固 相关文档 keepalive及444状态码 nginx location匹配顺序 nginx重定向 nginx http请求处理流程 配置为系统服务 生成文件 tee /usr/lib/systemd/system/nginx.service 启动 systemctl daemon-reload systemctl enable nginx --now 查看nginx最大连接数 grep 'open files' /proc/$( cat /var/run/nginx.pid )/limits nginx.conf最大连接数配置 worker_rlimit_nofile 65535; events { use epoll; worker_connections 65535; } 安装部署 使用源码编译安装 灵活性最高 官网下载地址 自用开源项目 使用预编译包安装 参考地址 RHEL7 or CentOS 7 From EPEL # Install epel repository: yum install epel-release # or alternative: # wget -c https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm # yum install epel-release-latest-7.noarch.rpm # Install NGINX: yum install nginx From Software Collections # Install and enable scl: yum install centos-release-scl yum-config-manager --enable rhel-server-rhscl-7-rpms # Install NGINX (rh-nginx14, rh-nginx16, rh-nginx18): yum install rh-nginx16 # Enable NGINX from SCL: scl enable rh-nginx16 bash From Official Repository # Where: # - is: rhel or centos cat > /etc/yum.repos.d/nginx.repo /$releasever/$basearch/ gpgcheck=0 enabled=1 __EOF__ # Install NGINX: yum install nginx Debian or Ubuntu From Debian/Ubuntu Repository # Install NGINX: apt-get install nginx From Official Repository # Where: # - is: debian or ubuntu # - is: xenial, bionic, jessie, stretch or other cat > /etc/apt/sources.list.d/nginx.list / nginx deb-src http://nginx.org/packages// nginx __EOF__ # Update packages list: apt-get update # Download the public key (or from your GPG error): apt-key adv --keyserver keyserver.ubuntu.com --recv-keys # Install NGINX: apt-get update apt-get install nginx 配置调优 nginx管理员手册 参考地址 绑定cpu worker_processes 2; worker_cpu_affinity 01 10; #2核CPU,开启4个进程 worker_processes 4; worker_cpu_affinity 01 10 01 10; #4核CPU，开户4个进程 worker_processes 4; worker_cpu_affinity 0001 0010 0100 1000; #8核 worker_processes 8; worker_cpu_affinity 00000001 00000010 00000100 00001000 00010000 00100000 01000000; 打开文件数 #与ulimit -n一致 worker_rlimit_nofile 655350; 每个进程允许的最多连接数 #ulimit -n / worker数量 worker_connections 102400; 请求头部的缓冲区大小 #与系统分页大小一致（getconf PAGESIZE获取分页大小） client_header_buffer_size 4k; 提高文件传输性能 #开启高效文件传输模式，sendfile 指令指定 Nginx 是否调用sendfile 函数来输出文件， #对于普通应用设为 on，如果用来进行下载等应用磁盘 IO 重负载应用，可设置为 off， #以平衡磁盘与网络 I/O 处理速度，降低系统的负载。 sendfile on; 小的数据包不等待直接传输 tcp_nodelay on; 开启gzip压缩 gzip on; gzip_min_length 1100; #对数据启用压缩的最少字节数,如:请求小于1K文件,不要压缩,压缩小数据会降低处理此请求的所有进程速度 gzip_buffers 4 16k; gzip_proxied any; #允许或者禁止压缩基于请求和响应的响应流,若设置为any,将会压缩所有请求 gzip_http_version 1.0; gzip_comp_level 9; #gzip压缩等级在0-9内,数值越大压缩率越高,CPU消耗也就越大 gzip_types text/plain text/css application/javascript application/x-javascript text/xml application/xml application/xml+rss text/javascript application/json image/jpeg image/gif image/png; #压缩类型 gzip_vary on; #varyheader支持,让前端的缓存服务器识别压缩后的文件,代理 open_file_cache max=65535 inactive=20s; #这个将为打开文件指定缓存,max 指定缓存数量.建议和打开文件数一致.inactive 是指经过多长时间文件没被请求后删除缓存 open_file_cache_valid 30s; #这个是指多长时间检查一次缓存的有效信息,例如我一直访问这个文件,30秒后检查是否更新,反之更新 open_file_cache_min_uses 2; #定义了open_file_cache中指令参数不活动时间期间里最小的文件数 open_file_cache_errors on; #NGINX可以缓存在文件访问期间发生的错误,这需要设置该值才能有效,如果启用错误缓存.则在访问资源（不查找资源）时.NGINX会报告相同的错误 关闭404日志记录 log_not_found off; 安全加固 隐藏版本号 server_tokens off; 修改server名 more_set_headers \"Server: web\"; 配置host白名单 #结合开源项目，定期更新IP黑名单 项目地址 #结合lua维护白名单 -- 校验Host合法性 function host_check(host) if tableFind(host,valid_hosts) == false then record_attack_log(\"BadHost\") ngx.exit(444) end end 配置refer黑白名单 #结合开源项目，定期更新refer黑名单 项目地址 #结合lua维护白名单 -- 校验Referer合法性 function referer_check(refer,host) if refer ~= nil and string.find(refer,host) == nil then if tableFind(refer,valid_referers) == false then record_attack_log(\"BadRefer\") ngx.exit(444) end end end 拦截sql注入 naxsi 结合naxsi模块 拦截XSS攻击 naxsi 结合naxsi模块 使用最新版openssl 编译nginx时指定 关于openssl版本维护信息 the next version of OpenSSL will be 3.0.0 version 1.1.1 will be supported until 2023-09-11 (LTS) last minor version: 1.1.1c (May 23, 2019) version 1.1.0 will be supported until 2019-09-11 last minor version: 1.1.0k (May 28, 2018) version 1.0.2 will be supported until 2019-12-31 (LTS) last minor version: 1.0.2s (May 28, 2018) any other versions are no longer supported 配置ssl协议 SSL/TLS ssl_protocols TLSv1.3 TLSv1.2; 使用tls时关闭gzip Some attacks are possible (e.g. the real BREACH attack is a complicated) because of gzip (HTTP compression not TLS compression) being enabled on SSL requests. In most cases, the best action is to simply disable gzip for SSL. gzip off; 降低XSS劫持配置 add_header Content-Security-Policy \"default-src 'none'; script-src 'self'; connect-src 'self'; img-src 'self'; style-src 'self';\" always; add_header X-XSS-Protection \"1; mode=block\" always; 剔除不安全HEADER https://veggiespam.com/headers/ location / { proxy_hide_header X-Powered-By; proxy_hide_header X-AspNetMvc-Version; proxy_hide_header X-AspNet-Version; proxy_hide_header X-Drupal-Cache; proxy_pass http://backend-server; } 配置Referrer-Policy refer介绍 https://scotthelme.co.uk/a-new-security-header-referrer-policy/ #http请求分为请求行，请求头以及请求体，而请求头又分为general，request headers，此字段设置与general中，用来约定request headers中的referer #任何情况下都不发送referer add_header Referrer-Policy \"origin\"; 可选值 \"no-referrer\", #任何情况下都不发送referer \"no-referrer-when-downgrade\", #在同等安全等级下（例如https页面请求https地址），发送referer，但当请求方低于发送方（例如https页面请求http地址），不发送referer \"same-origin\", #当双方origin相同时发送 \"origin\", #仅仅发送origin，即protocal+host \"strict-origin\", #当双方origin相同且安全等级相同时发送 \"origin-when-cross-origin\", #跨域时发送origin \"strict-origin-when-cross-origin\", \"unsafe-url\" #任何情况下都显示完整的referer 配置X-Frame-Option add_header X-Frame-Options \"SAMEORIGIN\" always; 配置Feature-Policy Feature Policy是一个新的http响应头属性，允许一个站点开启或者禁止一些浏览器属性和API，来更好的确保站点的安全性和隐私性。 可以严格的限制站点允许使用的属性是很愉快的，而可以对内嵌在站点中的iframe进行限制则更加增加了站点的安全性。 W3C标准 https://w3c.github.io/webappsec-feature-policy/ add_header Feature-Policy \"geolocation 'none'; midi 'none'; notifications 'none'; push 'none'; sync-xhr 'none'; microphone 'none'; camera 'none'; magnetometer 'none'; gyroscope 'none'; speaker 'none'; vibrate 'none'; fullscreen 'none'; payment 'none'; usb 'none';\"; 禁用不安全HTTP方法 if ($request_method !~ ^(GET|POST|HEAD)$) { return 405; } 禁止缓存敏感数据 expires 0; add_header Cache-Control \"no-cache, no-store\"; 防止缓冲区溢出攻击 client_max_body_size 100m; client_body_buffer_size 128k; client_header_buffer_size 512k; large_client_header_buffers 4 512k 相关文档 keepalive及444状态码 keepalive 该配置官方文档给出的默认值为75s 官方文档地址 1、nginx keepalive配置方便起见配置为30s #配置于nginx.conf 中的 http{}内 keepalive_timeout 30s; 2、nginx server配置 server { listen 8089; location /123 { proxy_pass http://192.168.1.145:8080; } location / { index html/index.html; } } 3、开启wireshark监听虚拟网卡（nginx部署于本地vmware上的虚机，nat模式） 4、使用POSTMAN发送请求 5、wireshark过滤观察 keepalive与断开连接 444状态码 适用于屏蔽非安全请求或DDOS防御 1、nginx server配置 server { listen 8089; location /123 { proxy_pass http://192.168.1.145:8080; } location / { index html/index.html; } location /abc { return 444; } } 2、开启wireshark监听虚拟网卡（nginx部署于本地vmware上的虚机，nat模式） 3、发送请求 4、wireshark过滤观察 nginx location匹配顺序 例子来源以下地址 https://github.com/trimstray/nginx-admins-handbook#introduction 假设配置如下 server { listen 80; server_name xyz.com www.xyz.com; location ~ ^/(media|static)/ { root /var/www/xyz.com/static; expires 10d; } location ~* ^/(media2|static2) { root /var/www/xyz.com/static2; expires 20d; } location /static3 { root /var/www/xyz.com/static3; } location ^~ /static4 { root /var/www/xyz.com/static4; } location = /api { proxy_pass http://127.0.0.1:8080; } location / { proxy_pass http://127.0.0.1:8080; } location /backend { proxy_pass http://127.0.0.1:8080; } location ~ logo.xcf$ { root /var/www/logo; expires 48h; } location ~* .(png|ico|gif|xcf)$ { root /var/www/img; expires 24h; } location ~ logo.ico$ { root /var/www/logo; expires 96h; } location ~ logo.jpg$ { root /var/www/logo; expires 48h; } } 匹配规则如下 请求URL 相匹配的location 最终匹配 / 1) prefix match for / / /css 1) prefix match for / / /api 1) exact match for /api /api /api/ 1) prefix match for / / /backend 1) prefix match for /2) prefix match for /backend /backend /static 1) prefix match for / / /static/header.png 1) prefix match for /2) case sensitive regex match for ^/(media|static)/ ^/(media|static)/ /static/logo.jpg 1) prefix match for /2) case sensitive regex match for ^/(media|static)/ ^/(media|static)/ /media2 1) prefix match for /2) case insensitive regex match for ^/(media2|static2) ^/(media2|static2) /media2/ 1) prefix match for /2) case insensitive regex match for ^/(media2|static2) ^/(media2|static2) /static2/logo.jpg 1) prefix match for /2) case insensitive regex match for ^/(media2|static2) ^/(media2|static2) /static2/logo.png 1) prefix match for /2) case insensitive regex match for ^/(media2|static2) ^/(media2|static2) /static3/logo.jpg 1) prefix match for /static32) prefix match for /3) case sensitive regex match for logo.jpg$ logo.jpg$ /static3/logo.png 1) prefix match for /static32) prefix match for /3) case insensitive regex match for .(png|ico|gif|xcf)$ .(png|ico|gif|xcf)$ /static4/logo.jpg 1) priority prefix match for /static42) prefix match for / /static4 /static4/logo.png 1) priority prefix match for /static42) prefix match for / /static4 /static5/logo.jpg 1) prefix match for /2) case sensitive regex match for logo.jpg$ logo.jpg$ /static5/logo.png 1) prefix match for /2) case insensitive regex match for .(png|ico|gif|xcf)$ .(png|ico|gif|xcf)$ /static5/logo.xcf 1) prefix match for /2) case sensitive regex match for logo.xcf$ logo.xcf$ /static5/logo.ico 1) prefix match for /2) case insensitive regex match for .(png|ico|gif|xcf)$ .(png|ico|gif|xcf)$ 匹配顺序说明 nginx根据uri进行最优匹配 基于前缀的NGINX位置匹配(没有正则表达式)。每个位置都将根据请求URI进行检查 NGINX搜索精确的匹配。如果=修饰符与请求URI完全匹配，则立即选择此特定位置块 如果没有找到精确的位置块(即没有相应的=修饰符)，NGINX将继续使用非精确的前缀。它从这个URI的最长匹配前缀位置开始，方法如下: 如果最长匹配前缀位置有^~修饰符，NGINX将立即停止搜索并选择该位置。 假设最长匹配前缀位置不使用^~修饰符，匹配将被临时存储，并继续执行。 一旦选择并存储了最长匹配前缀位置，NGINX就会继续计算区分大小写和不敏感的正则表达式位置。第一个适合URI的正则表达式位置将立即被选中来处理请求 如果没有找到匹配请求URI的正则表达式位置，则选择先前存储的前缀位置来服务请求 nginx重定向 实现方式 rewrite && return nginx http请求处理流程 参考文章 https://github.com/trimstray/nginx-admins-handbook#introduction https://blog.51cto.com/wenxi123/2296295?source=dra nginx处理一个请求共分为11个阶段 阶段一，NGX_HTTP_POST_READ_PHASE 获取请求头信息 #相关模块: ngx_http_realip_module 阶段二，NGX_HTTP_SERVER_REWRITE_PHASE 实现在server{}块中定义的重写指令: 使用PCRE正则表达式更改请求uri，返回重定向uri； #相关模块: ngx_http_rewrite_module 阶段三，NGX_HTTP_FIND_CONFIG_PHASE **仅nginx核心模块可以参与** 根据阶段二的uri匹配location 阶段四，NGX_HTTP_REWRITE_PHASE 由阶段三匹配到location，并在location{}块中再次进行uri转换 #相关模块: ngx_http_rewrite_module 阶段五，NGX_HTTP_POST_REWRITE_PHASE **仅nginx核心模块可以参与** 请求地址重写提交阶段，防止递归修改uri造成死循环，（一个请求执行10次就会被nginx认定为死循环） #相关模块: ngx_http_rewrite_module 阶段六，NGX_HTTP_PREACCESS_PHASE 访问控制阶段一： 验证预处理请求限制，访问频率、连接数限制（访问限制） #相关模块：ngx_http_limit_req_module, ngx_http_limit_conn_module, ngx_http_realip_module 阶段七，NGX_HTTP_ACCESS_PHASE 访问控制阶段二： 客户端验证(源IP是否合法，是否通过HTTP认证) #相关模块：ngx_http_access_module, ngx_http_auth_basic_module 阶段八，NGX_HTTP_POST_ACCESS_PHASE **仅nginx核心模块可以参与** 访问控制阶段三： 访问权限检查提交阶段；如果请求不被允许访问nginx服务器，该阶段负责向用户返回错误响应； #相关模块：ngx_http_access_module, ngx_http_auth_basic_module 阶段九，NGX_HTTP_PRECONTENT_PHASE **仅nginx核心模块可以参与** 如果http请求访问静态文件资源，try_files配置项可以使这个请求顺序地访问多个静态文件资源，直到某个静态文件资源符合选取条件 #相关模块：ngx_http_try_files_module 阶段十，NGX_HTTP_CONTENT_PHASE 内容产生阶段，大部分HTTP模块会介入该阶段，是所有请求处理阶段中最重要的阶段，因为这个阶段的指令通常是用来生成HTTP响应内容的； #相关模块：ngx_http_index_module, ngx_http_autoindex_module, ngx_http_gzip_module 阶段十一，NGX_HTTP_LOG_PHASE 记录日志阶段 #相关模块：ngx_http_log_module 示例图 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:19:35 "},"3.集成部署/nodejs/":{"url":"3.集成部署/nodejs/","title":"nodejs","keywords":"","body":"nodejs环境安装 下载 curl -O https://nodejs.org/dist/v12.14.0/node-v12.14.0-linux-x64.tar.xz 解压 sudo tar xvf node-v12.14.0-linux-x64.tar.xz -C /usr/local/ 加入PATH echo \"export PATH=\\$PATH:/usr/local/node-v12.14.0-linux-x64/bin/\" >> ~/.bashrc . ~/.bashrc 修改镜像源 npm config set registry https://registry.npm.taobao.org Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"3.集成部署/ntp/":{"url":"3.集成部署/ntp/","title":"ntp","keywords":"","body":"ntp yum -y install ntpdate /usr/sbin/ntpdate ntp1.aliyun.com echo \"*/5 * * * * /usr/sbin/ntpdate ntp1.aliyun.com\" >> /etc/crontab ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"3.集成部署/openjdk/":{"url":"3.集成部署/openjdk/","title":"openjdk","keywords":"","body":"openjdk yum install -y java-1.8.0-openjdk.x86_64 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"3.集成部署/oracle/":{"url":"3.集成部署/oracle/","title":"oracle","keywords":"","body":" oracle 安装条件检测 安装oracle单机 docker启动oracle oracle 安装条件检测 以下内容仅为官网要求部分摘抄，详细环境要求如下 https://docs.oracle.com/en/database/oracle/oracle-database/12.2/ladbi/oracle-database-installation-checklist.html#GUID-E847221C-1406-4B6D-8666-479DB6BDB046 硬件要求 1、DVD光驱（如果采用DVD光盘安装） 2、linux系统运行级别为3或5 3、显卡分辨率最低1024 x 768（Oracle Universal Installer图形化安装需要） 4、oracle宿主机联网（拥有网络适配器） 5、最小1g内存，建议2g #Linux系统有7个运行级别(runlevel) 运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动 运行级别1：单用户工作状态，root权限，用于系统维护，禁止远程登陆 运行级别2：多用户状态(没有NFS) 运行级别3：完全的多用户状态(有NFS)，登陆后进入控制台命令行模式 运行级别4：系统未使用，保留 运行级别5：X11控制台，登陆后进入图形GUI模式 运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动 #查看当前系统运行级别(即多用户级别) systemctl get-default 或 runlevel 运行级别的原理： 1。在目录/etc/rc.d/init.d下有许多服务器脚本程序，一般称为服务(service) 2。在/etc/rc.d下有7个名为rcN.d的目录，对应系统的7个运行级别 3。rcN.d目录下都是一些符号链接文件，这些链接文件都指向init.d目录下的service脚本文件，命名规则为K+nn+服务名或S+nn+服务名，其中nn为两位数字。 4。系统会根据指定的运行级别进入对应的rcN.d目录，并按照文件名顺序检索目录下的链接文件 对于以K开头的文件，系统将终止对应的服务 对于以S开头的文件，系统将启动对应的服务 5。查看运行级别用：runlevel 6。进入其它运行级别用：init N 7。另外init0为关机，init 6为重启系统 操作系统要求 1、安装OpenSSH服务 2、针对X86-64系统内核支持 Oracle Linux 7 with the Unbreakable Enterprise Kernel 3: 3.8.13-35.3.1.el7uek.x86_64 or later Oracle Linux 7.2 with the Unbreakable Enterprise Kernel 4: 4.1.12-32.2.3.el7uek.x86_64 or later Oracle Linux 7 with the Red Hat Compatible kernel: 3.10.0-123.el7.x86_64 or later Red Hat Enterprise Linux 7: 3.10.0-123.el7.x86_64 or later Oracle Linux 6.4 with the Unbreakable Enterprise Kernel 2: 2.6.39-400.211.1.el6uek.x86_64or later Oracle Linux 6.6 with the Unbreakable Enterprise Kernel 3: 3.8.13-44.1.1.el6uek.x86_64 or later Oracle Linux 6.8 with the Unbreakable Enterprise Kernel 4: 4.1.12-37.6.2.el6uek.x86_64 or later Oracle Linux 6.4 with the Red Hat Compatible kernel: 2.6.32-358.el6.x86_64 or later Red Hat Enterprise Linux 6.4: 2.6.32-358.el6.x86_64 or later SUSE Linux Enterprise Server 12 SP1: 3.12.49-11.1 or later SUSE Linux Enterprise Server 15: 4.12.14-25-default or later Review the system requirements section for a list of minimum package requirements. 3、若宿主机操作系统为Oracle Linux，建议使用oracle预编译rpm包进行oracle环境初始化 宿主机配置要求 1、/tmp下只要1GB可用存储空间 2、交换区内存大小应满足以下要求 当物理内存在1GB与2GB之间，swap内存应为物理内存的1.5倍 当物理内存在2GB与16GB之间，swap内存应等于物理内存 当物理内存高于16GB，swap内存固定16G 需要注意的是，如果为Linux服务器启用了HugePages，那么在计算交换空间之前，应该从可用RAM中减去分配给HugePages的内存 3、oracle安装目录必须为ASCII字符 4、清除以下变量（如果当前主机安装过oracle，会存在以下变量） $ORACLE_HOME,$ORA_NLS10, $TNS_ADMIN, $ORACLE_BASE, $ORACLE_SID 5、使用root用户或具有root权限的用户（sudo）进行安装 宿主机存储空间要求 针对Linux x86-64: 单节点最低8.6 GB 企业版最低7.5 GB 安装oracle单机 参考地址 下载安装介质（迅雷下载） https://updates.oracle.com/Orion/Services/download/p13390677_112040_Linux-x86-64_1of7.zip?aru=16716375&patch_file=p13390677_112040_Linux-x86-64_1of7.zip https://updates.oracle.com/Orion/Services/download/p13390677_112040_Linux-x86-64_2of7.zip?aru=16716375&patch_file=p13390677_112040_Linux-x86-64_2of7.zip 安装依赖 yum -y install binutils compat-libstdc++-33 elfutils-libelf gcc gcc-c++ \\ glibc glibc-common glibc-devel glibc-headers ksh libaio libaio-devel \\ libgomp libgcc libstdc++ libstdc++-devel make sysstat unixODBC \\ unixODBC-devel numactl-devel kernel-headers glibc-headers \\ glibc-devel elfutils-libelf-devel pdksh readline-dev* libXp-* unzip perl psmisc 修改系统参数 HOSTNAME=ora11g echo \"$HOSTNAME\">/etc/hostname echo \"$(grep -E '127|::1' /etc/hosts)\">/etc/hosts echo \"$(ip a|grep \"inet \"|grep -v 127|awk -F'[ /]' '{print $6}') $HOSTNAME\">>/etc/hosts rm -rf /etc/systemd/system/default.target ln -s /lib/systemd/system/multi-user.target /etc/systemd/system/default.target setenforce 0 sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/selinux/config systemctl disable firewalld --now systemctl disable NetworkManager --now systemctl disable NetworkManager-dispatcher --now systemctl disable postfix --now 创建用户用户组 groupadd oinstall groupadd dba # 服务器一定要配置成新建用户同时新建用户家目录！！！ useradd -g oinstall -G dba oracle echo oracle|passwd --stdin oracle echo 'fs.suid_dumpable = 1'>>/etc/sysctl.conf echo 'fs.aio-max-nr = 1048576'>>/etc/sysctl.conf echo 'fs.file-max = 6815744'>>/etc/sysctl.conf echo 'kernel.shmmni = 4096'>>/etc/sysctl.conf echo 'kernel.shmmax = 1075267584'>>/etc/sysctl.conf echo 'kernel.shmall = 2097152'>>/etc/sysctl.conf echo 'kernel.sem = 250 32000 100 128'>>/etc/sysctl.conf echo 'net.ipv4.ip_local_port_range = 9000 65500'>>/etc/sysctl.conf echo 'net.core.rmem_default = 1048576'>>/etc/sysctl.conf echo 'net.core.rmem_max = 4194304'>>/etc/sysctl.conf echo 'net.core.wmem_default = 262144'>>/etc/sysctl.conf echo 'net.core.wmem_max = 1048586'>>/etc/sysctl.conf sysctl -p echo 'oracle soft nproc 2047'>>/etc/security/limits.conf echo 'oracle hard nproc 16384'>>/etc/security/limits.conf echo 'oracle soft nofile 4096'>>/etc/security/limits.conf echo 'oracle hard nofile 65536'>>/etc/security/limits.conf echo 'oracle soft stack 10240'>>/etc/security/limits.conf echo 'session required pam_limits.so'>>/etc/pam.d/login mkdir -p /u01/app/oracle/product/11.2.0/db_1 chown -R oracle:oinstall /u01 chmod -R 775 /u01 配置环境变量 cat >> /home/oracle/.bash_profile 上传安装介质至/tmp下,root执行 chown oracle:oinstall /tmp/p13390677_112040_Linux-x86-64_* 切换用户解压 su - oracle cd /tmp # linux安装zip unzip: yum install -y unzip zip; unzip p13390677_112040_Linux-x86-64_1of7.zip unzip p13390677_112040_Linux-x86-64_2of7.zip 创建配置文件 # 复制粘贴执行即可#############开始###################### cd database/ cat >>/tmp/database/response/install_11g.rsp 静默安装 # 很重要的一个过程,出现警告正常，出现FATAL不可继续执行！！！ # 出现FATAL,请检查tmp及其子目录是否有多余的一些垃圾文件 # 这个执行过程是很长的，一定要等待这一步完全执行完成，方可进行下一步操作，可以通过：ps -ef | grep oracle ,查看此命令的执行进程！！！！！ ./runInstaller -force -silent -responseFile /tmp/database/response/install_11g.rsp oracle用户执行 /u01/app/oracle/oraInventory/orainstRoot.sh /u01/app/oracle/product/11.2.0/db_1/root.sh oracle用户创建监听 cat >/u01/app/oracle/product/11.2.0/db_1/network/admin/listener.ora root执行建库脚本 mkdir -p /oradata/orcl chown -R oracle: /oradata su - oracle ###建库脚本，执行并挂到后台 vi /oradata/orcl/dbca.sh # 以下是脚本内容：**************************************** #!/bin/bash cat >>/home/oracle/init.ora>/etc/oratab /u01/app/oracle/product/11.2.0/db_1/bin/sqlplus /nolog 启动oracle sqlplus / as sysdba 调整配置 ALTER PROFILE DEFAULT LIMIT PASSWORD_LIFE_TIME UNLIMITED; #将默认的密码生存周期由180天改为无限制 alter system set audit_trail=none scope=spfile; shutdown immediate; #关闭默认库级审计 startup alter system set deferred_segment_creation=false; #关闭段延迟分配 ################################################### host mkdir -p /oradata/arch/orcl alter system set log_archive_format='arch_%t_%s_%r.arc' scope=spfile; alter system set log_archive_dest_10='location=/oradata/arch/orcl/' scope=spfile; shutdown immediate; startup mount; alter database archivelog; alter database open; alter system archive log current; alter system set control_file_record_keep_time=30; docker启动oracle 1.安装docker 2.拉群镜像 docker pull registry.cn-hangzhou.aliyuncs.com/helowin/oracle_11g 3.启动 docker run -itd -p 11521:1521 --restart=always --name oracle11g registry.cn-hangzhou.aliyuncs.com/helowin/oracle_11g Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"3.集成部署/pdns/":{"url":"3.集成部署/pdns/","title":"pdns","keywords":"","body":"pdns 安装epel源 yum install -y epel-release 安装mariadb yum -y install mariadb mariadb-server 启动mariadb systemctl enable mariadb --now 配置mariadb mysql_secure_installation 依次输入以下内容 Enter current password for root (enter for none): -- 回车 Set root password? [Y/n] -- Y New password: -- 输入root口令，这里演示用设置为root Re-enter new password: -- 输入上一步设置的root口令进行确认 Remove anonymous users? [Y/n] -- 回车 Disallow root login remotely? [Y/n] -- 回车 Remove test database and access to it? [Y/n] -- 回车 Reload privilege tables now? [Y/n] -- 回车 修改mariadb字符集 修改服务端 sed -i \"s/\\[mysqld\\]/&\\ \\ninit_connect='SET collation_connection = utf8_unicode_ci'\\ \\ninit_connect='SET NAMES utf8'\\ \\ncharacter-set-server=utf8\\ \\ncollation-server=utf8_unicode_ci\\ \\nskip-character-set-client-handshake/\" /etc/my.cnf 修改客户端 sed -i \"s/\\[client\\]/&\\ndefault-character-set=utf8/\" /etc/my.cnf.d/client.cnf sed -i \"s/\\[mysql\\]/&\\ndefault-character-set=utf8/\" /etc/my.cnf.d/mysql-clients.cnf 重启mariadb systemctl restart mariadb 查看字符集 mysql -uroot -proot 输出如下： Variable_name Value character_set_client utf8 character_set_connection utf8 character_set_database utf8 character_set_filesystem binary character_set_results utf8 character_set_server utf8 character_set_system utf8 character_sets_dir /usr/share/mysql/charsets/ Variable_name Value collation_connection utf8_unicode_ci collation_database utf8_unicode_ci collation_server utf8_unicode_ci 创建pdns_db mysql -uroot -proot 创建pdns用户 mysql -uroot -proot 初始化数据 mysql -u root -proot poweradmin 安装pdns yum install -y pdns.x86_64 pdns-backend-mysql 配置pdns sed -i \"s#launch=bind#launch=gmysql\\ \\ngmysql-host=localhost\\ \\ngmysql-user=powerdns\\ \\ngmysql-dbname=pdns_db\\ \\ngmysql-password=powerdns#\" /etc/pdns/pdns.conf Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"3.集成部署/poweradmin/":{"url":"3.集成部署/poweradmin/","title":"poweradmin","keywords":"","body":"poweradmin 下载poweradmin curl -L https://sourceforge.net/projects/poweradmin/files/poweradmin-2.1.7.tgz/download -o ./poweradmin-2.1.7.tgz 安装lnap yum install httpd php php-common php-curl php-devel php-gd php-pear php-imap php-mcrypt php-mhash php-mysql php-xmlrpc gettext -y 拷贝项目文件 tar xvf poweradmin-2.1.7.tgz cp -r poweradmin-2.1.7/* /var/www/html/ 启动apache systemctl enable httpd --now 启动powerdns systemctl enable pdns --now 配置poweradmin 浏览器访问：http://宿主机互联网IP:80/install 下一步 配置数据库信息(全为：poweradmin) 配置如下 下一步 配置/var/www/html/inc/config.inc.php vim /var/www/html/inc/config.inc.php 内容为web页面红框内容: 删除/var/www/html/install/ rm -rf /var/www/html/install/ 访问宿主机80端口，进行登录(admin/poweradmin) Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"3.集成部署/python/":{"url":"3.集成部署/python/","title":"python","keywords":"","body":"Python3.6 yum install -y python36 python36-pip python36-devel Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"3.集成部署/tomcat/":{"url":"3.集成部署/tomcat/","title":"tomcat","keywords":"","body":" 调优 --> Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"3.集成部署/trafodion/":{"url":"3.集成部署/trafodion/","title":"trafodion","keywords":"","body":"trafodion CDH集群部署 主机列表 192.168.1.11 192.168.1.12 192.168.1.13 操作系统（必须） CentOS6 环境初始化 1.设置hostname，并配置host解析 192.168.1.11主机执行： cat >> /etc/sysconfig/network /proc/sys/kernel/hostname cat >> /etc/hosts 192.168.1.12主机执行： cat >> /etc/sysconfig/network /proc/sys/kernel/hostname cat >> /etc/hosts 192.168.1.13主机执行： cat >> /etc/sysconfig/network /proc/sys/kernel/hostname cat >> /etc/hosts 2.安装必要软件 yum install -y openssh-clients yum-utils createrepo yum-plugin-priorities 3.实现主机互信（hadoop1节点执行） hadoop1节点执行以下命令，生成ssh密钥 ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa 分发密钥完成互信（依次输入各节点密码） ssh-copy-id hadoop1 ssh-copy-id hadoop2 ssh-copy-id hadoop3 4.调整系统参数（三个节点均执行） # 调整文件句柄数 echo \"* soft nofile 655350\" >> /etc/security/limits.conf echo \"* hard nofile 655350\" >> /etc/security/limits.conf echo \"* soft nproc 65535\" >> /etc/security/limits.conf echo \"* hard nproc 65535\" >> /etc/security/limits.conf ulimit -n 655350 # 关闭防火墙 service iptables stop chkconfig iptables off # 关闭selinux setenforce 0 sed -i \"s#SELINUX=enforcing#SELINUX=disabled#g\" /etc/selinux/config 5.安装oracle jdk1.8（所有节点） 安装oracle jdk1.8,并配置软链接 mkdir -p /usr/java ln -s /opt/java /usr/java/jdk1.8 6.安装mysql，初始化用户 mysql部署文档 创建scm_server用户 mysql -uroot -proot 7.安装配置ntp hadoop1节点执行： yum install -y ntp service ntpd start chkconfig ntpd on sed -i \"s;restrict default kod nomodify notrap nopeer noquery;#restrict default kod nomodify notrap nopeer noquery;g\" /etc/ntp.conf sed -i \"s;restrict -6 default kod nomodify notrap nopeer noquery;#restrict -6 default kod nomodify notrap nopeer noquery;g\" /etc/ntp.conf sed -i \"s#restrict -6 ::1#restrict ::1#g\" /etc/ntp.conf sed -i \"s;server 0.centos.pool.ntp.org iburst;#server 0.centos.pool.ntp.org iburst;g\" /etc/ntp.conf sed -i \"s;server 1.centos.pool.ntp.org iburst;#server 1.centos.pool.ntp.org iburst;g\" /etc/ntp.conf sed -i \"s;server 2.centos.pool.ntp.org iburst;#server 2.centos.pool.ntp.org iburst;g\" /etc/ntp.conf sed -i \"s;server 3.centos.pool.ntp.org iburst;#server 3.centos.pool.ntp.org iburst;g\" /etc/ntp.conf echo \"server 127.127.1.0\" >> /etc/ntp.conf echo \"fudge 127.127.1.0 stratum 10\" >> /etc/ntp.conf echo \"disable monitor\" >> /etc/ntp.conf echo \"restrict default nomodify\" >> /etc/ntp.conf hadoop2、hadoop3节点执行： yum install -y ntp sed -i \"s;restrict default kod nomodify notrap nopeer noquery;#restrict default kod nomodify notrap nopeer noquery;g\" /etc/ntp.conf sed -i \"s;restrict -6 default kod nomodify notrap nopeer noquery;#restrict -6 default kod nomodify notrap nopeer noquery;g\" /etc/ntp.conf sed -i \"s#restrict -6 ::1#restrict ::1#g\" /etc/ntp.conf sed -i \"s;server 0.centos.pool.ntp.org iburst;#server 0.centos.pool.ntp.org iburst;g\" /etc/ntp.conf sed -i \"s;server 1.centos.pool.ntp.org iburst;#server 1.centos.pool.ntp.org iburst;g\" /etc/ntp.conf sed -i \"s;server 2.centos.pool.ntp.org iburst;#server 2.centos.pool.ntp.org iburst;g\" /etc/ntp.conf sed -i \"s;server 3.centos.pool.ntp.org iburst;#server 3.centos.pool.ntp.org iburst;g\" /etc/ntp.conf echo \"server hadoop1\" >> /etc/ntp.conf echo \"disable monitor\" >> /etc/ntp.conf echo \"restrict default nomodify\" >> /etc/ntp.conf ntpdate hadoop1 8.下载mysql驱动（hadoop1节点） 下载mysql-connector-java-5.1.48.tar.gz上传至hadoop1:/tmp下 执行以下命令，调整名称 mkdir -p /usr/share/java tar zxvf mysql-connector-java-5.1.48.tar.gz cp mysql-connector-java-5.1.48/mysql-connector-java-5.1.48.jar /usr/share/java/mysql-connector-java.jar 安装CDH5.4 1.创建离线镜像源目录（所有节点） mkdir -p /cm 下载cloudera-manager下的所有文件，并上传至/cm下 2.配置cm源（所有节点） 创建repo源文件 cd /cm && createrepo ./ cat > /etc/yum.repos.d/cm.repo /cm目录最终结构如下： /cm ├── cloudera-manager-agent-5.4.0-1.cm540.p0.165.el6.x86_64.rpm ├── cloudera-manager-daemons-5.4.0-1.cm540.p0.165.el6.x86_64.rpm ├── cloudera-manager-server-5.4.0-1.cm540.p0.165.el6.x86_64.rpm ├── cloudera-manager-server-db-2-5.4.0-1.cm540.p0.165.el6.x86_64.rpm ├── enterprise-debuginfo-5.4.0-1.cm540.p0.165.el6.x86_64.rpm ├── jdk-6u31-linux-amd64.rpm ├── oracle-j2sdk1.7-1.7.0+update67-1.x86_64.rpm ├── repodata │ ├── 3a8b6a8a03c3846eadd0f0d8df2ef1142e6e32d21ce7e4e58a304ad3bef8b5b7-primary.sqlite.bz2 │ ├── 853ca50d5d1f076b5f53cd06ed4d74c62ee729af1a86e3caa1bd39aaf6e68cf7-other.sqlite.bz2 │ ├── a6b67b1228bbb6791eb66fd52cfc2044a681a9444e1a1aa044111029b6f4760c-filelists.xml.gz │ ├── b05946fbbf3fec9e107640249a183e7109d0e336ef23fcbe199b3dd1743f84f3-other.xml.gz │ ├── c2c48ea8c58913116c14e8ec853d2fd2731bee779edafc81dec0d60771709f17-filelists.sqlite.bz2 │ ├── fd1f07dacbe9d5e3be1e7f7930fbb6eac4d29a75c172c2a31e6e18baa56b5fee-primary.xml.gz │ └── repomd.xml └── RPM-GPG-KEY-cloudera 3.安装cloudera-manager-server(hadoop1节点) yum install cloudera-manager-daemons cloudera-manager-server -y 4.安装cloudera-manager-agent(hadoop1、hadoop2、hadoop3节点) yum install cloudera-manager-agent -y 5.修改cloudera-manager-agent配置文件(hadoop1、hadoop2、hadoop3节点) hadoop1、hadoop2、hadoop3节点执行： sed -i \"s#server_host=localhost#server_host=hadoop1#g\" /etc/cloudera-scm-agent/config.ini echo \"listening_ip=`hostname`\" >> /etc/cloudera-scm-agent/config.ini echo \"listening_hostname=`hostname`\" >> /etc/cloudera-scm-agent/config.ini 6.创建CDH离线源仓储 创建目录(hadoop1节点) mkdir -p /opt/cloudera/parcel-repo 上传以下文件至hadoop1节点/opt/cloudera/parcel-repo下 manifest.json CDH-5.4.0-1.cdh5.4.0.p0.27-el6.parcel CDH-5.4.0-1.cdh5.4.0.p0.27-el6.parcel.sha1 调整CDH-5.4.0-1.cdh5.4.0.p0.27-el6.parcel.sha1名称 cd /opt/cloudera/parcel-repo mv CDH-5.4.0-1.cdh5.4.0.p0.27-el6.parcel.sha1 CDH-5.4.0-1.cdh5.4.0.p0.27-el6.parcel.sha 7.初始化CM Server数据库(hadoop1节点) /usr/share/cmf/schema/scm_prepare_database.sh mysql scm_server_db scm_server scm_server -h 127.0.0.1 8.启动cloudera-manager-agent（hadoop1、hadoop2、hadoop3节点） service cloudera-scm-agent start chkconfig cloudera-scm-agent on 9.启动cloudera-manager-server(hadoop1节点) service cloudera-scm-server start chkconfig cloudera-scm-server on 查看日志 tail -200f /var/log/cloudera-scm-server/cloudera-scm-server.log 10.访问控制台初始化 主要hadoop1替换为实际IP地址 访问http://hadoop1:7180，账号密码: admin/admin 部署免费版本 确认部署应用，点击继续 添加部署节点，点击继续 确认cdh版本，继续 确认安装完成，点击继续 确认检测结果，点击完成 选取hbase内核hadoop安装 默认角色分配，继续 输入用户名密码数据库实例名，测试连接后点击继续 hive/hive hive oozie/oozie oozie 确认审核设置，点击继续 安装完成 安装trafodion 主机列表 192.168.1.11 192.168.1.12 192.168.1.13 1.下载安装介质及脚本 互联网下载地址: installer trafodion_server 2.创建/trafodion目录，上传安装介质及脚本至该目录下 目录结构如下： /trafodion/ ├── apache-trafodion_installer-2.1.0-incubating.tar.gz └── apache-trafodion_server-2.1.0-RH6-x86_64-incubating.tar.gz 3.解压运行安装脚本 cd /trafodion tar zxvf apache-trafodion_pyinstaller-2.1.0-incubating.tar.gz cd python-installer ./db_install.py 4.按提示输入相关信息 Enter HDP/CDH web manager URL:port, (full URL, if no http/https prefix, default prefix is http://): -- 输入 http://192.168.1.11:7180 Enter HDP/CDH web manager user name [admin]: -- 回车默认 Enter HDP/CDH web manager user password: -- 输入 admin Confirm Enter HDP/CDH web manager user password: -- 输入 admin Enter full path to Trafodion tar file: -- 输入 /trafodion/apache-trafodion_server-2.1.0-RH6-x86_64-incubating.tar.gz Enter directory name to install trafodion to [apache-trafodion-2.1.0]: -- 回车默认 Enter trafodion user password: -- 输入 trafodion Enter number of DCS client connections per node [4]: -- 回车默认 Enter trafodion scratch file folder location(should be a large disk), if more than one folder, use comma seperated [$TRAF_HOME/tmp]: -- 回车默认 Start instance after installation (Y/N) [Y]: -- 回车默认 Enable LDAP security (Y/N) [N]: -- 回车默认 Enable DCS High Avalability (Y/N) [N]: -- 回车默认 Enter Hadoop admin password, default is [admin]: -- 回车默认 Confirm result (Y/N) [N]: -- 输入 Y 安装过程 安装完毕 5.查看trafodion状态 登录hadoop2，切换到trafodion用户执行以下语句： sqcheck 返回如下，说明成功 *** Checking Trafodion Environment *** Checking if processes are up. Checking attempt: 1; user specified max: 2. Execution time in seconds: 0. The Trafodion environment is up! Process Configured Actual Down ------- ---------- ------ ---- DTM 2 2 RMS 4 4 DcsMaster 1 1 DcsServer 2 2 mxosrvr 8 8 RestServer 1 1 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"4.持续集成&交付/argo/":{"url":"4.持续集成&交付/argo/","title":"argo","keywords":"","body":"Argo CD 简介 Argo CD是什么？ Argo CD是一个基于Kubernetes声明性的GitOps持续交付工具 为什么使用Argo CD 声明式定义应用程序、配置和环境，并且是版本控制的 应用程序部署和生命周期管理是自动化的、可审计的和易于理解的 工作原理 Argo CD遵循GitOps模式，使用Git存储库作为定义应用程序期望状态的数据源。 Kubernetes应用清单可以通过以下几种方式指定: kustomize应用 helm应用 ksonnet应用 jsonnet 带有yaml|json清单的目录 任意自定义配置管理工具|插件 Argo CD可以在指定的目标环境中自动部署、维护期望的应用程序状态，该期望状态由清单文件定义。 应用程序清单版本可以基于Git提交时跟踪对分支、tag或固定到特定版本的Git commit。 Argo CD基于kubernetes控制器实现，它持续监控运行中的应用程序， 并将当前的活动状态与期望的目标状态(如Git repo中所指定的)进行比较。 如果已部署的应用程序的活动状态偏离目标状态，则将被视为OutOfSync。 Argo CD可视化展现程序状态差异，同时提供自动或手动同步工具。 特性 将应用程序自动部署到指定的目标环境 支持多种应用配置管理工具/模板（Kustomize, Helm, Ksonnet, Jsonnet, plain-YAML） 能够管理和部署到多个k8s集群 单点登录（OIDC, OAuth2, LDAP, SAML 2.0, GitHub, GitLab, Microsoft, LinkedIn） 用于授权的多租户和RBAC策略 回滚至Git仓库中指定的commit 应用程序资源的运行状况分析 自动配置漂移检测和可视化 自动/手动同步应用至期望状态 提供应用程序活动的实时视图的Web UI 用于自动化和CI集成的CLI Webhook集成(GitHub, BitBucket, GitLab) PreSync, Sync, PostSync钩子来支持复杂应用(例如蓝/绿和金丝雀的升级) 应用程序事件审计和追踪API调用 Prometheus指标 覆盖Git中ksonnet/helm的参数 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"4.持续集成&交付/argo/01-核心概念.html":{"url":"4.持续集成&交付/argo/01-核心概念.html","title":"01-核心概念","keywords":"","body":"核心概念 argocd架构示意图 以下argocd概念需要具有Git、Docker、Kubernetes、Continuous Delivery和GitOps相关背景 Application(应用): 基于Kubernetes CRD定义的一组Kubernetes资源清单 应用数据源类型: 构建应用的工具类型(helm等) 目标状态: 描述应用的期望状态（如副本数、配额、调度等），由git仓库内的应用清单文件描述 活动状态: 描述应用的活动状态（如副本数、配额、调度、探针状态等） 同步状态: 描述应用活动状态与目标状态同步情况（是否一致） 同步: 一个动作，使应用程序（集群内）与目标状态（git仓库清单文件描述）达成一致 同步操作执行的状态: 描述同步动作是否成功 刷新: 对比git仓库内的应用目标状态与活动状态，指出不同之处 健康状态: 描述应用程序是否运行正常，可以对外提供服务 工具: 创建应用程序清单描述文件的工具（如helm、Kustomize） Argo CD中项目是什么？ 项目提供了应用程序的逻辑分组，这在Argo CD被多个团队使用时非常有用。项目提供以下特性: 限制部署的内容(如可Git源代码库) 限制应用部署的位置(目标k8s集群和命名空间) 限制可部署或不可部署的对象类型(例如RBAC、CRDs、daemonset、NetworkPolicy等) 定义项目角色以提供应用程序RBAC(绑定到OIDC组和/或JWT令牌) 关于默认项目 每个应用程序都属于一个项目。如果未指定，应用程序属于默认项目，该项目是自动创建的， 默认情况下允许从任何源repo部署到任何集群，以及所有资源类型。 默认业务群组只能被修改，不能被删除。最初创建时，它的规范声明如下: spec: sourceRepos: - '*' destinations: - namespace: '*' server: '*' clusterResourceWhitelist: - group: '*' kind: '*' Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"4.持续集成&交付/argo/02-部署argocd.html":{"url":"4.持续集成&交付/argo/02-部署argocd.html","title":"02-部署argocd","keywords":"","body":"部署argocd 1.下载声明文件 install.yaml 2.发布 修改文件内镜像引用tag [root@node1 ~]# grep \"image:\" install.sh image: ghcr.io/dexidp/dex:v2.27.0 image: quay.io/argoproj/argocd:v2.0.4 image: redis:6.2.4-alpine image: quay.io/argoproj/argocd:v2.0.4 image: quay.io/argoproj/argocd:v2.0.4 image: quay.io/argoproj/argocd:v2.0.4 发布创建 kubectl create namespace argocd kubectl apply -n argocd -f install.yaml 查看部署状态 [root@node1 ~]# kubectl get pod -n argocd -w NAME READY STATUS RESTARTS AGE argocd-application-controller-0 1/1 Running 0 113s argocd-dex-server-764699868-28tmj 1/1 Running 0 113s argocd-redis-675b9bbd9d-dtbzh 1/1 Running 0 113s argocd-repo-server-59ffd86d98-2w7k4 1/1 Running 0 113s argocd-server-6d66686c5c-nqfpf 1/1 Running 0 113s 3.调整服务类型为NodePort kubectl -n argocd expose deployments/argocd-server --type=\"NodePort\" --port=8080 --name=argocd-server-nodeport 获取NodePort [root@node1 ~]# kubectl get service/argocd-server-nodeport -n argocd NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE argocd-server-nodeport NodePort 10.233.34.101 8080:31398/TCP 87s 4.查看登录口令 kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d|xargs -n1 echo 5.登录 登录地址： http://NodeIP:31418 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"4.持续集成&交付/argo/03-使用argocd.html":{"url":"4.持续集成&交付/argo/03-使用argocd.html","title":"03-使用argocd","keywords":"","body":"实践 通过一个样例来说明，argocd是如何结合gitlab与k8s实现应用的cicd流程 关于argocd其他部分内容（用户管理、安全审计、自定义hook）等内容，这里不做过多讨论 相关技术&工具： gitlab: 存放源代码与应用清单 docker: 构建镜像&容器运行时 harbor: 镜像制品库，管理镜像 jenkins: ci流水线工具 k8s: 容器编排工具 argocd: 基于k8s的cd工具 流程解析 代码库变更 开发人员提交代码，触发jenkins ci pipeline ci pipeline执行构建 包含以下步骤： 打包构建应用程序 构建应用镜像 根据commit id创建镜像tag 推送至镜像库 变更配置库配置 变更配置库配置 ci pipeline最后一个流程，执行以下内容： checkout配置库 利用yq工具变更配置库内yaml清单文件内容（主要为镜像tag） 提交变更至配置库 cd流程 argocd拉取配置库清单文件，比对内容。 文件发生变更 -> 执行变更 文件未发生变更 -> 继续观测配置库变更 源码库关键文件 源码工程 demo: 基于spring boot工程 Jenkinsfile内容 pipeline { agent any environment { DEMO_IMAGE_TAG=\"harbor.wl.com/library/demo\" DOCKER_REGISTRY_DOMAIN=\"harbor.wl.com\" DOCKER_CREDENTIAL_ID = 'harbor-secret' GIT_CREDENTIAL_ID='d145edf3-929c-4efa-aa46-48ea0cf4336e' GIT_CONFIG_REPO_URL=\"192.168.1.1:80/demo-group/demo.git\" } stages { stage ('checkout scm') { steps { checkout(scm) } } // 获取git提交的commit id stage('get commit id...') { steps { script { env.GIT_COMMIT_ID = sh (script: 'git rev-parse --short HEAD', returnStdout: true).trim() } } } // 基于Dockerfile内容构建demo应用镜像，生成两个版本tag：latest && commit id stage ('build demo image...') { steps { sh ''' sudo docker build -t $DEMO_IMAGE_TAG -f Dockerfile . sudo docker tag $DEMO_IMAGE_TAG $DEMO_IMAGE_TAG:$GIT_COMMIT_ID ''' } } // 推送镜像至本地Harbor库，票据有jenkins管理 stage ('publish image with portal...') { steps { withCredentials([usernamePassword(passwordVariable : 'DOCKER_PASSWORD' ,usernameVariable : 'DOCKER_USERNAME' ,credentialsId : \"$DOCKER_CREDENTIAL_ID\" ,)]) { sh 'sudo echo \"$DOCKER_PASSWORD\" | sudo docker login $DOCKER_REGISTRY_DOMAIN -u \"$DOCKER_USERNAME\" --password-stdin' sh ''' sudo docker push \"$DEMO_IMAGE_TAG\" sudo docker push \"$DEMO_IMAGE_TAG:$GIT_COMMIT_ID\" ''' } } } // checkout 配置库 stage ('checkout config repo ...') { steps { checkout([$class: 'GitSCM', branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[credentialsId: \"$GIT_CREDENTIAL_ID\", url: \"http://${GIT_CONFIG_REPO_URL}\"]]]) } } // 更改demo-config库下demo/demo.yaml文件内镜像tag // 提交更改至demo-config库 stage ('commit config repo changes ...') { steps { withCredentials([usernamePassword(credentialsId: \"$GIT_CREDENTIAL_ID\", passwordVariable: 'GIT_PASSWORD', usernameVariable: 'GIT_USERNAME')]) { sh ''' echo \"#$GIT_COMMIT_ID#\" tag=$DEMO_IMAGE_TAG:$GIT_COMMIT_ID tag=$tag yq eval \".spec.template.spec.containers[0].image = strenv(tag)\" -i demo/demo.yaml git add demo/demo.yaml git commit -m \"modify version\" git config --global push.default simple git push http://$GIT_USERNAME:$GIT_PASSWORD@${GIT_CONFIG_REPO_URL} HEAD:master ''' } } } } } Dockerfile内容 FROM harbor.wl.com/library/maven:3.8.1 AS builder WORKDIR /usr/local ADD . . RUN mvn clean package FROM harbor.wl.com/library/openjdk-1.8:alpine COPY --from=builder /usr/local/demo/target/demo-0.0.1-SNAPSHOT.jar /opt/app.jar EXPOSE 8080 配置库关键文件 demo-config配置库层级及清单内容 层级 demo-config └── demo ├── demo-svc.yaml └── demo.yaml demo.yaml内容： apiVersion: apps/v1 kind: Deployment metadata: name: demo-app labels: app: demo-app spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: demo-app template: metadata: labels: app: demo-app spec: containers: - name: demo-app image: harbor.wl.com/library/demo:da28fcb imagePullPolicy: Always args: - java - '-Xms2048m' - '-Xmx2048m' - '-jar' - /opt/app.jar - '--server.port=8080' - '--spring.profiles.active=dev' livenessProbe: failureThreshold: 10 httpGet: path: /actuator/health port: 7002 scheme: HTTP initialDelaySeconds: 30 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 10 readinessProbe: failureThreshold: 10 httpGet: path: /actuator/health port: 7002 scheme: HTTP initialDelaySeconds: 30 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 10 ports: - containerPort: 8080 name: http-8080 protocol: TCP dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 demo-svc.yaml内容 --- apiVersion: v1 kind: Service metadata: name: demo-svc labels: app: demo-svc spec: ports: - name: http-8080 port: 80 protocol: TCP targetPort: 8080 selector: app: demo-app sessionAffinity: None type: ClusterIP harbor库配置信息 配置镜像清理策略，以免垃圾镜像过多 argocd配置信息 配置仓库 web控制台进入仓库配置界面 点击CONNECT REPO USING HTTPS添加仓库 配置相关信息点击CONNECT 查看项目下仓库状态 配置集群 点击设置->集群 编辑集群信息，namespace值为空（保存后会自动填充为All namespaces） 创建项目（逻辑分组） 点击设置->项目 创建demo项目 配置项目关联的git仓库与k8s集群信息 创建应用 新建应用 配置应用 至此流程配置完毕 样例应用 以下展示实际开发项目的cd应用 应用关联的资源对象 应用同步信息 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"4.持续集成&交付/argo/04-最佳实践.html":{"url":"4.持续集成&交付/argo/04-最佳实践.html","title":"04-最佳实践","keywords":"","body":"最佳实践 分离配置库和源代码库 使用单独的Git存储库来保存kubernetes清单，将配置与应用程序源代码分开，强烈推荐使用，原因如下: 清晰分离了应用程序代码与应用程序配置。有时您希望只修改清单，而不触发整个CI构建。 例如，如果您只是希望增加部署规范中的副本数量，那么您可能不希望触发构建（由于构建周期可能较长） 更清洁的审计日志。出于审计目的，只保存配置库历史更改记录，而不是掺有日常开发提交的日志记录。 微服务场景下，应用程序可能由多个Git库构建的服务组成，但是作为单个单元部署（比如同一pod内）。 通常，微服务应用由不同版本和不同发布周期的服务组成(如ELK, Kafka + Zookeeper)。 将配置清单存储在单个组件的一个源代码库中可能没有意义 访问的分离。开发应用程序的开发人员不一定是能够/应该推送到生产环境的同一个人，无论是有意的还是无意的。 通过使用单独的库，可以将提交访问权限授予源代码库，而不是应用程序配置库 自动化CI Pipeline场景下，将清单更改推送到同一个Git存储库可能会触发构建作业和Git提交触发器的无限循环。 使用一个单独的repo来推送配置更改，可以防止这种情况发生。 确保在Git版本中的清单是真正不可变的 当使用像helm或kustomize这样的模板工具时，清单的内容可能会随着时间的推移而改变。 这通常是由对上游helm库或kustomize库的更改引起的。 以下面kustomization.yaml为例 bases: - github.com/argoproj/argo-cd//manifests/cluster-install 由于这不是一个稳定的目标，因此这个自定义应用程序的清单可能会突然改变内容，甚至不需要对自己的Git存储库进行任何更改。（比如git master分支） 更好的选择是使用Git标记或提交SHA的版本。例如: bases: - github.com/argoproj/argo-cd//manifests/cluster-install?ref=v0.11.1 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"4.持续集成&交付/gitlab-ci/git-runner.html":{"url":"4.持续集成&交付/gitlab-ci/git-runner.html","title":"git-runner","keywords":"","body":"离线安装 下载地址 安装 yum localinstall gitlab-runner-13.11.0-1.x86_64.rpm -y 启动 systemctl enable gitlab-runner --now 编译安装git-cli ```shell script curl -L https://mirrors.edge.kernel.org/pub/software/scm/git/git-2.9.5.tar.xz -o ./git-2.9.5.tar.xz -k yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel gcc perl-ExtUtils-MakeMaker -y tar xvf git-2.9.5.tar.xz cd git-2.9.5 ./configure --prefix=/usr/local/git make && make install cat >> ~/.bash_profile . ~/.bash_profile > 安装docker ```shell groupadd docker gpasswd -a gitlab-runner docker newgrp docker 重启docker 配置 注册 gitlab-runner register # 键入gitlab地址 配置 1.集成k8s集群 1.获取api-server地址 kubectl cluster-info | grep -E 'Kubernetes master|Kubernetes control plane' | awk '/http/ {print $NF}' 2.获取ca证书 caTokenName=`kubectl get secrets|grep default-token|awk '{print $1}'` kubectl get secret $caTokenName -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 --decode 3.获取用户token 创建用户 cat 获取token kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep gitlab | awk '{print $1}') Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-30 16:59:02 "},"4.持续集成&交付/gitlab-ci/yq.html":{"url":"4.持续集成&交付/gitlab-ci/yq.html","title":"yq","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-30 19:47:42 "},"4.持续集成&交付/gitlab-ci/变量定义与引用.html":{"url":"4.持续集成&交付/gitlab-ci/变量定义与引用.html","title":"变量定义与引用","keywords":"","body":" 按分支定义不同变量 workflow: rules: - if: $CI_COMMIT_BRANCH == \"java8-runtime-base\" variables: CI_IMAGE_TAG: \"base\" - if: $CI_COMMIT_BRANCH == \"java8-runtime-advance\" variables: CI_IMAGE_TAG: \"advance\" - if: $CI_COMMIT_BRANCH == \"java8-runtime-base\" variables: CI_IMAGE_TAG: \"base\" - if: $CI_COMMIT_BRANCH == \"java8-runtime-base\" variables: CI_IMAGE_TAG: \"base\" - if: $CI_COMMIT_BRANCH == \"java8-runtime-base\" variables: CI_IMAGE_TAG: \"base\" Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-31 14:15:29 "},"4.持续集成&交付/jenkins/01-jenkins安装.html":{"url":"4.持续集成&交付/jenkins/01-jenkins安装.html","title":"01-jenkins安装","keywords":"","body":"jenkins安装 方便起见我们假设jenkins宿主机IP为 192.168.1.2 1.环境依赖 系统要求 最低推荐配置: 256MB可用内存,1GB可用磁盘空间(作为一个Docker容器运行jenkins的话推荐10GB) 为小团队推荐的硬件配置: 1GB+可用内存,50GB+可用磁盘空间 软件配置: Java8—无论是Java运行时环境（JRE）还是Java开发工具包（JDK）都可以。 2.安装jdk1.8 yum install -y java-1.8.0-openjdk.x86_64 java-1.8.0-openjdk-devel.x86_64 3.安装jenkins wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key yum install jenkins -y 4.调整默认配置 sed -i \"s#JENKINS_PORT=\\\"8080\\\"#JENKINS_PORT=\\\"8081\\\"#g\" /etc/sysconfig/jenkins sed -i \"s#JENKINS_ARGS=\\\"\\\"#s#JENKINS_ARGS=\\\"--prefix=/jenkins\\\"#g\" /etc/sysconfig/jenkins 5.启动并开机自启动 systemctl enable jenkins --now 6.初始化jenkins账号 浏览器访问http://192.168.1.2:8081/jenkins 根据提示获取管理员初始密码 cat /var/lib/jenkins/secrets/initialAdminPassword 根据提示安装默认插件（默认即可，后续更改插件源地址，按需下载） 根据提示创建新用户 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"4.持续集成&交付/jenkins/02-插件管理.html":{"url":"4.持续集成&交付/jenkins/02-插件管理.html","title":"02-插件管理","keywords":"","body":"插件管理 浏览器访问http://192.168.1.2:8081/jenkins/pluginManager/advanced 按需配置代理 更改插件源为国内源 更改为清华源 https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json 安装以下插件 Credentials Binding Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"4.持续集成&交付/jenkins/03-github配置webhook.html":{"url":"4.持续集成&交付/jenkins/03-github配置webhook.html","title":"03-github配置webhook","keywords":"","body":"github配置webhook 登录GitHub，进入要本次构建用到的工程 在工程主页面点击右上角的Settings，再点击左侧Webhooks，然后点击Add webhook，如下图： 如下图，在Payload URL位置填入webhook地址，再点击底部的Add webhook按钮，这样就完成webhook配置了， 今后当前工程有代码提交，GitHub就会向此webhook地址发请求，通知`Jenkins``构建： http://192.168.1.2:8081/jenkins/github-webhook push触发效果 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"4.持续集成&交付/jenkins/04-github生成Tocken.html":{"url":"4.持续集成&交付/jenkins/04-github生成Tocken.html","title":"04-github生成Tocken","keywords":"","body":"github生成Tocken 图片内地址链接 选择Personal access tokens 选择Generate new tocken 勾选repo与admin:repo_hook 点击最下方绿色按钮Generate tocken，复制该tocken jenkins配置github Tocken 访问jenkins系统设置页面，找到配置Github 服务器 生成凭据 配置凭据 测试连通性 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"4.持续集成&交付/jenkins/111-trouble-shooting.html":{"url":"4.持续集成&交付/jenkins/111-trouble-shooting.html","title":"111-trouble-shooting","keywords":"","body":"常见问题 安装插件时提示：No valid crumb was included in the request 更改配置地址 http://192.168.1.2:8081/jenkins/configureSecurity/ 解决方案： 在jenkins 的Configure Global Security下 , 取消“防止跨站点请求伪造（`Prevent Cross Site Request Forgery exploits）”的勾选。 GitHub webhook触发时You are authenticated as: anonymous 403 解决方案：99%是tocken过期了，重新生成并配置 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"4.持续集成&交付/jenkins/jenkinsfile.html":{"url":"4.持续集成&交付/jenkins/jenkinsfile.html","title":"jenkinsfile","keywords":"","body":" 使用预定义密钥构建 stage('build java8-centos-office...') { steps { withCredentials([usernamePassword(passwordVariable : 'DOCKER_PASSWORD' ,usernameVariable : 'DOCKER_USERNAME' ,credentialsId : \"$DOCKER_CREDENTIAL_ID\" ,)]) { sh ''' make all ''' } } } Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-31 14:13:59 "},"5.cncf/cncf.html":{"url":"5.cncf/cncf.html","title":"cncf","keywords":"","body":" CNCF基金会 CNCF生态 CNCF介绍 加入CNCF好处 基金会 CNCF宪章 CNCF使命 CNCF角色 CNCF价值观 CNCF会员制 CNCF理事会 TOC技术监督委员会 终端用户社区 终端用户技术咨询委员会(End User TAB) CNCF项目 营销委员会 知识产权策略 反垄断指南 行为准则 关联公司 CNCF项目 CNCF项目提交流程 沙盒阶段 项目孵化流程 相关术语 CLA TODO CNCF毕业项目落地使用 CNCF基金会 CNCF生态 全景图 CNCF介绍 CNCF为开源项目提供了强大的服务支柱，围绕着维持大多数项目需求的目标而建立，而不仅仅是代码管理和技术决策。 我们通过专业人员提供一组增强的服务，这些专业人员培养本地云开源项目的成熟度和更多的采用。 我们采用数据驱动的方法与我们的项目和维护人员社区一起工作; 我们积极进行调查，以改善我们的服务和社区对我们的满意程度，并为社区提供优质服务。 加入CNCF好处 基金会 1.开源项目的中立组织增加了来自企业软件公司、初创公司和独立开发人员协作、贡献和成为提交者的意愿 2.CNCF的技术监督委员会是技术管理机构，由有文件记录的负责人指导，并承认和监督所有项目 3.被CNCF TOC(CNCF技术监督委员会)接纳的项目，证明你的项目质量达标。 4.加入CNCF基金会的项目，项目所有者不会变，CNCF基金会为项目提供一个文档化良好的、中立的管理过程。 5. CNCF宪章 github Linux基金会 2015年11月6日生效/ 2018年12月10日更新 CNCF使命 CNCF的使命是让云原生计算无处不在，CNCF云原生v1.0定义内容: 云原生技术有利于各组织在现代化动态环境（公有云、私有云和混合云等）中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API。 这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。 云原生计算基金会（CNCF）致力于培育和维护一个厂商中立的开源生态系统，来推广云原生技术。我们通过将最前沿的模式民主化，让这些创新为大众所用。 CNCF角色 CNCF将在开源社区中扮演一个负责以下内容的角色: 云原生项目管理 确保技术对社区开放，不受党派影响 确保技术的品牌(商标和标识)得到社区成员的关注和适当使用，特别强调统一的用户体验和高水平的应用程序兼容性 促进云原生生态系统的发展 评估应该添加哪些额外的技术来满足云原生应用程序的愿景，努力鼓励社区提供这些服务，予以整合并推进进度 提供促进各领域通用技术标准落地方式 云原生基础技术的推广，以及云原生应用程序定义和管理的方法，包括:活动和会议、营销(SEM，直接营销)、培训课程和开发者认证 通过使云原生技术可及和可靠的方式服务于社区 CNCF基金会致力于提供云原生标准架构各领域的组件 CNCF价值观 CNCF将努力坚持以下原则: 快 推进项目高速发展，以支持用户能够积极的使用 开放 基金会是开放的、可访问的，并且独立于特定的党派利益运作。基金会根据贡献者的贡献的价值接受所有贡献者，并且基金会的技术必须根据开放源码的价值对所有人开放。技术社区及其决定应该是透明的 公平 基金会将避免不当影响、不良行为或“付费”决策 雄厚的技术身份 基金会将实现并维持它自己的高度的技术识别，并且跨项目共享 清晰的界限 基金会应该建立明确的目标，在某些情况下，确定什么不是基金会的目标，并帮助生态系统了解新的创新应该集中在哪里 可伸缩的 能够支持所有规模的部署，从小型的以开发人员为中心的环境到企业和服务提供商的规模。这意味着在某些部署中可能不部署某些可选组件，但总体设计和体系结构仍然适用。 跨平台 所开发的规范不会是特定于平台的，这样它们就可以在各种架构和操作系统上实现。 CNCF会员制 CNCF由白金会员、黄金会员、白银会员、终端用户、学术和非盈利会员组成，不同级别的会员在理事会中的投票权不同。 所有的成员申请将由Linux基金会审查,基金会将决定是否将该申请人归类为CNCF会员、终端用户、学术/非营利或CNCF会员供应商 白金会员权益 任命一名代表加入CNCF理事会 在任何小组委员会或理事会活动中指定一名代表作为有表决权的成员 享受最突出的位置在会员展示，包括在网站上 如果该成员也是经过批准的终端用户，则享有终端用户成员的所有特权 黄金会员权益 每五名黄金会员指派一名黄金会员代表加入CNCF理事会，最多有三名黄金会员代表 如果该成员也是经过批准的终端用户，则享有终端用户成员的所有特权 白银会员权益 每十名白银会员指派一名白银会员代表加入CNCF理事会，最多有三名白银会员代表 如果该成员也是经过批准的终端用户，则享有终端用户成员的所有特权 CNCF理事会 CNCF理事会负责CNCF的营销和其他业务监督以及预算决策。 理事会不为CNCF做技术决定，只是和TOC(技术监督委员会)一起协作，为CNCF设定云原生定义中所述的技术决策 理事会处理以下商业事务： 在与CNCF技术委员会协商后，确定CNCF的整体范围 定义及执行有关使用基金会商标及版权的政策 指导市场营销，包括布道，活动和生态系统参与 按需建立品牌合规管理制度 监督运营，业务发展 筹资和财务治理 理事会组织结构：理事会投票成员由成员代表和技术社区代表组成 会员代表包括：每名白金会员委任一名代表、每五名黄金会员指派一名黄金会员代表、每十名白银会员指派一名白银会员代表 技术社区代表包括：技术委员会主席、从CNCF项目中选出两名经理事会批准的技术委员会成员 对于被理事会视为战略技术贡献者、营收低于5000万美元的初创公司，理事会可以按银牌会员比例逐年延长其白金会员资格，最长可延长5年。 同类型企业中最多有两个代表，且一人作为会员代表，一人作为技术社区代表 理事会职责 资金管理：指导使用筹集的资金，用于推云原生技术、营销或社区投资 选举理事会主席来主持会议，审批资金支出并管理日常业务 对理事会的决定或事项进行表决 指定执行有关基金会知识产权(版权、专利或商标)的政策 通过活动、新闻、网络、社交和其他营销活动进行直接营销和宣传 监督运营，业务发展 建立和监督为推动CNCF使命而创建的委员会 基于CNCF需求，制定执行品牌合规计划，包括认证测试，使用TOC建立的品牌标志 制定商标的使用方针或政策 提供全面的财务治理 资金收入用于下列用途: CNCF项目推广 关键基础设施的建设与运营 推广云原生概述的基于容器的计算原理，并通过CNCF的项目实施 TOC技术监督委员会 权力职责：TOC期望通过以下方式推动达成中立共识: 定义和维护CNCF的技术远景 批准董事会确定的CNCF范围内的新项目，并为这些项目建立概念架构 调整、删除或归档项目 接受来自终端用户委员会的反馈并映射到项目 将接口与管理下的组件对齐（标准化前的代码参考实现） 定义在CNCF项目中实施的通用实践 组织结构： 目前11名成员 当选的TOC成员涵盖关键技术领域：容器技术、操作系统、技术操作、分布式系统、用户级应用程序设计等 投票权：理事会6票，终端用户2票，非沙盒项目维护者1票，TOC2票。 如果有超过2(2)个TOC成员来自同一组关联公司，无论是在选举时，还是在后来的工作变动中，由TOC成员共同决定谁下台，如果没有达成一致，则随机抽签 运营模式： 选举出TOC主席，制定议程，召开理事会会议 定期举行面对面的会议，讨论关键问题 TOC会在需要时召开会议，讨论新出现的问题。问题可由以下人员提出，供TOC审查： TOC成员 董事会成员 CNCF顶级项目的负责人 CNCF执行董事 终端用户技术咨询委员会多数票通过 透明度:TOC定期召开公开的TOC会议，所有与项目有关的决定应在这些会议、公开邮件列表或公开问题上作出 简单的TOC问题可以通过简短的讨论和多数投票来解决。可以通过电子邮件或在TOC的会议上进行讨论 审核意见后，确定方案，寻求共识，并在必要时进行表决 其目的是让TOC找到一条在TOC和社区内达成共识的途径。在符合法定人数之会议上，理事会的决定，要求应以超过50%的委员投票通过 TOC会议的法定人数应为TOC全体成员的三分之二，以进行表决或作出任何决定。如果TOC会议未能达到法定人数要求，可进行讨论，但不得进行表决或作出决定 TOC的决定可以在不召开会议的情况下通过电子方式作出，但要通过投票，要求达到会议法定人数所需的票数。在电子投票过程中，如果任何两(2)TOC成员要求召开会议讨论该决定，电子投票过程将无效终止，会议结束后可以发起新的投票讨论该决定 提名标准。TOC的提名人应: 承诺有足够的时间投入CNCF TOC 在CNCF领域内具有高级工程师的专业经验 在讨论中保持中立，将CNCF的目标和成功与公司目标或CNCF的任何特定项目相平衡 TOC成员提名及选举流程： 提名：每个选择组（理事会选择组、终端用户选择组、非沙盒项目维护者选择组）中的每个人最多可提名两人，其中最多一人可来自同一组关联公司。每一位被提名人在被加入提名名单之前必须同意参加 提名要求最多一(1)页的提名陈述，其中应包括被提名人的姓名、联系信息和证明被提名人在CNCF领域的经验的支持性陈述 理事会应决定TOC成员提名、资格和选举的流程和时间表 评估期应至少为14个工作日，理事会成员和TOC成员可在该期间联系TOC的提名人 资格确认：评估期结束后，理事会和TOC成员应分别对每一位被提名者进行投票，以确认被提名者符合资格标准。有效的投票要求至少有50%的参与率。得票率超过50%的候选人为合格候选人 选举：如果合格的候选人数目等于或少于TOC可供选举的席位数目，合格的候选人将在提名期结束后获得批准。如果被提名者多于TOC席位可供选举数量，则继续从提名者中投票选出TOC成员。 新一轮提名：如合格的提名人数少于选举小组可供选举的TOC席位，则选举小组进行新一轮提名 TOC推选的TOC成员可以提名并有资格参加选举，但在选举时不能投票 成员约束： TOC成员任期两年，交错任期 TOC成员可由其他TOC成员的三分之二投票罢免，受影响的个人无资格参加投票 任何未能连续三（3）次参加会议的TOC成员，在连续参加两次会议之前，自动取消其投票资格。 TOC议程将由TOC确定。TOC讨论和决定包含: 评估CNCF中包括的技术 制定新技术纳入CNCF的验收标准 定义将贡献的技术批准为标准API的过程 终端用户社区 CNCF的终端用户成员应有权协调和驱动CNCF用户作为CNCF设计的消费者的重要活动。 任何作为终端用户的会员或非会员，均为“终端用户参与者”，均应被邀请参加。 终端用户参与者将帮助就与用户相关的主题向技术咨询委员会和CNCF社区提供投入 终端用户社区成员应选举一个终端用户技术咨询委员会 终端用户社区成员将由CNCF执行董事批准，如果不存在，则由Linux基金会执行董事批准 终端用户技术咨询委员会(End User TAB) 人员组成： 终端用户技术咨询委员会由来自终端用户参与者的7名代表和TOC的1名成员组成，以便于从终端用户技术咨询委员会输入到TOC 选举： 为了鼓励终端用户参与CNCF，前七（7）名终端用户成员可指定一（1）名代表参加终端用户咨询委员会，剩余席位由CNCF董事分配给终端用户参与者。 在最初的一年之后，所有终端用户参与者可提名一（1）名代表，终端用户社区应通过当时的终端用户咨询委员会批准的流程投票选择终端用户咨询委员会成员 终端用户咨询委员会可以以三分之二的票数更改终端用户的数量，前提是至少有七（7）名终端用户技术咨询委员会代表 终端用户代表应在业务和技术上具有敏锐的洞察力。被提名者应在建设和运营基础设施和应用方面具有重要的实践经验，这些经验体现了CNCF的原则 终端用户技术咨询委员会将讨论和推进各项议题，重点是找出差距，并为TOC和CNCF社区开发人员提出优先事项 终端用户技术咨询委员会还可以侧重于主动提出终端用户关注的话题、促进CNCF的市场采用、为终端用户主持会议或向理事会提供建议 如果终端用户技术咨询委员会需要，它可以批准Special Interest Groups（SIGs）来处理行业或专门主题 TOC的终端用户技术委员会输入应与其他输入和反馈一起进行，以便TOC做出决策和计划。建议仅为咨询性建议，在任何情况下， 终端用户技术咨询小组的建议均不得用于命令或指示任何TOC或项目参与方采取任何行动或取得任何结果。 CNCF项目 成员公司和开放源码社区成员将把项目资产提交TOC讨论，并纳入CNCF所有此类捐款均应符合技术选择委员会制定并经理事会批准的一套标准。 我们的目标是使越来越多的项目与已经被CNCF接受的项目结合在一起。 项目可通过以下三种方式与CNCF进行关联: 纳入CNCF，在一个中立的合作之家 项目的所有方面都由CNCF管理 该项目被CNCF称为CNCF项目 该项目应该是CNCF解决方案的核心功能部分。（例如Kubernetes、Mesos、etcd等） 通过API或规范与CNCF相关联 包括CNCF可能提供或启用多个选项的组件 该项目被称为与CNCF集成的组件，而不是由CNCF托管的项目 被CNCF使用 在osi批准的开源许可下完全获得许可的项目或组件，并且在CNCF中作为组件得到良好的管理和使用 CNCF未积极推广的项目 上游社区积极开发完成的项目或组件 现有的开源项目应该继续通过它们现有的技术治理结构来保持凝聚力和速度。 经技术监督委员会批准列入CNCF的项目将“轻微”接受技术监督委员会的监督 应根据个人的贡献水平和持续时间，在各个项目中建立一个标准协议， 以实现提交者的地位。维护者状态是通过一段时间内对给定项目的贡献和对等提交者的验证来实现的 在CNCF发起的新开源项目应完成TOC采用的项目建议书模板，并由TOC批准纳入CNCF。TOC成员应被给予足够的时间来讨论和审查新的项目建议。 新的项目提案应包括项目中角色的详细信息，为项目提出的治理方案，并确定与CNCF角色和价值观的一致性 营销委员会 组成:营销委员会将开放给所有成员参加。应选举一名营销委员会主席，以制定会议议程，引导讨论，并帮助委员会实现其目标。 营销委员会应在可能的情况下寻求共识。任何不能在市场委员会中达成大致共识的问题应提交给理事会 职责：营销委员会应代表董事会负责开展、执行营销工作 如果市场营销委员会规模过大，无法有效运作，市场营销委员会可以选择选举经销管理局，并将决策权下放给经销管理局 知识产权策略 a.任何加入到CNCF的项目必须将其商标和标识资产的所有权转移给Linux基金会 b.每个项目应确定是否需要使用经批准的CNCF CLA，对于选择使用CLA的项目，所有代码贡献者都将承担Apache贡献者许可协议中规定的义务，只有在必要时才会修改，以确定CNCF是贡献的接受者，该协议应得到治理委员会的批准。 c.所有提交贡献CNCF项目的新入站代码应: 附带开发者源签字证书 源代码基于Apache License, Version 2.0 许可, 该许可是对(b)中规定的贡献许可协议项下所承担的义务的补充，且不应取代 d.所有出站代码都将在 Apache许可2.0 版本下提供 e.所有被评估纳入CNCF的项目都应该完全按照osi批准的开源许可证进行许可。如果CNCF中包含的项目许可证不是 Apache许可2.0 版，则需要得到理事会的批准。 f.CNCF将根据Creative Commons Attribution 4.0 International License(知识共享署名4.0国际许可协议)接收并提供所有文档。 反垄断指南 所有成员都应遵守Linux基金会反垄断政策中对Linux基金会的要求，详见 所有成员应鼓励任何能够满足成员资格要求的组织公开参与，而不考虑竞争利益。 换言之，理事会不应试图以任何标准、要求或理由排除所有成员所使用的标准、要求或理由以外的任何标准、要求或理由 行为准则 所有参与者同意遵守Linux基金会的行为准则，详见。TOC可以投票通过自己的CNCF社区行为准则 关联公司 关联公司的定义： “子公司”是指成员直接或间接拥有该实体百分之五十以上表决权证券或成员权益的任何实体 关联公司是指任何实体控制或由一个成员或控制,与成员一起,共同控制的第三方,在每种情况下,这样的控制结果所有权,直接或间接,超过百分之五十的投票证券或会员利益的实体问题 关联公司”是指各为成员关联公司的实体 只有签署了参与协议的法人实体及其附属公司才有权享受该成员资格的权利和特权;但是，该成员及其附属公司应被视为一个单一成员 一组关联公司中只有一名成员有权一次任命或提名一名董事会代表进行成员类别选举 如果成员本身是基金会、财团、开源项目、会员组织、用户组或其他具有成员或发起人的实体， 则授予该成员的权利和特权仅限于该成员的雇员代表，而不限于其成员或发起人，除非在特定情况下理事会另有批准。 成员资格应不可转让、不可出售和不可转让，除非任何成员可以通过合并、 出售或其他方式将其现有的成员资格利益和义务转让给其几乎所有业务和/或资产的继承人； 前提是受让人同意受本章程和Linux基金会成员要求的规章制度和政策的约束 CNCF项目 CNCF项目提交流程 v1.3 主要分为以下阶段： 沙盒 孵化 毕业 沙盒阶段 沙盒阶段目标： 通过确保所有项目遵守CNCF的法律约束、行为准则和知识产权政策要求，消除可能的法律和治理障碍 Promote an environment that encourages visibility of experiments to the TOC and SIGs and early work that can add value to the CNCF mission and collaboration across the CNCF community 如果（且仅当）需要，促进与现有项目的一致性 通过CNCFService Desk请求培育项目 （注意:沙盒项目不提供营销协助） 沙盒提交流程: 项目通过提交一个 表单 来申请加入沙盒，该表单在公共可见的电子表格中填充一行。表格待定，但将包括以下内容： 必需项：项目的GitHub链接地址或其他公共源代码存储库上的项目链接地址 可选项：项目网站链接地址 必需项：复选框，确认如果项目被CNCF接受，需要遵循CNCF IP政策 必需项：复选框，确认将给予CNCF项目的商标和帐户，如果CNCF接受该项目 必需项：对云原生的简短描述 必需项：与同类项目对比 必需项：链接到公共路线图（可以是任何格式-文档、GitHub issues、项目板等。这个要求很简单，只是要求提供有一些关于项目未来方向和目标的公共文档） 必需项：链接到贡献指南 必需项：链接到行为准则 可选项: 与现有CNCF项目保持一致的声明 可选项：链接到预先存在的演示文稿（胶片/视频） CNCF工作人员检查提交表单内容是否满足沙盒提交标准，如果目前不符合这些标准，则建议项目维护人员提供上述必须项 TOC每季度参考以下标准审查一次提交的沙盒项目： TOC评估该项目是否适合CNCF 项目的路线图是否与CNCF的目标一致? 该项目是否处于良好的治理,且厂商中立 在提交审查期间，技术咨询委员会对每一份提交的文件进行表决， 如果被拒绝，我们将指出TOC认为该项目不符合的标准。欢迎项目提出反馈意见，并在未来再次提交。除非另有通知，否则项目在六个月内不得再次提交。 沙盒项目仍然受 年度审查程序 的约束 项目孵化流程 项目计划书： 通过创建 GitHub issue 提出项目孵化 SIG评估1-2个月 项目提交给SIG SIG预 尽职调查 确认项目满足孵化要求 SIG向TOC提出建议：沙箱/孵化/拒绝 注意，这个步骤仍然是一个轻量级的、dd(尽职调查)之前的过程 TOC成员提出项目孵化情况 如果TOC成员作为项目孵化发起人，则开始进行尽职调查 2-3个月时间进行尽职调查 TOC孵化发起人推动尽职调查(参见 模板 和 指南 ) TOC孵化发起人可将尽职调查工作委托给CNCF SIG 或 其他TOC成员 TOC孵化发起人可以要求项目维护者首先完成尽职调查模板 CNCF员工做项目管理与合法性的尽职调查 在尽职调查期间，一些谈话可能是私下进行的（例如，用户希望保持匿名的用户访谈），并使用酌情权进行记录。 TOC孵化发起人确定尽职调查何时完成。尽职调查文档应该在GitHub上，公开给公众评论 2-6周时间进行审查尽职调查结果 尽职调查文件可供公众审查和评论（GitHub、TOC邮件列表、TOC公开会议） TOC孵化发起人决定何时召集TOC投票，在召集投票前至少留出两周时间征求公众意见 6周时间进行TOC投票 TOC成员评估项目是否符合孵化标准 如果2/3的TOC成员投赞成票，项目将被接受孵化。 如果投票在6周后仍没有结论，TOC主席可以延长投票，或得出沉默=弃权的结论 TODO 相关术语 CLA Contributor License Agreement的缩写，大致包含以下两个内容 Grant of Copyright License：贡献者拥有代码的版权（copyright），同时授权组织者和项目的所有使用者使用这个版权。 Grant of Patent License：贡献者的贡献如果申请了专利（patent）保护，那么贡献者持有专利，同时授权组织者和项目使用者使用这个专利。 CLA与开源许可（Apache 2.0、MIT、BSD License等）区别 CLA 定义代码贡献者和项目组织者的法律权责 开源许可定义的是贡献者和使用者之间的法律权责 TODO CNCF毕业项目落地使用 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"5.cncf/kubevirt.html":{"url":"5.cncf/kubevirt.html","title":"kubevirt","keywords":"","body":"集成部署 依赖检测 内核：5.10.2-1.el7.elrepo.x86_64 操作系统：CentOS Linux release 7.9.2009 (Core) docker：20.10.1 kubernetes：v1.18.6 KubeVirt：v0.35.0 检测宿主是否满足虚拟化条件 安装libvirt-client yum install -y libvirt-client 检测 [root@node3 ~]# virt-host-validate qemu QEMU: Checking for hardware virtualization : PASS QEMU: Checking if device /dev/kvm exists : PASS QEMU: Checking if device /dev/kvm is accessible : PASS QEMU: Checking if device /dev/vhost-net exists : PASS QEMU: Checking if device /dev/net/tun exists : PASS QEMU: Checking for cgroup 'memory' controller support : PASS QEMU: Checking for cgroup 'memory' controller mount-point : PASS QEMU: Checking for cgroup 'cpu' controller support : PASS QEMU: Checking for cgroup 'cpu' controller mount-point : PASS QEMU: Checking for cgroup 'cpuacct' controller support : PASS QEMU: Checking for cgroup 'cpuacct' controller mount-point : PASS QEMU: Checking for cgroup 'cpuset' controller support : PASS QEMU: Checking for cgroup 'cpuset' controller mount-point : PASS QEMU: Checking for cgroup 'devices' controller support : PASS QEMU: Checking for cgroup 'devices' controller mount-point : PASS QEMU: Checking for cgroup 'blkio' controller support : PASS QEMU: Checking for cgroup 'blkio' controller mount-point : PASS QEMU: Checking for device assignment IOMMU support : PASS QEMU: Checking if IOMMU is enabled by kernel : WARN (IOMMU appears to be disabled in kernel. Add intel_iommu=on to kernel cmdline arguments) 处理IOMMU告警 方法一 修改GRUB_CMDLINE_LINUX添加intel_iommu=on 修改前 [root@node3 ~]# cat /etc/default/grub GRUB_TIMEOUT=5 GRUB_DISTRIBUTOR=\"$(sed 's, release .*$,,g' /etc/system-release)\" GRUB_DEFAULT=saved GRUB_DISABLE_SUBMENU=true GRUB_TERMINAL_OUTPUT=\"console\" GRUB_CMDLINE_LINUX=\"crashkernel=auto rd.lvm.lv=centos00/root rhgb quiet\" GRUB_DISABLE_RECOVERY=\"true\" 修改后 GRUB_TIMEOUT=5 GRUB_DISTRIBUTOR=\"$(sed 's, release .*$,,g' /etc/system-release)\" GRUB_DEFAULT=saved GRUB_DISABLE_SUBMENU=true GRUB_TERMINAL_OUTPUT=\"console\" GRUB_CMDLINE_LINUX=\"crashkernel=auto rd.lvm.lv=centos00/root rhgb quiet intel_iommu=on\" GRUB_DISABLE_RECOVERY=\"true\" 重建内核引导文件 grub2-mkconfig -o /boot/grub2/grub.cfg dracut --regenerate-all --force 重启 reboot 验证 cat /proc/cmdline |grep intel_iommu=on 如返回为空，尝试方法二 方法二 查询引导文件 [root@node3 ~]# find / -name \"grub.cfg\" /boot/efi/EFI/centos/grub.cfg /boot/grub2/grub.cfg 修改/boot/efi/EFI/centos/grub.cfg文件内容 修改前 linuxefi /vmlinuz-5.10.2-1.el7.elrepo.x86_64 root=/dev/mapper/centos00-root ro crashkernel=auto rd.lvm.lv=centos00/root rhgb quiet LANG=en_US.UTF-8 修改后 linuxefi /vmlinuz-5.10.2-1.el7.elrepo.x86_64 root=/dev/mapper/centos00-root ro crashkernel=auto rd.lvm.lv=centos00/root rhgb quiet intel_iommu=on LANG=en_US.UTF-8 重建内核引导文件 grub2-mkconfig -o /boot/grub2/grub.cfg dracut --regenerate-all --force 重启 reboot 验证 cat /proc/cmdline |grep intel_iommu=on 部署KubeVirt operator 下载kubevirt-operator.yaml v0.35.0版本下载链接 下载kubevirt operator所需镜像 镜像列表 kubevirt/virt-operator:v0.35.0 kubevirt/virt-api:v0.35.0 kubevirt/virt-controller:v0.35.0 kubevirt/virt-handler:v0.35.0 修改镜像tag，修改后如下 harbor.neusoft.com/kubevirt/virt-operator:v0.35.0 harbor.neusoft.com/kubevirt/virt-api:v0.35.0 harbor.neusoft.com/kubevirt/virt-controller:v0.35.0 harbor.neusoft.com/kubevirt/virt-handler:v0.35.0 推送至私有仓库 docker push harbor.neusoft.com/kubevirt/virt-operator:v0.35.0 docker push harbor.neusoft.com/kubevirt/virt-api:v0.35.0 docker push harbor.neusoft.com/kubevirt/virt-controller:v0.35.0 docker push harbor.neusoft.com/kubevirt/virt-handler:v0.35.0 上传kubevirt-operator.yaml并调整镜像tag ... containers: - command: - virt-operator - --port - \"8443\" - -v - \"2\" env: - name: OPERATOR_IMAGE value: harbor.neusoft.com/kubevirt/virt-operator:v0.35.0 - name: WATCH_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.annotations['olm.targetNamespaces'] - name: KUBEVIRT_VERSION value: v0.35.0 - name: VIRT_API_SHASUM value: sha256:bf38c1997f3c60a71d53b956f235973834d37c0c604b5711084b2a7ef8cd3c7b - name: VIRT_CONTROLLER_SHASUM value: sha256:7b81c59034df51c1a1f54d3180e0df678469790b6a8ac4fcc5fcaa615b1ca84c - name: VIRT_HANDLER_SHASUM value: sha256:14b4bd6d62b585ef2f4dbacafc75a66a6c575c64ed630835cf4ef6c0f77d40d1 - name: VIRT_LAUNCHER_SHASUM value: sha256:a6d9f1dada1d33a218ba9ed0494d2e2cd09f5596eff5eb5b8d70bfe1fd4f8812 image: harbor.neusoft.com/kubevirt/virt-operator:v0.35.0 ... 上传k8s节点发布 kubectl apply -f kubevirt-operator.yaml 部署KubeVirt CR 下载kubevirt-cr.yaml v0.35.0版本下载链接 上传k8s节点发布 kubectl apply -f kubevirt-cr.yaml 等待组件启动 kubectl -n kubevirt wait kv kubevirt --for condition=Available Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"6.编程/golang/":{"url":"6.编程/golang/","title":"golang","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"6.编程/golang/01-安装配置/01-golang安装配置.html":{"url":"6.编程/golang/01-安装配置/01-golang安装配置.html","title":"01-golang安装配置","keywords":"","body":"linux 下载安装 wget https://studygolang.com/dl/golang/go1.13.4.linux-amd64.tar.gz sudo tar zxvf go1.13.4.linux-amd64.tar.gz -C /usr/local 配置 sudo mkdir -p $HOME/{src,bin,pkg} cat >> ~/.bash_profile Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"6.编程/golang/01-安装配置/02-配置代理.html":{"url":"6.编程/golang/01-安装配置/02-配置代理.html","title":"02-配置代理","keywords":"","body":"配置代理 类unix配置goproxy cat >> ~/.bash_profile windwos配置goproxy 打开powershell执行: # Enable the go modules feature $env:GO111MODULE=\"on\" # Set the GOPROXY environment variable $env:GOPROXY=\"https://goproxy.cn\" Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"6.编程/golang/111-开源库/01-logrus.html":{"url":"6.编程/golang/111-开源库/01-logrus.html","title":"01-logrus","keywords":"","body":"日志管理 logrus项目地址 1.添加或直接import(利用gomod自动下载) go get github.com/sirupsen/logrus 2.默认输出格式 样例如下入参为 /opt/nginx/sbin/nginx package process import ( \"bytes\" log \"github.com/sirupsen/logrus\" \"os/exec\" ) var( exitCode = 0 out bytes.Buffer ) func Shell(shell string) (stdout string,exitCode int){ cmd := exec.Command(\"/bin/bash\", \"-c\", shell + \" 2>&1\") log.Info(\"执行命令为\" + shell) cmd.Stdout = &out if err := cmd.Start();err != nil { println(err.Error()) } if err := cmd.Wait();err != nil { exitCode = 1 } return out.String(),exitCode } 输出如下 3.引入格式包 go get github.com/antonfisher/nested-logrus-formatter 4.格式化日志输出 package utils import ( nested \"github.com/antonfisher/nested-logrus-formatter\" \"github.com/sirupsen/logrus\" \"time\" ) type Formatter struct { FieldsOrder []string // by default fields are sorted alphabetically TimestampFormat string // by default time.StampMilli = \"Jan _2 15:04:05.000\" is used HideKeys bool // to show only [fieldValue] instead of [fieldKey:fieldValue] NoColors bool // to disable all colors NoFieldsColors bool // to disable colors only on fields and keep levels colored ShowFullLevel bool // to show full level (e.g. [WARNING] instead of [WARN]) TrimMessages bool // to trim whitespace on messages } func Logger(format string, args ...interface{}) { logrus.SetFormatter(&logrus.TextFormatter{ForceColors: true}) logrus.SetLevel(logrus.InfoLevel) logrus.SetFormatter(&nested.Formatter{ HideKeys: true, NoColors: false, TrimMessages:false, TimestampFormat: time.RFC3339, FieldsOrder: []string{\"component\", \"category\"}, }) } 输出信息如下 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-09-19 11:20:15 "},"6.编程/letcode/letcode.html":{"url":"6.编程/letcode/letcode.html","title":"letcode","keywords":"","body":" 两数之和 二分查找 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"7.FAQ/sre.html":{"url":"7.FAQ/sre.html","title":"sre","keywords":"","body":" 1.linux启动顺序 启动第一步－－加载 BIOS 当你打开计算机电源，计算机会首先加载 BIOS 信息，BIOS 信息是如此的重要，以至于计 算机必须在最开始就找到它。这是因为 BIOS 中包含了 CPU 的相关信息、设备启动顺序信 息、硬盘信息、内存信息、时钟信息、PnP 特性等等。在此之后，计算机心里就有谱了， 知道应该去读取哪个硬件设备了。 启动第二步－－读取 MBR 众所周知，硬盘上第 0 磁道第一个扇区被称为 MBR，也就是 Master Boot Record，即主 引导记录，它的大小是 512 字节，别看地方不大，可里面却存放了预启动信息、分区表信 息。 系统找到 BIOS 所指定的硬盘的 MBR 后，就会将其复制到 0x7c00 地址所在的物理内存中。 其实被复制到物理内存的内容就是 Boot Loader，而具体到你的电脑，那就是 lilo 或者 grub 了。 启动第三步－－Boot Loader Boot Loader 就是在操作系统内核运行之前运行的一段小程序。通过这段小程序，我们可 以初始化硬件设备、建立内存空间的映射图，从而将系统的软硬件环境带到一个合适的状态， 以便为最终调用操作系统内核做好一切准备。 Boot Loader 有若干种，其中 Grub、Lilo 和 spfdisk 是常见的 Loader。 我们以 Grub 为例来讲解吧，毕竟用 lilo 和 spfdisk 的人并不多。 系统读取内存中的 grub 配置信息（一般为 menu.lst 或 grub.lst），并依照此配置信息来 启动不同的操作系统。 启动第四步－－加载内核 根据 grub 设定的内核映像所在路径，系统读取内核映像，并进行解压缩操作。此时，屏幕 一般会输出“Uncompressing Linux”的提示。当解压缩内核完成后，屏幕输出“OK, booting the kernel”。 系统将解压后的内核放置在内存之中，并调用 start_kernel()函数来启动一系列的初始化函 数并初始化各种设备，完成 Linux 核心环境的建立。至此，Linux 内核已经建立起来了，基 于 Linux 的程序应该可以正常运行了。 启动第五步－－用户层 init 依据 inittab 文件来设定运行等级 内核被加载后，第一个运行的程序便是/sbin/init，该文件会读取/etc/inittab 文件，并依据 此文件来进行初始化工作。 其实/etc/inittab 文件最主要的作用就是设定 Linux 的运行等级，其设定形式是“： id:5:initdefault:”，这就表明 Linux 需要运行在等级 5 上。Linux 的运行等级设定如下： 0：关机 1：单用户模式 2：无网络支持的多用户模式 3：有网络支持的多用户模式 4：保留，未使用 5：有网络支持有 X-Window 支持的多用户模式 6：重新引导系统，即重启 启动第六步－－init 进程执行 rc.sysinit 在设定了运行等级后，Linux 系统执行的第一个用户层文件就是/etc/rc.d/rc.sysinit 脚本程 序，它做的工作非常多，包括设定 PATH、设定网络配置（/etc/sysconfig/network）、启 动 swap 分区、设定/proc 等等。如果你有兴趣，可以到/etc/rc.d 中查看一下 rc.sysinit 文 件。 启动第七步－－启动内核模块 具体是依据/etc/modules.conf 文件或/etc/modules.d 目录下的文件来装载内核模块。 启动第八步－－执行不同运行级别的脚本程序 根据运行级别的不同，系统会运行 rc0.d 到 rc6.d 中的相应的脚本程序，来完成相应的初始 化工作和启动相应的服务。 启动第九步－－执行/etc/rc.d/rc.local 你如果打开了此文件，里面有一句话，读过之后，你就会对此命令的作用一目了然 rc.local 就是在一切初始化工作后，Linux 留给用户进行个性化的地方。你可以把你想设置 和启动的东西放到这里。 启动第十步－－执行/bin/login 程序，进入登录状态 Linux 常见的系统日志文件 /var/log/messages 内核及公共消息日志 /var/log/cron 计划任务日志 /var/log/dmesg 系统引导日志 /var/log/maillog 邮件系统日志 /var/log/secure 记录与访问限制相关日志 3.讲一下 Keepalived 的工作原理 在一个虚拟路由器中，只有作为 MASTER 的 VRRP 路由器会一直发送 VRRP 通告信息， BACKUP 不会抢占 MASTER，除非它的优先级更高。 当 MASTER 不可用时(BACKUP收不到通告信息)多台 BACKUP 中优先级最高的这台会被抢占为 MASTER。这种抢占是非常快速的( 4.OSI协议 物理层：EIA/TIA-232, EIA/TIA-499, V.35, V.24, RJ45, Ethernet, 802.3, 802.5, FDDI, NRZI, NRZ, B8ZS 数据链路层：Frame Relay, HDLC, PPP, IEEE 802.3/802.2, FDDI, ATM, IEEE 802.5/802.2 网络层：IP，IPX，AppleTalk DDP 传输层：TCP，UDP，SPX 会话层：RPC,SQL,NFS,NetBIOS,names,AppleTalk,ASP,DECnet,SCP 表示层:TIFF,GIF,JPEG,PICT,ASCII,EBCDIC,encryption,MPEG,MIDI,HTML 应用层：FTP,WWW,Telnet,NFS,SMTP,Gateway,SNMP 5.文件系统只读及恢复 6.raid 主要性能排序 冗余从好到坏：raid 1 raid 10 raid 5 raid 0 　　性能从好到坏：raid 0 raid 10 raid 5 raid 1 　　成本从低到高：raid 0 raid 5 raid 1 raid 10 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-21 11:06:05 "},"vocabulary.html":{"url":"vocabulary.html","title":"vocabulary","keywords":"","body":"DevOps 持续交付 Continuous Delivery [kənˈtɪnjuəs] [dɪˈlɪvəri] Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2021-08-07 15:52:08 "}}